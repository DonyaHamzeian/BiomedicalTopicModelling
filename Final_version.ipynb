{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(2)\n",
    "torch.cuda.current_device()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "import re\n",
    "\n",
    "from medner import MedNER\n",
    "from medlinker import MedLinker\n",
    "from umls import umls_kb_st21pv as umls_kb\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read  metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/2019-03/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (1,4,5,6,13,14,15,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "meta = pd.read_csv('/home/dhamzeia/Data/metadata.csv')\n",
    "# meta = meta['abstract']\n",
    "\n",
    "\n",
    "# calculate the number of empty documents\n",
    "#empty = 0\n",
    "# # from langdetect import detect\n",
    "\n",
    "# for i, m in enumerate(meta):\n",
    "#     if str(m)=='nan':\n",
    "#         empty+=1\n",
    "#         continue\n",
    "# print(empty)\n",
    "    \n",
    "# 414020 of the abstracts are empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load medlinker models (NER + EL) for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# default models, best configuration from paper\n",
    "# to experiment with different configurations, just comment/uncomment components\n",
    "\n",
    "cx_ner_path = 'models/ContextualNER/mm_st21pv_SCIBERT_uncased/'\n",
    "em_ner_path = 'models/ExactMatchNER/umls.2017AA.active.st21pv.nerfed_nlp_and_matcher.max3.p'\n",
    "ngram_db_path = 'models/SimString/umls.2017AA.active.st21pv.aliases.3gram.5toks.db'\n",
    "ngram_map_path = 'models/SimString/umls.2017AA.active.st21pv.aliases.5toks.map'\n",
    "st_vsm_path = 'models/VSMs/mm_st21pv.sts_anns.scibert_scivocab_uncased.vecs'\n",
    "cui_vsm_path = 'models/VSMs/mm_st21pv.cuis.scibert_scivocab_uncased.vecs'\n",
    "cui_clf_path = 'models/Classifiers/softmax.cui.h5'\n",
    "sty_clf_path = 'models/Classifiers/softmax.sty.h5'\n",
    "cui_val_path = 'models/Validators/mm_st21pv.lr_clf_cui.dev.joblib'\n",
    "sty_val_path = 'models/Validators/mm_st21pv.lr_clf_sty.dev.joblib'\n",
    "\n",
    "print('Loading MedNER ...')\n",
    "medner = MedNER(umls_kb)\n",
    "medner.load_contextual_ner(cx_ner_path, 2)\n",
    "\n",
    "print('Loading MedLinker ...')\n",
    "medlinker = MedLinker(medner, umls_kb)\n",
    "\n",
    "medlinker.load_string_matcher(ngram_db_path, ngram_map_path)  # simstring approximate string matching\n",
    "\n",
    "# medlinker.load_st_VSM(st_vsm_path)\n",
    "medlinker.load_sty_clf(sty_clf_path)\n",
    "# medlinker.load_st_validator(sty_val_path, validator_thresh=0.45)\n",
    "\n",
    "# medlinker.load_cui_VSM(cui_vsm_path)\n",
    "medlinker.load_cui_clf(cui_clf_path)\n",
    "# medlinker.load_cui_validator(cui_val_path, validator_thresh=0.70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of MedLinker's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'The severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) pandemic was first reported in Wuhan, China in December 2019, moved across the globe at an unprecedented speed, and is having a profound and yet still unfolding health and socioeconomic impacts. SARS-CoV-2, a β-coronavirus, is a highly contagious respiratory pathogen that causes a disease that has been termed the 2019 coronavirus disease (COVID-19). Clinical experience thus far indicates that COVID-19 is highly heterogeneous, ranging from being asymptomatic and mild to severe and causing death. Host factors including age, sex, and comorbid conditions are key determinants of disease severity and progression. Aging itself is a prominent risk factor for severe disease and death from COVID-19. We hypothesize that age-related decline and dysregulation of immune function, i.e., immunosenescence and inflammaging play a major role in contributing to heightened vulnerability to severe COVID-19 outcomes in older adults. Much remains to be learned about the immune responses to SARS-CoV-2 infection. We need to begin partitioning all immunological outcome data by age to better understand disease heterogeneity and aging. Such knowledge is critical not only for understanding of covid-19 pathogenesis but also for COVID-19 vaccine development.',\n",
       " 'tokens': ['The',\n",
       "  'severe',\n",
       "  'acute',\n",
       "  'respiratory',\n",
       "  'syndrome',\n",
       "  'coronavirus-2',\n",
       "  '(SARS-CoV-2)',\n",
       "  'pandemic',\n",
       "  'was',\n",
       "  'first',\n",
       "  'reported',\n",
       "  'in',\n",
       "  'Wuhan,',\n",
       "  'China',\n",
       "  'in',\n",
       "  'December',\n",
       "  '2019,',\n",
       "  'moved',\n",
       "  'across',\n",
       "  'the',\n",
       "  'globe',\n",
       "  'at',\n",
       "  'an',\n",
       "  'unprecedented',\n",
       "  'speed,',\n",
       "  'and',\n",
       "  'is',\n",
       "  'having',\n",
       "  'a',\n",
       "  'profound',\n",
       "  'and',\n",
       "  'yet',\n",
       "  'still',\n",
       "  'unfolding',\n",
       "  'health',\n",
       "  'and',\n",
       "  'socioeconomic',\n",
       "  'impacts.',\n",
       "  'SARS-CoV-2,',\n",
       "  'a',\n",
       "  'β-coronavirus,',\n",
       "  'is',\n",
       "  'a',\n",
       "  'highly',\n",
       "  'contagious',\n",
       "  'respiratory',\n",
       "  'pathogen',\n",
       "  'that',\n",
       "  'causes',\n",
       "  'a',\n",
       "  'disease',\n",
       "  'that',\n",
       "  'has',\n",
       "  'been',\n",
       "  'termed',\n",
       "  'the',\n",
       "  '2019',\n",
       "  'coronavirus',\n",
       "  'disease',\n",
       "  '(COVID-19).',\n",
       "  'Clinical',\n",
       "  'experience',\n",
       "  'thus',\n",
       "  'far',\n",
       "  'indicates',\n",
       "  'that',\n",
       "  'COVID-19',\n",
       "  'is',\n",
       "  'highly',\n",
       "  'heterogeneous,',\n",
       "  'ranging',\n",
       "  'from',\n",
       "  'being',\n",
       "  'asymptomatic',\n",
       "  'and',\n",
       "  'mild',\n",
       "  'to',\n",
       "  'severe',\n",
       "  'and',\n",
       "  'causing',\n",
       "  'death.',\n",
       "  'Host',\n",
       "  'factors',\n",
       "  'including',\n",
       "  'age,',\n",
       "  'sex,',\n",
       "  'and',\n",
       "  'comorbid',\n",
       "  'conditions',\n",
       "  'are',\n",
       "  'key',\n",
       "  'determinants',\n",
       "  'of',\n",
       "  'disease',\n",
       "  'severity',\n",
       "  'and',\n",
       "  'progression.',\n",
       "  'Aging',\n",
       "  'itself',\n",
       "  'is',\n",
       "  'a',\n",
       "  'prominent',\n",
       "  'risk',\n",
       "  'factor',\n",
       "  'for',\n",
       "  'severe',\n",
       "  'disease',\n",
       "  'and',\n",
       "  'death',\n",
       "  'from',\n",
       "  'COVID-19.',\n",
       "  'We',\n",
       "  'hypothesize',\n",
       "  'that',\n",
       "  'age-related',\n",
       "  'decline',\n",
       "  'and',\n",
       "  'dysregulation',\n",
       "  'of',\n",
       "  'immune',\n",
       "  'function,',\n",
       "  'i.e.,',\n",
       "  'immunosenescence',\n",
       "  'and',\n",
       "  'inflammaging',\n",
       "  'play',\n",
       "  'a',\n",
       "  'major',\n",
       "  'role',\n",
       "  'in',\n",
       "  'contributing',\n",
       "  'to',\n",
       "  'heightened',\n",
       "  'vulnerability',\n",
       "  'to',\n",
       "  'severe',\n",
       "  'COVID-19',\n",
       "  'outcomes',\n",
       "  'in',\n",
       "  'older',\n",
       "  'adults.',\n",
       "  'Much',\n",
       "  'remains',\n",
       "  'to',\n",
       "  'be',\n",
       "  'learned',\n",
       "  'about',\n",
       "  'the',\n",
       "  'immune',\n",
       "  'responses',\n",
       "  'to',\n",
       "  'SARS-CoV-2',\n",
       "  'infection.',\n",
       "  'We',\n",
       "  'need',\n",
       "  'to',\n",
       "  'begin',\n",
       "  'partitioning',\n",
       "  'all',\n",
       "  'immunological',\n",
       "  'outcome',\n",
       "  'data',\n",
       "  'by',\n",
       "  'age',\n",
       "  'to',\n",
       "  'better',\n",
       "  'understand',\n",
       "  'disease',\n",
       "  'heterogeneity',\n",
       "  'and',\n",
       "  'aging.',\n",
       "  'Such',\n",
       "  'knowledge',\n",
       "  'is',\n",
       "  'critical',\n",
       "  'not',\n",
       "  'only',\n",
       "  'for',\n",
       "  'understanding',\n",
       "  'of',\n",
       "  'covid-19',\n",
       "  'pathogenesis',\n",
       "  'but',\n",
       "  'also',\n",
       "  'for',\n",
       "  'COVID-19',\n",
       "  'vaccine',\n",
       "  'development.'],\n",
       " 'spans': [{'start': 1,\n",
       "   'end': 6,\n",
       "   'text': 'severe acute respiratory syndrome coronavirus-2',\n",
       "   'st': ('T047', 0.8379305815963922),\n",
       "   'cui': ('C3694279', 0.9645399)},\n",
       "  {'start': 7,\n",
       "   'end': 8,\n",
       "   'text': 'pandemic',\n",
       "   'st': ('T005', 0.96061546),\n",
       "   'cui': ('C1214775', 0.75)},\n",
       "  {'start': 12,\n",
       "   'end': 14,\n",
       "   'text': 'Wuhan, China',\n",
       "   'st': ('T082', 0.99807715),\n",
       "   'cui': ('C0008115', 1.0)},\n",
       "  {'start': 20,\n",
       "   'end': 21,\n",
       "   'text': 'globe',\n",
       "   'st': ('T082', 0.96710956),\n",
       "   'cui': ('C2700280', 0.99110854)},\n",
       "  {'start': 38,\n",
       "   'end': 39,\n",
       "   'text': 'SARS-CoV-2,',\n",
       "   'st': ('T005', 0.9999075),\n",
       "   'cui': ('C0010076', 0.999775)},\n",
       "  {'start': 40,\n",
       "   'end': 41,\n",
       "   'text': 'β-coronavirus,',\n",
       "   'st': ('T005', 1.0),\n",
       "   'cui': ('C0010076', 0.9999988)},\n",
       "  {'start': 50,\n",
       "   'end': 51,\n",
       "   'text': 'disease',\n",
       "   'st': ('T047', 1.0),\n",
       "   'cui': ('C0012634', 1.0)},\n",
       "  {'start': 57,\n",
       "   'end': 59,\n",
       "   'text': 'coronavirus disease',\n",
       "   'st': ('T005', 0.9999999),\n",
       "   'cui': ('C0010076', 1.0)},\n",
       "  {'start': 60,\n",
       "   'end': 62,\n",
       "   'text': 'Clinical experience',\n",
       "   'st': ('T041', 0.7254762501100117),\n",
       "   'cui': ('C4084924', 0.9999927)},\n",
       "  {'start': 73,\n",
       "   'end': 74,\n",
       "   'text': 'asymptomatic',\n",
       "   'st': ('T033', 1.0),\n",
       "   'cui': ('C0231221', 1.0)},\n",
       "  {'start': 80,\n",
       "   'end': 81,\n",
       "   'text': 'death.',\n",
       "   'st': ('T043', 0.5163977794943222),\n",
       "   'cui': ('C1306577', 0.99563247)},\n",
       "  {'start': 87,\n",
       "   'end': 89,\n",
       "   'text': 'comorbid conditions',\n",
       "   'st': ('T033', 0.87052685),\n",
       "   'cui': ('C1275743', 1.0)},\n",
       "  {'start': 97,\n",
       "   'end': 98,\n",
       "   'text': 'Aging',\n",
       "   'st': ('T040', 1.0),\n",
       "   'cui': ('C0001811', 1.0)},\n",
       "  {'start': 102,\n",
       "   'end': 104,\n",
       "   'text': 'risk factor',\n",
       "   'st': ('T033', 1.0),\n",
       "   'cui': ('C0035648', 1.0)},\n",
       "  {'start': 117,\n",
       "   'end': 118,\n",
       "   'text': 'dysregulation',\n",
       "   'st': ('T038', 0.9530414),\n",
       "   'cui': ('C3714634', 0.9977302)},\n",
       "  {'start': 119,\n",
       "   'end': 121,\n",
       "   'text': 'immune function,',\n",
       "   'st': ('T038', 0.9561472),\n",
       "   'cui': ('C3714634', 0.9212033)},\n",
       "  {'start': 122,\n",
       "   'end': 123,\n",
       "   'text': 'immunosenescence',\n",
       "   'st': ('T039', 1.0),\n",
       "   'cui': ('C0596761', 1.0)},\n",
       "  {'start': 124,\n",
       "   'end': 125,\n",
       "   'text': 'inflammaging',\n",
       "   'st': ('T038', 0.6746919),\n",
       "   'cui': ('C3540840', 0.986704)},\n",
       "  {'start': 148,\n",
       "   'end': 150,\n",
       "   'text': 'immune responses',\n",
       "   'st': ('T042', 0.9036961141150639),\n",
       "   'cui': ('C0301872', 1.0)},\n",
       "  {'start': 181,\n",
       "   'end': 182,\n",
       "   'text': 'pathogenesis',\n",
       "   'st': ('T046', 1.0),\n",
       "   'cui': ('C0699748', 1.0)},\n",
       "  {'start': 185,\n",
       "   'end': 187,\n",
       "   'text': 'COVID-19 vaccine',\n",
       "   'st': ('T103', 0.9832191),\n",
       "   'cui': ('C0042210', 0.9974548)}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = '''The severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) pandemic was first reported in Wuhan, China in December 2019, moved across the globe at an unprecedented speed, and is having a profound and yet still unfolding health and socioeconomic impacts. SARS-CoV-2, a β-coronavirus, is a highly contagious respiratory pathogen that causes a disease that has been termed the 2019 coronavirus disease (COVID-19). Clinical experience thus far indicates that COVID-19 is highly heterogeneous, ranging from being asymptomatic and mild to severe and causing death. Host factors including age, sex, and comorbid conditions are key determinants of disease severity and progression. Aging itself is a prominent risk factor for severe disease and death from COVID-19. We hypothesize that age-related decline and dysregulation of immune function, i.e., immunosenescence and inflammaging play a major role in contributing to heightened vulnerability to severe COVID-19 outcomes in older adults. Much remains to be learned about the immune responses to SARS-CoV-2 infection. We need to begin partitioning all immunological outcome data by age to better understand disease heterogeneity and aging. Such knowledge is critical not only for understanding of covid-19 pathogenesis but also for COVID-19 vaccine development.'''\n",
    "medlinker.predict(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do some preprocessing before medlinker: remove numbers/floats/ratio/all_capitals/Non_english& empty abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# t = re.findall('([A-Z]{4,})', text)\n",
    "# elapsed time: 85s for 100 docs. for 400k would be >90h!\n",
    "medlinked = []\n",
    "non_english = 0\n",
    "empty =0\n",
    "predicted_index = []\n",
    "\n",
    "for i, text in enumerate(meta['abstract']):\n",
    "    if i <150000:\n",
    "        continue\n",
    "    print(i)\n",
    "    print('medlinked is: {}'.format(len(medlinked)))\n",
    "    if i in predicted_index:\n",
    "        continue\n",
    "    if text=='' or str(text)=='nan': \n",
    "        empty+=1\n",
    "        print(text)\n",
    "        continue\n",
    "    try:\n",
    "        lang= detect(text)\n",
    "        if lang!= 'en':\n",
    "            non_english +=1\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(text)\n",
    "        print(e)\n",
    "        empty+=1\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    sent_text = nltk.sent_tokenize(text)\n",
    "    for sent in sent_text:\n",
    "        sent = re.sub(r'(PATIENTS|STATISTICAL|ANALYSIS|SETTING|REVIEWERS|QUESTIONS|PURPOSES|PURPOSE|EXPERIMENTAL|DESIGN|RELEVANCE|CLINICAL|ENHANCED|VERSION|REGISTRATION|MATERIALS|ELECTRONIC|MATERIAL|PRESENTATION|OBJECTIVE|METHODS|CONCLUSION|RESULTS|BACKGROUND|INTRODUCTION|DISCUSSION|SIGNIFICANCE|DESCRIPTION|ABSTRACT|METHODOLOGY|PRINCIPAL|FINDINGS|INFORMATION|SUPPLEMENTARY|SUMMARY)', ' ', sent)\n",
    "        sent = re.sub(r\"\\d+\\.\\d+\",' ', sent)\n",
    "        sent = re.sub(r\"\\d+\\.\\d+%\",' ', sent)\n",
    "        sent = re.sub(r\"\\d+\\,\\d+\",' ', sent)\n",
    "        sent = re.sub(r\"\\.\\d+\",' ', sent)\n",
    "        sent = re.sub(r\"\\,\\d+\",' ', sent)\n",
    "        sent= re.sub(r' [0-9]+ ','  ',sent)\n",
    "        sent = re.sub(r'\\[.*?\\]', '', sent)\n",
    "        sent = re.sub(r'[0-9]+%',' ', sent)\n",
    "        if len(sent)>10 and len(sent)< 1000:\n",
    "            r = medlinker.predict(sent)\n",
    "            r['index'] = i\n",
    "            medlinked.append(r)\n",
    "            predicted_index.append(i)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(text)\n",
    "#         print(e)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "until i= 241807, empty= 31170, and non_english = 1732, predicted = 47241\n"
     ]
    }
   ],
   "source": [
    "print('until i= {}, empty= {}, and non_english = {}, predicted = {}'.format(i, empty, non_english, len(set(predicted_index))))\n",
    "#until i= 52309, empty= 17737, and non_english = 998\n",
    "#until i= 56384, empty= 18726, and non_english = 1015, predicted = 36832\n",
    "#from i = 150k until  i= 241807, empty= 31170, and non_english = 1732, predicted = 47241\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save medlinked to csv\n",
    "medlinked_df = pd.DataFrame(medlinked)   \n",
    "medlinked_df.to_csv('medlinked_testset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokens</th>\n",
       "      <th>spans</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Extreme prolonged drought over south-eastern A...</td>\n",
       "      <td>['Extreme', 'prolonged', 'drought', 'over', 's...</td>\n",
       "      <td>[{'start': 4, 'end': 6, 'text': 'south-eastern...</td>\n",
       "      <td>150001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Whilst the significant release of metal-enrich...</td>\n",
       "      <td>['Whilst', 'the', 'significant', 'release', 'o...</td>\n",
       "      <td>[{'start': 5, 'end': 6, 'text': 'metal-enriche...</td>\n",
       "      <td>150001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Here we investigate the spatial distribution, ...</td>\n",
       "      <td>['Here', 'we', 'investigate', 'the', 'spatial'...</td>\n",
       "      <td>[{'start': 12, 'end': 14, 'text': 'S, Fe,', 's...</td>\n",
       "      <td>150001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The highest concentrations of all metals (Enri...</td>\n",
       "      <td>['The', 'highest', 'concentrations', 'of', 'al...</td>\n",
       "      <td>[{'start': 5, 'end': 6, 'text': 'metals', 'st'...</td>\n",
       "      <td>150001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Localised enrichment indicates metals have not...</td>\n",
       "      <td>['Localised', 'enrichment', 'indicates', 'meta...</td>\n",
       "      <td>[{'start': 3, 'end': 4, 'text': 'metals', 'st'...</td>\n",
       "      <td>150001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           sentence  \\\n",
       "0           0  Extreme prolonged drought over south-eastern A...   \n",
       "1           1  Whilst the significant release of metal-enrich...   \n",
       "2           2  Here we investigate the spatial distribution, ...   \n",
       "3           3  The highest concentrations of all metals (Enri...   \n",
       "4           4  Localised enrichment indicates metals have not...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['Extreme', 'prolonged', 'drought', 'over', 's...   \n",
       "1  ['Whilst', 'the', 'significant', 'release', 'o...   \n",
       "2  ['Here', 'we', 'investigate', 'the', 'spatial'...   \n",
       "3  ['The', 'highest', 'concentrations', 'of', 'al...   \n",
       "4  ['Localised', 'enrichment', 'indicates', 'meta...   \n",
       "\n",
       "                                               spans   index  \n",
       "0  [{'start': 4, 'end': 6, 'text': 'south-eastern...  150001  \n",
       "1  [{'start': 5, 'end': 6, 'text': 'metal-enriche...  150001  \n",
       "2  [{'start': 12, 'end': 14, 'text': 'S, Fe,', 's...  150001  \n",
       "3  [{'start': 5, 'end': 6, 'text': 'metals', 'st'...  150001  \n",
       "4  [{'start': 3, 'end': 4, 'text': 'metals', 'st'...  150001  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read medlinked. medlinked was applied on sentences. We need to combine them to form a abstract\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "medlinked_df = pd.read_csv('medlinked_testset.csv' )\n",
    "medlinked_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply lemmatization, punctuation removal, and lowercase after medlinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tok):\n",
    "#     print(\"tok  \" + tok)\n",
    "    clean_tok = tok.lower()\n",
    "    clean_chars = ''\n",
    "    for ch in clean_tok:\n",
    "        if ch in '0123456789abcdefghijklmnopqrstuvwxyz-':\n",
    "            clean_chars+= ch\n",
    "        else:\n",
    "            clean_chars+=' '\n",
    "    clean_tok = clean_chars\n",
    "    clean_tok = clean_tok.strip()\n",
    "    if clean_tok.lower() not in stopwords.words('english'):\n",
    "        clean_tok = re.sub(r' [0-9]+ ','  ',clean_tok)\n",
    "        clean_tok = re.sub(r'[0-9]+ ','  ',clean_tok)\n",
    "        clean_tok = re.sub(r' [0-9]+','  ',clean_tok)\n",
    "        clean_tok = re.sub(r'[0-9]+s','  ',clean_tok)\n",
    "        clean_tok = re.sub(r'[0-9]+th','  ',clean_tok)\n",
    "        clean_tok = clean_tok.strip()\n",
    "\n",
    "        if clean_tok !='' and not clean_tok.isnumeric():\n",
    "\n",
    "            if clean_tok!='eg' and clean_tok !='ie' and len(clean_tok)>1 and clean_tok not in stops:\n",
    "\n",
    "                clean_tok = wordnet_lemmatizer.lemmatize(clean_tok, pos = 'n')\n",
    "                clean_tok = wordnet_lemmatizer.lemmatize(clean_tok, pos = 'v')\n",
    "                clean_tok = wordnet_lemmatizer.lemmatize(clean_tok, pos = 'a')\n",
    "                if clean_tok[0] not in string.digits:\n",
    "                    if clean_tok[0]=='-':\n",
    "                        clean_tok = clean_tok[1:len(clean_tok)] \n",
    "                    return(clean_tok)\n",
    "#                     updated_str+= ' '+clean_tok\n",
    "    return ''\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#process medlinked\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "exclude = set(string.punctuation)\n",
    "# preprocessed = []\n",
    "medlinked_df['preprocessed'] = ''\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stops = stopwords.words('english')\n",
    "for a in ['ii', 'iii', 'iv', 'vi', 'ix', 'vii', 'viii']:\n",
    "    stops.append(a)\n",
    "for row in range(len(medlinked_df)) :\n",
    "    tokens, spans, sentence = medlinked_df['tokens'][row], medlinked_df['spans'][row], medlinked_df['sentence'][row]\n",
    "    if medlinked_df['preprocessed'][row]!='':\n",
    "        continue\n",
    "    if row %100==0:\n",
    "        print(row)\n",
    "        \n",
    "    last_end = 0\n",
    "    updated_str = ''\n",
    "    try:\n",
    "        tokens = yaml.load(tokens)\n",
    "        spans = yaml.load(spans)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(row)\n",
    "        continue\n",
    "    if len(spans)>0:\n",
    "        for sp in spans:\n",
    "            start = sp['start']\n",
    "            end = sp['end']\n",
    "            cui = sp['cui']\n",
    "            cui = cui[2:len(cui)-1]\n",
    "    #         print(cui)\n",
    "    #         print()\n",
    "            for tok in tokens[last_end:start]:\n",
    "#                 print(clean(tok))\n",
    "                updated_str +=' ' + clean(tok)\n",
    "            updated_str+=' '\n",
    "            updated_str+= cui\n",
    "            updated_str+=' '\n",
    "            last_end = end\n",
    "\n",
    "        if end < len(tokens):\n",
    "            updated_str+= ' '\n",
    "            for tok in tokens[end:len(tokens)]:\n",
    "                updated_str+= ' '+clean(tok)\n",
    "            \n",
    "    else:\n",
    "        for tok in tokens:\n",
    "                updated_str +=' ' + clean(tok)\n",
    "        \n",
    "#             clean_tok = re.sub(re.compile('[!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~]'),' ', tok)\n",
    "#             clean_tok = re.sub(r'•', ' ', clean_tok)\n",
    "#             clean_tok = re.sub(r'’', ' ', clean_tok)\n",
    "            \n",
    "            \n",
    "#     print(updated_str)\n",
    "\n",
    "    medlinked_df['preprocessed'][row] = updated_str\n",
    "#     preprocessed.append(updated_str)\n",
    "# medlinked_df['preprocessed'] = preprocessed\n",
    "#     print()\n",
    "#     cov['tokens'][start:end] = reduce(cui)\n",
    "#     print('----')\n",
    "#     print(end)\n",
    "#     print(cui)\n",
    "#     print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed& medlinked abstracts\n",
    "medlinked_df.to_csv('medlinked_testset_prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    index                                          prep_text\n",
      "0  150001   extreme prolong drought  C0004340    decade a...\n",
      "1  150002    recent C0546788   pandemic  cause unpreceden...\n",
      "2  150004   C0078939   C0085401   currently represent  tw...\n",
      "3  150007   C0001617  c C0021496   commonly use   C003313...\n",
      "4  150013    early evidence indicate increase C0025353  b...\n"
     ]
    }
   ],
   "source": [
    "# aggregate the results of sentences to form abstracts again\n",
    "agg_df = medlinked_df.groupby(['index'], as_index = False)['preprocessed'].agg({'prep_text': ' '.join})\n",
    "\n",
    "# agg_df['original'] = meta['abstract'][agg_df['index']]\n",
    "print(agg_df.head())\n",
    "agg_df.to_csv('agg_df_testset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read test and train documents (preprocessed) and add url, abstract and ID columns from meta dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>prep_text</th>\n",
       "      <th>url</th>\n",
       "      <th>abstract</th>\n",
       "      <th>cord_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43284</th>\n",
       "      <td>43285</td>\n",
       "      <td>47235</td>\n",
       "      <td>241800</td>\n",
       "      <td>association  age   high vulnerability  C0546...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The association of age with a higher vulnerabi...</td>\n",
       "      <td>xet6b64x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43285</th>\n",
       "      <td>43286</td>\n",
       "      <td>47236</td>\n",
       "      <td>241801</td>\n",
       "      <td>outbreak  C0010076  covid-19  C0008115  caus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>An outbreak of coronavirus disease (COVID-19) ...</td>\n",
       "      <td>7jakv2ge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43286</th>\n",
       "      <td>43287</td>\n",
       "      <td>47237</td>\n",
       "      <td>241802</td>\n",
       "      <td>increase C0038435  resilience  C2936506   imp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Increasing stress resilience of livestock is i...</td>\n",
       "      <td>vy8s51p9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43287</th>\n",
       "      <td>43288</td>\n",
       "      <td>47238</td>\n",
       "      <td>241803</td>\n",
       "      <td>context  C0282574  pandemic  highlight  essen...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CONTEXT: The COVID-19 pandemic has highlighted...</td>\n",
       "      <td>fexdvnuj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43288</th>\n",
       "      <td>43289</td>\n",
       "      <td>47239</td>\n",
       "      <td>241804</td>\n",
       "      <td>C0031323  need effective communication skill ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pharmacists need effective communication skill...</td>\n",
       "      <td>djn6ltnu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       level_0  Unnamed: 0   index  \\\n",
       "43284    43285       47235  241800   \n",
       "43285    43286       47236  241801   \n",
       "43286    43287       47237  241802   \n",
       "43287    43288       47238  241803   \n",
       "43288    43289       47239  241804   \n",
       "\n",
       "                                               prep_text  url  \\\n",
       "43284    association  age   high vulnerability  C0546...  NaN   \n",
       "43285    outbreak  C0010076  covid-19  C0008115  caus...  NaN   \n",
       "43286   increase C0038435  resilience  C2936506   imp...  NaN   \n",
       "43287   context  C0282574  pandemic  highlight  essen...  NaN   \n",
       "43288   C0031323  need effective communication skill ...  NaN   \n",
       "\n",
       "                                                abstract  cord_uid  \n",
       "43284  The association of age with a higher vulnerabi...  xet6b64x  \n",
       "43285  An outbreak of coronavirus disease (COVID-19) ...  7jakv2ge  \n",
       "43286  Increasing stress resilience of livestock is i...  vy8s51p9  \n",
       "43287  CONTEXT: The COVID-19 pandemic has highlighted...  fexdvnuj  \n",
       "43288  Pharmacists need effective communication skill...  djn6ltnu  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "agg_df = pd.read_csv('agg_df_56384.csv')\n",
    "test_df = pd.read_csv('agg_df_testset.csv')\n",
    "test_df = test_df.drop_duplicates(subset = ['prep_text']).reset_index(drop = True)\n",
    "agg_df = agg_df.drop_duplicates(subset = ['prep_text']).reset_index(drop = True)\n",
    "agg_df.loc[:, 'url'] = meta.loc[agg_df['index'].astype(int), 'url'].reset_index()\n",
    "agg_df.loc[:, 'abstract'] = meta.loc[agg_df['index'].astype(int), 'abstract'].reset_index()\n",
    "agg_df.loc[:, 'cord_uid'] = meta.loc[agg_df['index'].astype(int), 'cord_uid'].reset_index()\n",
    "test_df= test_df.loc[test_df['prep_text'].astype(str)!='nan'].reset_index()\n",
    "test_df.loc[:, 'url'] = meta.loc[test_df['index'].astype(int), 'url'].reset_index()\n",
    "test_df.loc[:, 'abstract'] = meta.loc[test_df['index'].astype(int), 'abstract'].reset_index()\n",
    "test_df.loc[:, 'cord_uid'] = meta.loc[test_df['index'].astype(int), 'cord_uid'].reset_index()\n",
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of a preprocessed document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:\n",
      "The recent COVID-19 pandemic has caused unprecedented impact across the globe. We have also witnessed millions of people with increased mental health issues, such as depression, stress, worry, fear, disgust, sadness, and anxiety, which have become one of the major public health concerns during this severe health crisis. For instance, depression is one of the most common mental health issues according to the findings made by the World Health Organisation (WHO). Depression can cause serious emotional, behavioural and physical health problems with significant consequences, both personal and social costs included. This paper studies community depression dynamics due to COVID-19 pandemic through user-generated content on Twitter. A new approach based on multi-modal features from tweets and Term Frequency-Inverse Document Frequency (TF-IDF) is proposed to build depression classification models. Multi-modal features capture depression cues from emotion, topic and domain-specific perspectives. We study the problem using recently scraped tweets from Twitter users emanating from the state of New South Wales in Australia. Our novel classification model is capable of extracting depression polarities which may be affected by COVID-19 and related events during the COVID-19 period. The results found that people became more depressed after the outbreak of COVID-19. The measures implemented by the government such as the state lockdown also increased depression levels. Further analysis in the Local Government Area (LGA) level found that the community depression level was different across different LGAs. Such granular level analysis of depression dynamics not only can help authorities such as governmental departments to take corresponding actions more objectively in specific regions if necessary but also allows users to perceive the dynamics of depression over the time.\n",
      "\n",
      "preprocessed:\n",
      "  recent C0546788   pandemic  cause unprecedented impact across  globe    also witness million  C0027361   increase C0025353    C0038443  C0025361  C0243095   C0003467     become one   major public health concern   severe health crisis   instance C0011570   one    common C0025353  issue accord   C0243095  make   C0043237     C0011570   cause serious C0233514    physical health problem  significant consequence  personal  social cost include   paper study community C0011570  dynamic due  C0012634   pandemic  user-generated content  twitter   new approach base  multi-modal feature  C0282574   term frequency-inverse document frequency tf-idf  propose  build C0429108  C0008902   model  multi-modal feature capture C0025361  cue  C0013987   topic  domain-specific perspective   study  C0033213  use recently scrap C0282574   twitter C1706077  emanate   C0017446    australia   novel C3161035   capable  extract C0429108  polarity  may  affect  C1826049    relate event   covid-19 period   result find  C0027361  become  C0344315     outbreak  covid-19   measure implement   C0018104     C0018696  also increase C0011570   level   C0936012    C1257890  lga level find   community C0011570   level  different across different lgas   granular level C0936012   C0011570  dynamic    help C1254351    C1708333   take correspond action  objectively  specific C0017446   necessary  also allow C1706077    perceive  dynamic  depression   time\n"
     ]
    }
   ],
   "source": [
    "print('original:')\n",
    "print(test_df.loc[1, 'abstract'])\n",
    "print()\n",
    "print('preprocessed:')\n",
    "print(test_df.loc[1, 'prep_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# from scispacy.abbreviation import AbbreviationDetector\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Add the abbreviation pipe to the spacy pipeline.\n",
    "# abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "# nlp.add_pipe(abbreviation_pipe)\n",
    "\n",
    "\n",
    "abrvs = []\n",
    "for i, text in enumerate(agg_df[ 'abstract']):\n",
    "    doc = nlp(text)\n",
    "    for abrv in doc._.abbreviations:\n",
    "        abrvs.append(abrv)\n",
    "    if i%100 ==0:\n",
    "        print(i)\n",
    "#         print(abrv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "abrvs_set = set([abrv.lower_ for abrv in abrvs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def custom_tokenizer(txt):\n",
    "    toks = []\n",
    "    txt = str(txt)\n",
    "    txt = txt.lower()\n",
    "    for s in txt.split(' '):\n",
    "        if len(s)<2:\n",
    "            continue\n",
    "        if s[0]=='-':\n",
    "            if s[1]=='-':\n",
    "                s= s[1:len(s)]\n",
    "            s = s[1:len(s)]\n",
    "        if len(s)<2:\n",
    "            continue\n",
    "        if s[len(s)-1]=='-':\n",
    "            s= s[0:len(s)-1]\n",
    "        if s.isnumeric():\n",
    "            continue\n",
    "        elif len(s)>2 and s not in abrvs_set:\n",
    "            toks.append(s)\n",
    "    return toks\n",
    "count_vect = CountVectorizer(tokenizer=custom_tokenizer, stop_words='english', max_features=10000)\n",
    "X_train_counts = count_vect.fit_transform(agg_df['prep_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a549\n",
      "abandon\n",
      "abbreviate\n",
      "abdomen\n",
      "abdominal\n",
      "ability\n",
      "abiotic\n",
      "ablate\n",
      "able\n",
      "abnormality\n",
      "abolish\n",
      "above-mentioned\n",
      "abroad\n",
      "abrupt\n",
      "abscess\n",
      "absence\n",
      "absent\n",
      "absenteeism\n",
      "absolutely\n",
      "absorb\n",
      "absorbable\n",
      "absorbance\n",
      "absorption\n",
      "abstinence\n",
      "abstraction\n",
      "abundance\n",
      "abundantly\n",
      "accelerate\n",
      "acceleration\n",
      "accentuate\n",
      "accept\n",
      "acceptability\n",
      "acceptable\n",
      "acceptance\n",
      "accessibility\n",
      "accessible\n",
      "accessory\n",
      "accidental\n",
      "accommodate\n",
      "accompany\n",
      "accomplish\n",
      "accomplishment\n",
      "accord\n",
      "accordance\n",
      "accordingly\n",
      "account\n",
      "accountability\n",
      "accreditation\n",
      "accumulate\n",
      "accumulation\n",
      "accurately\n",
      "achievable\n",
      "achieve\n",
      "achievement\n",
      "acidic\n",
      "acknowledge\n",
      "acquire\n",
      "acquisition\n",
      "actionable\n",
      "activation\n",
      "activator\n",
      "active\n",
      "actor\n",
      "actual\n",
      "acuity\n",
      "acute\n",
      "acutely\n",
      "acyclic\n",
      "ad-hoc\n",
      "adaptability\n",
      "adaptable\n",
      "adaptation\n",
      "addiction\n",
      "additional\n",
      "additionally\n",
      "address\n",
      "adenocarcinoma\n",
      "adenovirus\n",
      "adequately\n",
      "adhere\n",
      "adherence\n",
      "adjunct\n",
      "adjunctive\n",
      "adjust\n",
      "adjustment\n",
      "adjuvanted\n",
      "administer\n",
      "administrate\n",
      "administration\n",
      "administrative\n",
      "admission\n",
      "admit\n",
      "adolescence\n",
      "adolescent\n",
      "adrenal\n",
      "adsorb\n",
      "adsorbent\n",
      "adsorption\n",
      "adult\n",
      "adulthood\n",
      "advance\n",
      "advancement\n",
      "advantage\n",
      "advantageous\n",
      "advent\n",
      "adversarial\n",
      "adverse\n",
      "adversely\n",
      "adversity\n",
      "advice\n",
      "advise\n",
      "advisor\n",
      "aerobic\n",
      "aerosol\n",
      "aerosolize\n",
      "aeruginosa\n",
      "aesthetic\n",
      "aetiological\n",
      "aetiology\n",
      "affair\n",
      "affect\n",
      "affection\n",
      "affective\n",
      "affiliate\n",
      "affiliation\n",
      "afflict\n",
      "afford\n",
      "affordable\n",
      "aforementioned\n",
      "africa\n",
      "african\n",
      "aftermath\n",
      "afterward\n",
      "age-adjusted\n",
      "age-dependent\n",
      "age-matched\n",
      "age-related\n",
      "age-specific\n",
      "agency\n",
      "agenda\n",
      "aggravate\n",
      "aggregate\n",
      "aggregation\n",
      "aggression\n",
      "aggressive\n",
      "agile\n",
      "ago\n",
      "agreement\n",
      "agricultural\n",
      "agriculture\n",
      "ahead\n",
      "air\n",
      "airflow\n",
      "airline\n",
      "airport\n",
      "ajph\n",
      "akt\n",
      "alarm\n",
      "albeit\n",
      "alberta\n",
      "algebra\n",
      "algebraic\n",
      "algorithm\n",
      "algorithmic\n",
      "align\n",
      "alignment\n",
      "alike\n",
      "alive\n",
      "all-cause\n",
      "allele\n",
      "allergy\n",
      "alleviate\n",
      "alleviation\n",
      "allocation\n",
      "allogeneic\n",
      "allograft\n",
      "allow\n",
      "alloy\n",
      "ally\n",
      "alongside\n",
      "alter\n",
      "alteration\n",
      "alternate\n",
      "alternative\n",
      "alternatively\n",
      "altogether\n",
      "ambient\n",
      "ambiguity\n",
      "ambiguous\n",
      "ambulance\n",
      "ambulatory\n",
      "ameliorate\n",
      "amendment\n",
      "amid\n",
      "amidst\n",
      "amino\n",
      "ample\n",
      "amplification\n",
      "amplify\n",
      "amplitude\n",
      "anaerobic\n",
      "anaesthesia\n",
      "analog\n",
      "analogous\n",
      "analogy\n",
      "analyse\n",
      "analytical\n",
      "analytically\n",
      "analyze\n",
      "anastomosis\n",
      "anastomotic\n",
      "anatomical\n",
      "anatomically\n",
      "anatomy\n",
      "ancestral\n",
      "anchor\n",
      "ancient\n",
      "anesthesia\n",
      "anesthesiologist\n",
      "anesthetic\n",
      "angiographic\n",
      "angiography\n",
      "angiotensin\n",
      "angle\n",
      "animal\n",
      "anneal\n",
      "annotation\n",
      "announcement\n",
      "annual\n",
      "anomalous\n",
      "anomaly\n",
      "anonymous\n",
      "answer\n",
      "antagonism\n",
      "antagonist\n",
      "antagonistic\n",
      "antagonize\n",
      "antecedent\n",
      "antenatal\n",
      "antenna\n",
      "anthropogenic\n",
      "anthropometric\n",
      "anti-cancer\n",
      "anti-inflammatory\n",
      "anti-tumor\n",
      "anti-viral\n",
      "antibacterial\n",
      "antibiotic\n",
      "anticancer\n",
      "anticipate\n",
      "anticipation\n",
      "anticoagulation\n",
      "antigenic\n",
      "antigenically\n",
      "antimicrobial\n",
      "antioxidant\n",
      "antiplatelet\n",
      "antitumor\n",
      "antiviral\n",
      "aortic\n",
      "apart\n",
      "apoptosis\n",
      "apoptotic\n",
      "apparatus\n",
      "apparently\n",
      "appeal\n",
      "appear\n",
      "appearance\n",
      "appendectomy\n",
      "applicability\n",
      "applicable\n",
      "application\n",
      "apply\n",
      "appointment\n",
      "apposition\n",
      "appraisal\n",
      "appreciable\n",
      "appreciate\n",
      "appreciation\n",
      "appropriate\n",
      "appropriately\n",
      "appropriateness\n",
      "approve\n",
      "approximate\n",
      "approximately\n",
      "approximation\n",
      "aquaculture\n",
      "aquatic\n",
      "aqueous\n",
      "arbitrary\n",
      "architectural\n",
      "architecture\n",
      "archive\n",
      "arena\n",
      "argue\n",
      "arithmetic\n",
      "armamentarium\n",
      "arrange\n",
      "arrangement\n",
      "array\n",
      "arrhythmia\n",
      "arrival\n",
      "arthroplasty\n",
      "article\n",
      "articulate\n",
      "artificial\n",
      "artificially\n",
      "ascend\n",
      "ascertain\n",
      "ascribe\n",
      "asia\n",
      "asian\n",
      "aside\n",
      "ask\n",
      "aspiration\n",
      "assault\n",
      "assemblage\n",
      "assess\n",
      "assessment\n",
      "asset\n",
      "assign\n",
      "assignment\n",
      "assimilation\n",
      "assist\n",
      "assistance\n",
      "assistant\n",
      "assistive\n",
      "associate\n",
      "associated\n",
      "association\n",
      "assume\n",
      "assumption\n",
      "assurance\n",
      "assure\n",
      "astrocyte\n",
      "asymmetric\n",
      "asymptomatic\n",
      "asymptotic\n",
      "asymptotically\n",
      "asynchronous\n",
      "asynchrony\n",
      "at-risk\n",
      "atherosclerosis\n",
      "atherosclerotic\n",
      "athlete\n",
      "atmosphere\n",
      "atmospheric\n",
      "atom\n",
      "atomic\n",
      "atrial\n",
      "attach\n",
      "attachment\n",
      "attack\n",
      "attain\n",
      "attainment\n",
      "attempt\n",
      "attend\n",
      "attendance\n",
      "attenuate\n",
      "attenuation\n",
      "attraction\n",
      "attractive\n",
      "attributable\n",
      "attribute\n",
      "attribution\n",
      "audio\n",
      "augment\n",
      "augmentation\n",
      "august\n",
      "authentic\n",
      "authenticity\n",
      "author\n",
      "authority\n",
      "authorization\n",
      "authorize\n",
      "autism\n",
      "autocorrelation\n",
      "autoimmune\n",
      "autoimmunity\n",
      "autologous\n",
      "automatic\n",
      "automatically\n",
      "automation\n",
      "automaton\n",
      "autonomous\n",
      "autophagy\n",
      "autoregressive\n",
      "autumn\n",
      "auxiliary\n",
      "availability\n",
      "available\n",
      "avenue\n",
      "avert\n",
      "avian\n",
      "avidity\n",
      "avoid\n",
      "avoidable\n",
      "avoidance\n",
      "await\n",
      "award\n",
      "aware\n",
      "awareness\n",
      "away\n",
      "axiom\n",
      "axis\n",
      "baby\n",
      "background\n",
      "bacteremia\n",
      "bacterial\n",
      "bacterium\n",
      "bad\n",
      "bag\n",
      "balb\n",
      "balloon\n",
      "ban\n",
      "band\n",
      "bandwidth\n",
      "bangladesh\n",
      "bank\n",
      "barb\n",
      "bare\n",
      "barely\n",
      "bariatric\n",
      "barrier\n",
      "basal\n",
      "base\n",
      "basically\n",
      "batch\n",
      "bath\n",
      "battery\n",
      "battle\n",
      "bay\n",
      "bead\n",
      "beam\n",
      "bear\n",
      "beat\n",
      "bedside\n",
      "begin\n",
      "behalf\n",
      "behave\n",
      "behavioral\n",
      "behaviour\n",
      "behavioural\n",
      "beijing\n",
      "belief\n",
      "believe\n",
      "belong\n",
      "benchmark\n",
      "benchmarking\n",
      "bend\n",
      "beneficial\n",
      "beneficiary\n",
      "benefit\n",
      "benign\n",
      "best\n",
      "between-group\n",
      "bidirectional\n",
      "bifurcation\n",
      "bilateral\n",
      "biliary\n",
      "billion\n",
      "bin\n",
      "binary\n",
      "bind\n",
      "binder\n",
      "bioactive\n",
      "bioactivity\n",
      "bioavailability\n",
      "biochemical\n",
      "biochemically\n",
      "biocompatibility\n",
      "biocompatible\n",
      "biodegradable\n",
      "biodegradation\n",
      "biodistribution\n",
      "biodiversity\n",
      "biofilm\n",
      "biofire\n",
      "biogenesis\n",
      "bioinformatics\n",
      "biological\n",
      "biologically\n",
      "biology\n",
      "biomarker\n",
      "biomarkers\n",
      "biomass\n",
      "biomechanical\n",
      "biomedical\n",
      "biomolecules\n",
      "biophysical\n",
      "bioreactor\n",
      "biosafety\n",
      "biosecurity\n",
      "biosensing\n",
      "biosensors\n",
      "biosynthesis\n",
      "biotechnological\n",
      "biotechnology\n",
      "bioterrorism\n",
      "biotic\n",
      "bipolar\n",
      "bird\n",
      "bivariate\n",
      "bladder\n",
      "blame\n",
      "bland-altman\n",
      "bleed\n",
      "blend\n",
      "blind\n",
      "blockchain\n",
      "bloom\n",
      "blot\n",
      "blue\n",
      "blueprint\n",
      "blunt\n",
      "blur\n",
      "board\n",
      "body\n",
      "bolster\n",
      "bolus\n",
      "bond\n",
      "bone\n",
      "book\n",
      "boolean\n",
      "boost\n",
      "booster\n",
      "bootstrap\n",
      "border\n",
      "borderline\n",
      "bottle\n",
      "bottleneck\n",
      "bottom-up\n",
      "bound\n",
      "bout\n",
      "bowel\n",
      "boy\n",
      "brain\n",
      "branch\n",
      "brand\n",
      "breach\n",
      "break\n",
      "breakdown\n",
      "breakthrough\n",
      "breast\n",
      "breastfeed\n",
      "breath\n",
      "breathe\n",
      "breaths\n",
      "breed\n",
      "bridge\n",
      "brief\n",
      "briefly\n",
      "bright\n",
      "bring\n",
      "broad\n",
      "broad-spectrum\n",
      "broadband\n",
      "broadcast\n",
      "broaden\n",
      "broadly\n",
      "bronchial\n",
      "bronchiolitis\n",
      "brown\n",
      "browse\n",
      "bubble\n",
      "bud\n",
      "budget\n",
      "buff\n",
      "bug\n",
      "build\n",
      "bulk\n",
      "bulky\n",
      "bundle\n",
      "burden\n",
      "bureau\n",
      "burn\n",
      "burnout\n",
      "burst\n",
      "bus\n",
      "buy\n",
      "by-product\n",
      "bystander\n",
      "c57bl\n",
      "cabin\n",
      "cage\n",
      "calcify\n",
      "calcium\n",
      "calculate\n",
      "calculation\n",
      "calf\n",
      "calibrate\n",
      "calibration\n",
      "california\n",
      "camel\n",
      "camera\n",
      "campylobacter\n",
      "canada\n",
      "cancel\n",
      "cancellation\n",
      "candida\n",
      "candidate\n",
      "canonical\n",
      "cantilever\n",
      "capability\n",
      "capable\n",
      "capillary\n",
      "capital\n",
      "capitalism\n",
      "capsid\n",
      "capsule\n",
      "captive\n",
      "capture\n",
      "caput\n",
      "carcinogenesis\n",
      "card\n",
      "cardiogenic\n",
      "cardioprotective\n",
      "cardiopulmonary\n",
      "cardiovascular\n",
      "career\n",
      "careful\n",
      "carefully\n",
      "caregiver\n",
      "caregiving\n",
      "carers\n",
      "cargo\n",
      "carlo\n",
      "carotid\n",
      "carrier\n",
      "carry\n",
      "cartilage\n",
      "cascade\n",
      "case-control\n",
      "case-fatality\n",
      "cash\n",
      "catalysis\n",
      "catalytic\n",
      "catastrophe\n",
      "catastrophic\n",
      "catch\n",
      "catchment\n",
      "categorical\n",
      "categorise\n",
      "categorize\n",
      "cattle\n",
      "causal\n",
      "causality\n",
      "causation\n",
      "causative\n",
      "cause\n",
      "caution\n",
      "cavity\n",
      "cd4\n",
      "cd8\n",
      "cease\n",
      "census\n",
      "cent\n",
      "centrality\n",
      "centralize\n",
      "centrally\n",
      "centre\n",
      "century\n",
      "cerebrovascular\n",
      "certain\n",
      "certainly\n",
      "certainty\n",
      "certificate\n",
      "certification\n",
      "certify\n",
      "cessation\n",
      "chair\n",
      "challenge\n",
      "chamber\n",
      "chance\n",
      "channel\n",
      "chaotic\n",
      "chap\n",
      "chapter\n",
      "character\n",
      "characterisation\n",
      "characterise\n",
      "characteristic\n",
      "characterization\n",
      "characterize\n",
      "charge\n",
      "chart\n",
      "cheap\n",
      "check\n",
      "checklist\n",
      "checkpoint\n",
      "cheetah\n",
      "chemically\n",
      "chemistry\n",
      "chemokines\n",
      "chemotherapy\n",
      "chest\n",
      "chi-square\n",
      "chick\n",
      "chicken\n",
      "chief\n",
      "chiefly\n",
      "child\n",
      "childbirth\n",
      "childcare\n",
      "childhood\n",
      "chinese\n",
      "chiral\n",
      "chloroquine\n",
      "cholecystectomy\n",
      "cholesterol\n",
      "choose\n",
      "chromatographic\n",
      "chromatography\n",
      "chromosome\n",
      "chronic\n",
      "chronically\n",
      "chronological\n",
      "cigarette\n",
      "circadian\n",
      "circuit\n",
      "circulate\n",
      "circulation\n",
      "circumference\n",
      "circumstance\n",
      "circumvent\n",
      "cirrhosis\n",
      "cite\n",
      "citizen\n",
      "citizenship\n",
      "city\n",
      "civic\n",
      "civil\n",
      "civilization\n",
      "claim\n",
      "clarification\n",
      "clarify\n",
      "clarity\n",
      "classic\n",
      "classical\n",
      "classically\n",
      "classification\n",
      "classifier\n",
      "classroom\n",
      "clause\n",
      "clavien-dindo\n",
      "clean\n",
      "clear\n",
      "clearance\n",
      "clearly\n",
      "cleave\n",
      "click\n",
      "client\n",
      "climate\n",
      "clinic\n",
      "clinically\n",
      "clinicaltrials\n",
      "clinician\n",
      "clinicopathologic\n",
      "clinicopathological\n",
      "clip\n",
      "clock\n",
      "clone\n",
      "close\n",
      "closed-loop\n",
      "closely\n",
      "closure\n",
      "clot\n",
      "cloth\n",
      "clothe\n",
      "cloud\n",
      "club\n",
      "clue\n",
      "cluster\n",
      "cm3\n",
      "cmh\n",
      "cmh2o\n",
      "co-evolution\n",
      "co-infection\n",
      "co-morbidities\n",
      "co-occurrence\n",
      "coach\n",
      "coagulation\n",
      "coagulopathy\n",
      "coalition\n",
      "coarse\n",
      "coastal\n",
      "coat\n",
      "code\n",
      "codon\n",
      "coefficient\n",
      "coevolution\n",
      "coexist\n",
      "coexistence\n",
      "cognate\n",
      "cognition\n",
      "cognitive\n",
      "cohen\n",
      "coherence\n",
      "coil\n",
      "coin\n",
      "coincidence\n",
      "coinfection\n",
      "colectomy\n",
      "colitis\n",
      "collaborate\n",
      "collaboration\n",
      "collaborative\n",
      "collaboratively\n",
      "collate\n",
      "collateral\n",
      "colleague\n",
      "collect\n",
      "collection\n",
      "collective\n",
      "collectively\n",
      "college\n",
      "collision\n",
      "colloidal\n",
      "colocalized\n",
      "colon\n",
      "colonization\n",
      "colony\n",
      "color\n",
      "colorectal\n",
      "colorimetric\n",
      "colour\n",
      "column\n",
      "com\n",
      "combat\n",
      "combination\n",
      "combinatorial\n",
      "combine\n",
      "combustion\n",
      "come\n",
      "comfort\n",
      "comfortable\n",
      "command\n",
      "commencement\n",
      "commensal\n",
      "comment\n",
      "commentary\n",
      "commercial\n",
      "commercialization\n",
      "commercially\n",
      "commission\n",
      "commit\n",
      "commitment\n",
      "committee\n",
      "commodity\n",
      "common\n",
      "commonality\n",
      "communicable\n",
      "communicate\n",
      "communication\n",
      "community\n",
      "community-acquired\n",
      "community-based\n",
      "community-dwelling\n",
      "commute\n",
      "comorbidities\n",
      "comorbidity\n",
      "compact\n",
      "companion\n",
      "company\n",
      "comparable\n",
      "comparative\n",
      "comparatively\n",
      "comparator\n",
      "compare\n",
      "comparison\n",
      "compartment\n",
      "compassion\n",
      "compassionate\n",
      "compatibility\n",
      "compatible\n",
      "compel\n",
      "compensate\n",
      "compensation\n",
      "compensatory\n",
      "compete\n",
      "competence\n",
      "competency\n",
      "competent\n",
      "competition\n",
      "competitive\n",
      "competitiveness\n",
      "competitor\n",
      "compile\n",
      "complaint\n",
      "complement\n",
      "complementary\n",
      "completely\n",
      "completeness\n",
      "completion\n",
      "complex\n",
      "complexity\n",
      "compliance\n",
      "compliant\n",
      "complicate\n",
      "complication\n",
      "comply\n",
      "component\n",
      "compose\n",
      "composite\n",
      "composition\n",
      "compositional\n",
      "comprehend\n",
      "comprehension\n",
      "comprehensive\n",
      "comprehensively\n",
      "compression\n",
      "compromise\n",
      "compulsory\n",
      "computation\n",
      "computational\n",
      "computationally\n",
      "compute\n",
      "computerize\n",
      "concentrate\n",
      "concentration\n",
      "concentration-dependent\n",
      "concept\n",
      "conception\n",
      "conceptual\n",
      "conceptualization\n",
      "conceptualize\n",
      "conceptually\n",
      "concern\n",
      "concert\n",
      "concise\n",
      "conclusion\n",
      "conclusive\n",
      "conclusively\n",
      "concomitant\n",
      "concomitantly\n",
      "concordance\n",
      "concordant\n",
      "concurrency\n",
      "concurrent\n",
      "concurrently\n",
      "condition\n",
      "conditional\n",
      "conducive\n",
      "conduct\n",
      "conductivity\n",
      "confer\n",
      "conference\n",
      "conferencing\n",
      "confidence\n",
      "confident\n",
      "confidentiality\n",
      "configuration\n",
      "configure\n",
      "confine\n",
      "confinement\n",
      "confirm\n",
      "confirmation\n",
      "confirmatory\n",
      "confluence\n",
      "confluent\n",
      "conform\n",
      "conformation\n",
      "confounders\n",
      "confuse\n",
      "confusion\n",
      "congenital\n",
      "congestion\n",
      "congregate\n",
      "conjecture\n",
      "conjugate\n",
      "conjugation\n",
      "conjunction\n",
      "connect\n",
      "connectedness\n",
      "connection\n",
      "connectivity\n",
      "conscious\n",
      "consecutive\n",
      "consecutively\n",
      "consensus\n",
      "consent\n",
      "consequence\n",
      "consequent\n",
      "consequential\n",
      "consequently\n",
      "conservation\n",
      "conservative\n",
      "conservatively\n",
      "considerable\n",
      "considerably\n",
      "consideration\n",
      "consist\n",
      "consistency\n",
      "consistent\n",
      "consistently\n",
      "console\n",
      "consolidate\n",
      "consolidation\n",
      "consortium\n",
      "conspiracy\n",
      "constantly\n",
      "constellation\n",
      "constituent\n",
      "constitute\n",
      "constitutional\n",
      "constitutive\n",
      "constitutively\n",
      "constrain\n",
      "constraint\n",
      "construct\n",
      "construction\n",
      "constructive\n",
      "consult\n",
      "consultant\n",
      "consultation\n",
      "consume\n",
      "consumption\n",
      "contagion\n",
      "contagious\n",
      "contain\n",
      "container\n",
      "containment\n",
      "contaminant\n",
      "contaminate\n",
      "contamination\n",
      "contemplate\n",
      "contemporary\n",
      "contend\n",
      "content\n",
      "contest\n",
      "context\n",
      "contextual\n",
      "contextualized\n",
      "continent\n",
      "contingency\n",
      "contingent\n",
      "continual\n",
      "continually\n",
      "continuation\n",
      "continuity\n",
      "continuous\n",
      "continuously\n",
      "continuum\n",
      "contour\n",
      "contraction\n",
      "contradiction\n",
      "contradictory\n",
      "contraindicate\n",
      "contraindication\n",
      "contribute\n",
      "contribution\n",
      "contributor\n",
      "controller\n",
      "controversial\n",
      "controversy\n",
      "convalescent\n",
      "convene\n",
      "convenience\n",
      "convenient\n",
      "convention\n",
      "conventional\n",
      "conventionally\n",
      "convergence\n",
      "convergent\n",
      "conversation\n",
      "conversely\n",
      "conversion\n",
      "convert\n",
      "convey\n",
      "convolutional\n",
      "cook\n",
      "cool\n",
      "cooperate\n",
      "cooperation\n",
      "cooperative\n",
      "coordinate\n",
      "coordination\n",
      "cope\n",
      "copies\n",
      "copy\n",
      "copyright\n",
      "cord\n",
      "core\n",
      "cornerstone\n",
      "corona\n",
      "coronavirus\n",
      "coronaviruses\n",
      "corporate\n",
      "corporation\n",
      "correct\n",
      "correction\n",
      "corrective\n",
      "correctly\n",
      "correctness\n",
      "correlate\n",
      "correlation\n",
      "correspond\n",
      "correspondence\n",
      "correspondingly\n",
      "corroborate\n",
      "corruption\n",
      "corticosteroid\n",
      "cost-effective\n",
      "cost-effectiveness\n",
      "costly\n",
      "cough\n",
      "counsel\n",
      "counter\n",
      "counteract\n",
      "counterbalance\n",
      "countermeasure\n",
      "counterpart\n",
      "country\n",
      "county\n",
      "couple\n",
      "course\n",
      "court\n",
      "covalently\n",
      "covariance\n",
      "covariate\n",
      "covariates\n",
      "cover\n",
      "covid-19-related\n",
      "cow\n",
      "craft\n",
      "crash\n",
      "creatinine\n",
      "creativity\n",
      "credibility\n",
      "crew\n",
      "crime\n",
      "crisis\n",
      "criterion\n",
      "critically\n",
      "criticism\n",
      "crohn\n",
      "crop\n",
      "cross\n",
      "cross-link\n",
      "cross-sectional\n",
      "cross-validation\n",
      "crossover\n",
      "crosstalk\n",
      "crowd\n",
      "crowdsourcing\n",
      "crucial\n",
      "crucially\n",
      "crude\n",
      "cruise\n",
      "cryptic\n",
      "crystal\n",
      "crystalline\n",
      "cue\n",
      "cuff\n",
      "cull\n",
      "culminate\n",
      "culprit\n",
      "cultivate\n",
      "cultivation\n",
      "culturally\n",
      "culture\n",
      "cumulative\n",
      "curated\n",
      "curb\n",
      "cure\n",
      "current\n",
      "currently\n",
      "curriculum\n",
      "curtail\n",
      "curvature\n",
      "curve\n",
      "custom\n",
      "custom-made\n",
      "customize\n",
      "cut\n",
      "cut-off\n",
      "cutoff\n",
      "cyber\n",
      "cyber-physical\n",
      "cybersecurity\n",
      "cycle\n",
      "cyclic\n",
      "cyst\n",
      "cystic\n",
      "cytometry\n",
      "cytopathic\n",
      "cytoplasm\n",
      "cytotoxic\n",
      "cytotoxicity\n",
      "czech\n",
      "d-dimer\n",
      "dairy\n",
      "damage\n",
      "damp\n",
      "dampen\n",
      "danger\n",
      "dangerous\n",
      "dark\n",
      "dashboard\n",
      "data-driven\n",
      "dataset\n",
      "day-to-day\n",
      "daytime\n",
      "dead\n",
      "deadly\n",
      "deal\n",
      "dearth\n",
      "debate\n",
      "debilitate\n",
      "debrief\n",
      "debris\n",
      "debt\n",
      "decade\n",
      "decay\n",
      "decease\n",
      "decentralize\n",
      "decidable\n",
      "decide\n",
      "decision-making\n",
      "declaration\n",
      "declare\n",
      "decline\n",
      "decode\n",
      "decompose\n",
      "decomposition\n",
      "decontamination\n",
      "deduce\n",
      "deem\n",
      "deep\n",
      "deepen\n",
      "deeply\n",
      "default\n",
      "defect\n",
      "defective\n",
      "defence\n",
      "defend\n",
      "defense\n",
      "defer\n",
      "deficiency\n",
      "deficient\n",
      "deficit\n",
      "define\n",
      "definitely\n",
      "definition\n",
      "definitive\n",
      "definitively\n",
      "deformation\n",
      "degenerate\n",
      "degradation\n",
      "degrade\n",
      "degree\n",
      "delete\n",
      "deleterious\n",
      "deliberate\n",
      "deliberation\n",
      "deliberative\n",
      "delineate\n",
      "delineation\n",
      "deliver\n",
      "delphi\n",
      "demand\n",
      "democracy\n",
      "democratic\n",
      "demographic\n",
      "demography\n",
      "demonstrate\n",
      "demonstration\n",
      "demyelination\n",
      "dengue\n",
      "denote\n",
      "dense\n",
      "densely\n",
      "dental\n",
      "deny\n",
      "department\n",
      "departure\n",
      "depend\n",
      "dependence\n",
      "dependency\n",
      "dependent\n",
      "depict\n",
      "deplete\n",
      "depletion\n",
      "deploy\n",
      "deployment\n",
      "deposit\n",
      "deposition\n",
      "depress\n",
      "depression\n",
      "depressive\n",
      "deprivation\n",
      "deprive\n",
      "depth\n",
      "derivation\n",
      "derivative\n",
      "derive\n",
      "derived\n",
      "descend\n",
      "descent\n",
      "description\n",
      "descriptive\n",
      "descriptor\n",
      "deserve\n",
      "design\n",
      "designate\n",
      "designation\n",
      "desirable\n",
      "desire\n",
      "despite\n",
      "destabilize\n",
      "destination\n",
      "destroy\n",
      "destruction\n",
      "destructive\n",
      "detachment\n",
      "detect\n",
      "detectable\n",
      "detection\n",
      "deter\n",
      "deteriorate\n",
      "deterioration\n",
      "determinant\n",
      "determination\n",
      "determine\n",
      "deterministic\n",
      "detrimental\n",
      "devastate\n",
      "develop\n",
      "development\n",
      "developmental\n",
      "deviate\n",
      "deviation\n",
      "devise\n",
      "devoid\n",
      "devote\n",
      "dexterity\n",
      "diagnostic\n",
      "diagnostics\n",
      "diagram\n",
      "diaphragm\n",
      "diarrhoea\n",
      "dichotomize\n",
      "dichotomy\n",
      "dictate\n",
      "didn\n",
      "dielectric\n",
      "diet\n",
      "dietary\n",
      "differ\n",
      "difference\n",
      "different\n",
      "differential\n",
      "differentially\n",
      "differentiate\n",
      "differentiation\n",
      "differently\n",
      "difficult\n",
      "difficulty\n",
      "diffuse\n",
      "diffusion\n",
      "digest\n",
      "digestion\n",
      "digestive\n",
      "digital\n",
      "digitalization\n",
      "digitization\n",
      "dignity\n",
      "dilate\n",
      "dilemma\n",
      "dilute\n",
      "dimension\n",
      "dimensional\n",
      "dimensionality\n",
      "diplomacy\n",
      "dire\n",
      "direct\n",
      "direction\n",
      "directive\n",
      "disability\n",
      "disable\n",
      "disadvantage\n",
      "disagreement\n",
      "disappear\n",
      "disappearance\n",
      "disappoint\n",
      "disassembly\n",
      "disastrous\n",
      "discard\n",
      "discern\n",
      "disciplinary\n",
      "discipline\n",
      "disclosure\n",
      "discomfort\n",
      "disconnect\n",
      "disconnection\n",
      "discontinuation\n",
      "discontinue\n",
      "discontinuous\n",
      "discordance\n",
      "discordant\n",
      "discourage\n",
      "discourse\n",
      "discovery\n",
      "discrepancy\n",
      "discriminant\n",
      "discriminate\n",
      "discrimination\n",
      "discriminative\n",
      "discriminatory\n",
      "discursive\n",
      "discus\n",
      "discuss\n",
      "discussion\n",
      "disease-free\n",
      "disease-specific\n",
      "disinfect\n",
      "disinfectant\n",
      "disinfection\n",
      "disinformation\n",
      "disparate\n",
      "disparity\n",
      "dispensable\n",
      "dispersal\n",
      "dispersion\n",
      "displacement\n",
      "display\n",
      "disposable\n",
      "disposition\n",
      "disproportionate\n",
      "disproportionately\n",
      "dispute\n",
      "disrupt\n",
      "disruption\n",
      "disruptive\n",
      "dissect\n",
      "dissection\n",
      "disseminate\n",
      "dissemination\n",
      "dissimilarity\n",
      "dissociation\n",
      "dissolution\n",
      "distal\n",
      "distant\n",
      "distantly\n",
      "distinction\n",
      "distinctive\n",
      "distinctly\n",
      "distinguish\n",
      "distort\n",
      "distortion\n",
      "distribute\n",
      "distribution\n",
      "disturb\n",
      "disturbance\n",
      "diurnal\n",
      "diverge\n",
      "divergence\n",
      "divergent\n",
      "diverse\n",
      "diversification\n",
      "diversify\n",
      "diversion\n",
      "diversity\n",
      "divert\n",
      "divide\n",
      "dock\n",
      "doctor\n",
      "documentation\n",
      "doff\n",
      "dog\n",
      "dollar\n",
      "domain-specific\n",
      "domesticate\n",
      "dominance\n",
      "domination\n",
      "donate\n",
      "donor\n",
      "door\n",
      "dope\n",
      "dosage\n",
      "dose-dependent\n",
      "dose-dependently\n",
      "dose-response\n",
      "double\n",
      "doubt\n",
      "downgrade\n",
      "downstream\n",
      "dozen\n",
      "draft\n",
      "drain\n",
      "dramatically\n",
      "drastic\n",
      "drastically\n",
      "draw\n",
      "drift\n",
      "drill\n",
      "drink\n",
      "drive\n",
      "driver\n",
      "drop\n",
      "droplet\n",
      "dropout\n",
      "drought\n",
      "drug\n",
      "drug-resistant\n",
      "dry\n",
      "dual\n",
      "duplex\n",
      "duplicate\n",
      "durability\n",
      "durable\n",
      "dust\n",
      "duty\n",
      "dwell\n",
      "dyad\n",
      "dye\n",
      "dynamic\n",
      "dynamical\n",
      "dynamically\n",
      "dysfunction\n",
      "dysfunctional\n",
      "dysphagia\n",
      "dysplasia\n",
      "dyspnea\n",
      "dysregulated\n",
      "dysregulation\n",
      "e-learning\n",
      "early\n",
      "early-stage\n",
      "earn\n",
      "earth\n",
      "earthquake\n",
      "ease\n",
      "easily\n",
      "east\n",
      "easy\n",
      "easy-to-use\n",
      "ecological\n",
      "ecology\n",
      "economical\n",
      "economically\n",
      "economics\n",
      "economy\n",
      "ecosystem\n",
      "edema\n",
      "edge\n",
      "edit\n",
      "edition\n",
      "editor\n",
      "editorial\n",
      "educate\n",
      "educational\n",
      "effective\n",
      "effectively\n",
      "effectiveness\n",
      "efficacious\n",
      "efficiency\n",
      "efficient\n",
      "efficiently\n",
      "effort\n",
      "egg\n",
      "eighth\n",
      "eighty\n",
      "elaborate\n",
      "elaboration\n",
      "elapse\n",
      "elastic\n",
      "elasticity\n",
      "elderly\n",
      "electrical\n",
      "electricity\n",
      "electrochemical\n",
      "electrode\n",
      "electromagnetic\n",
      "electronic\n",
      "electronically\n",
      "electronics\n",
      "electrostatic\n",
      "elementary\n",
      "elevate\n",
      "elevation\n",
      "elicit\n",
      "elicitation\n",
      "eligibility\n",
      "eliminate\n",
      "elimination\n",
      "elite\n",
      "elongation\n",
      "elucidate\n",
      "elucidation\n",
      "elusive\n",
      "elute\n",
      "email\n",
      "embed\n",
      "embeddings\n",
      "embody\n",
      "embolization\n",
      "embrace\n",
      "embryo\n",
      "embryonic\n",
      "emerge\n",
      "emergence\n",
      "emergency\n",
      "emit\n",
      "emotion\n",
      "emotional\n",
      "empathy\n",
      "emphasise\n",
      "emphasize\n",
      "empiric\n",
      "empirical\n",
      "empirically\n",
      "employ\n",
      "employment\n",
      "empower\n",
      "empowerment\n",
      "enable\n",
      "enact\n",
      "encapsulate\n",
      "encapsulation\n",
      "encephalitis\n",
      "encephalopathy\n",
      "enclose\n",
      "enclosure\n",
      "encompass\n",
      "encounter\n",
      "encourage\n",
      "end-expiratory\n",
      "end-of-life\n",
      "end-stage\n",
      "end-tidal\n",
      "end-to-end\n",
      "endeavour\n",
      "endemic\n",
      "endocrine\n",
      "endocytosis\n",
      "endogenous\n",
      "endorse\n",
      "endoscopic\n",
      "endoscopically\n",
      "endoscopy\n",
      "endothelial\n",
      "endotracheal\n",
      "endovascular\n",
      "endow\n",
      "endurance\n",
      "endure\n",
      "enemy\n",
      "energetic\n",
      "energy\n",
      "enforce\n",
      "enforcement\n",
      "engage\n",
      "engagement\n",
      "engine\n",
      "england\n",
      "english\n",
      "enhance\n",
      "enhancement\n",
      "enjoy\n",
      "enlarge\n",
      "enrich\n",
      "enrichment\n",
      "enrol\n",
      "enroll\n",
      "enrollment\n",
      "enrolment\n",
      "ensue\n",
      "ensure\n",
      "entail\n",
      "enter\n",
      "enteral\n",
      "enteric\n",
      "enterovirus\n",
      "enterprise\n",
      "entertainment\n",
      "entire\n",
      "entitle\n",
      "entity\n",
      "entrepreneur\n",
      "entrepreneurial\n",
      "entrepreneurship\n",
      "entry\n",
      "enumerate\n",
      "envelop\n",
      "environment\n",
      "environmental\n",
      "environmentally\n",
      "enzymatic\n",
      "enzyme\n",
      "enzyme-linked\n",
      "epicenter\n",
      "epidemiologic\n",
      "epidemiological\n",
      "epidemiology\n",
      "epigenetic\n",
      "episode\n",
      "epithelial\n",
      "epithelium\n",
      "epitope\n",
      "epoch\n",
      "equal\n",
      "equally\n",
      "equilibrium\n",
      "equip\n",
      "equipment\n",
      "equitable\n",
      "equity\n",
      "equivalence\n",
      "equivalent\n",
      "equivocal\n",
      "eradicate\n",
      "eradication\n",
      "erosion\n",
      "erroneous\n",
      "escalation\n",
      "escherichia\n",
      "esophageal\n",
      "esophagectomy\n",
      "esophagus\n",
      "especially\n",
      "essay\n",
      "essential\n",
      "essentially\n",
      "establish\n",
      "establishment\n",
      "estate\n",
      "estimation\n",
      "ethic\n",
      "ethical\n",
      "ethically\n",
      "ethnic\n",
      "ethnicity\n",
      "etiologic\n",
      "etiological\n",
      "euclidean\n",
      "eukaryotic\n",
      "euro\n",
      "evacuation\n",
      "evade\n",
      "evaluation\n",
      "evaporation\n",
      "evasion\n",
      "evenly\n",
      "eventually\n",
      "evidence-based\n",
      "evident\n",
      "evoke\n",
      "evolution\n",
      "evolutionarily\n",
      "evolutionary\n",
      "evolve\n",
      "exacerbate\n",
      "exacerbation\n",
      "exact\n",
      "exactly\n",
      "exaggerate\n",
      "examination\n",
      "examine\n",
      "example\n",
      "exceed\n",
      "excellence\n",
      "excellent\n",
      "exception\n",
      "exceptional\n",
      "exceptionally\n",
      "excess\n",
      "excessive\n",
      "excise\n",
      "excitation\n",
      "excite\n",
      "exclude\n",
      "exclusion\n",
      "exclusive\n",
      "exclusively\n",
      "excrete\n",
      "excretion\n",
      "execute\n",
      "execution\n",
      "executive\n",
      "exemplify\n",
      "exert\n",
      "exhale\n",
      "exhaust\n",
      "exhaustion\n",
      "exhaustive\n",
      "exhibit\n",
      "exist\n",
      "existence\n",
      "existential\n",
      "exogenous\n",
      "exotic\n",
      "expand\n",
      "expect\n",
      "expectancy\n",
      "expectation\n",
      "expenditure\n",
      "expense\n",
      "expensive\n",
      "experience\n",
      "experiential\n",
      "experiment\n",
      "experimental\n",
      "experimentally\n",
      "expert\n",
      "expertise\n",
      "expiratory\n",
      "expire\n",
      "explain\n",
      "explainable\n",
      "explanation\n",
      "explanatory\n",
      "explicitly\n",
      "exploit\n",
      "exploitation\n",
      "exploration\n",
      "exploratory\n",
      "explore\n",
      "explosion\n",
      "explosive\n",
      "exponential\n",
      "exponentially\n",
      "export\n",
      "expose\n",
      "express\n",
      "expression\n",
      "expressive\n",
      "extant\n",
      "extend\n",
      "extension\n",
      "extensive\n",
      "extensively\n",
      "extent\n",
      "externally\n",
      "extinction\n",
      "extra\n",
      "extracellular\n",
      "extracorporeal\n",
      "extraction\n",
      "extraordinary\n",
      "extrapolate\n",
      "extrapolation\n",
      "extreme\n",
      "extremely\n",
      "extremity\n",
      "extrinsic\n",
      "extubation\n",
      "eye\n",
      "fabric\n",
      "fabricate\n",
      "fabrication\n",
      "face\n",
      "face-to-face\n",
      "facet\n",
      "facial\n",
      "facile\n",
      "facilitate\n",
      "facilitation\n",
      "facilitator\n",
      "facility-based\n",
      "fact\n",
      "factorial\n",
      "factorization\n",
      "factory\n",
      "faculty\n",
      "fail\n",
      "failure\n",
      "fairly\n",
      "fake\n",
      "fall\n",
      "false\n",
      "false-positive\n",
      "familiarity\n",
      "famous\n",
      "far\n",
      "farm\n",
      "fascinate\n",
      "fashion\n",
      "fatal\n",
      "father\n",
      "fatigue\n",
      "fault\n",
      "fauna\n",
      "favor\n",
      "favorable\n",
      "favorably\n",
      "favour\n",
      "favourable\n",
      "feasibility\n",
      "feature\n",
      "febrile\n",
      "fecal\n",
      "feces\n",
      "federal\n",
      "feed\n",
      "feel\n",
      "feline\n",
      "fell\n",
      "fellow\n",
      "fellowship\n",
      "felt\n",
      "femoral\n",
      "ferret\n",
      "fertility\n",
      "fetal\n",
      "fibrillation\n",
      "fibroblast\n",
      "field\n",
      "fifth\n",
      "fifty-four\n",
      "fifty-nine\n",
      "fifty-one\n",
      "fifty-seven\n",
      "fifty-six\n",
      "fifty-three\n",
      "fifty-two\n",
      "fig\n",
      "fight\n",
      "figure\n",
      "file\n",
      "filter\n",
      "filtration\n",
      "final\n",
      "finally\n",
      "finance\n",
      "financial\n",
      "financially\n",
      "fine\n",
      "fine-grained\n",
      "fine-tune\n",
      "fingerprint\n",
      "finish\n",
      "finite\n",
      "fio\n",
      "fio2\n",
      "firm\n",
      "firmly\n",
      "first-generation\n",
      "first-line\n",
      "first-order\n",
      "first-time\n",
      "first-year\n",
      "firstly\n",
      "fiscal\n",
      "fisher\n",
      "fistula\n",
      "fitness\n",
      "five-year\n",
      "flank\n",
      "flat\n",
      "flatten\n",
      "flavivirus\n",
      "flaw\n",
      "flexibility\n",
      "flight\n",
      "flock\n",
      "flood\n",
      "floor\n",
      "flora\n",
      "fluctuate\n",
      "fluctuation\n",
      "fluid\n",
      "fluorescence\n",
      "fluorescent\n",
      "flux\n",
      "fly\n",
      "focus\n",
      "fold\n",
      "follow\n",
      "follow-up\n",
      "food\n",
      "foodborne\n",
      "foot\n",
      "footprint\n",
      "forage\n",
      "force\n",
      "forefront\n",
      "foreign\n",
      "foresee\n",
      "foreseeable\n",
      "forest\n",
      "forge\n",
      "forget\n",
      "form\n",
      "formal\n",
      "formalism\n",
      "formalization\n",
      "formalize\n",
      "format\n",
      "formation\n",
      "formidable\n",
      "formula\n",
      "formulate\n",
      "formulation\n",
      "forth\n",
      "fortunately\n",
      "forty-eight\n",
      "forty-five\n",
      "forty-four\n",
      "forty-one\n",
      "forty-six\n",
      "forty-three\n",
      "forty-two\n",
      "forum\n",
      "fossil\n",
      "foster\n",
      "foundation\n",
      "foundational\n",
      "founder\n",
      "fourth\n",
      "fractional\n",
      "fractionate\n",
      "fractionation\n",
      "fragile\n",
      "fragility\n",
      "fragmentation\n",
      "frailty\n",
      "frame\n",
      "frameshift\n",
      "frameshifting\n",
      "framework\n",
      "france\n",
      "free\n",
      "free-living\n",
      "free-ranging\n",
      "freely\n",
      "freeze\n",
      "french\n",
      "frequently\n",
      "fresh\n",
      "freshwater\n",
      "friend\n",
      "front-line\n",
      "frontline\n",
      "fruit\n",
      "fuel\n",
      "fulfil\n",
      "fulfill\n",
      "full-length\n",
      "full-text\n",
      "full-time\n",
      "fully\n",
      "fulminant\n",
      "functional\n",
      "functionality\n",
      "functionalization\n",
      "functionalized\n",
      "functionally\n",
      "fund\n",
      "fundamental\n",
      "fundamentally\n",
      "fungal\n",
      "fungi\n",
      "furthermore\n",
      "fusion\n",
      "future\n",
      "gamble\n",
      "game\n",
      "gamification\n",
      "garden\n",
      "garner\n",
      "gastrectomy\n",
      "gastric\n",
      "gastroenteritis\n",
      "gastrointestinal\n",
      "gate\n",
      "gather\n",
      "gauge\n",
      "gear\n",
      "gender\n",
      "general\n",
      "generalise\n",
      "generalist\n",
      "generalizability\n",
      "generalizable\n",
      "generalization\n",
      "generalize\n",
      "generally\n",
      "generation\n",
      "generative\n",
      "generator\n",
      "generic\n",
      "genetic\n",
      "genetically\n",
      "genome\n",
      "genomic\n",
      "genotyped\n",
      "genotypic\n",
      "genotyping\n",
      "genuine\n",
      "genus\n",
      "geographic\n",
      "geographical\n",
      "geographically\n",
      "geography\n",
      "geometric\n",
      "geopolitical\n",
      "geospatial\n",
      "geriatric\n",
      "germ\n",
      "german\n",
      "germany\n",
      "germicidal\n",
      "gestation\n",
      "gestational\n",
      "ghana\n",
      "giant\n",
      "gilead\n",
      "girl\n",
      "github\n",
      "gland\n",
      "globalisation\n",
      "globalization\n",
      "globalize\n",
      "globe\n",
      "glue\n",
      "glycoprotein\n",
      "glycosylation\n",
      "goal\n",
      "goat\n",
      "golgi\n",
      "good\n",
      "google\n",
      "gov\n",
      "govern\n",
      "governance\n",
      "government\n",
      "governmental\n",
      "gradual\n",
      "gradually\n",
      "graft\n",
      "gram\n",
      "grammar\n",
      "grand\n",
      "grandparent\n",
      "grant\n",
      "graphic\n",
      "graphical\n",
      "grasp\n",
      "gray\n",
      "graze\n",
      "great\n",
      "greatly\n",
      "green\n",
      "greenhouse\n",
      "grey\n",
      "grid\n",
      "grief\n",
      "grind\n",
      "grip\n",
      "groin\n",
      "gross\n",
      "ground\n",
      "groundwater\n",
      "grow\n",
      "growth\n",
      "gsh\n",
      "guarantee\n",
      "guide\n",
      "guideline\n",
      "gut\n",
      "gynecologic\n",
      "h1n\n",
      "h2o\n",
      "h3n2\n",
      "h9n2\n",
      "habit\n",
      "habitat\n",
      "half\n",
      "half-life\n",
      "halt\n",
      "hamper\n",
      "handle\n",
      "haplotype\n",
      "happen\n",
      "harbor\n",
      "harbour\n",
      "hardly\n",
      "harm\n",
      "harmful\n",
      "harmonize\n",
      "harness\n",
      "harsh\n",
      "harvest\n",
      "hash\n",
      "hazard\n",
      "hazardous\n",
      "head\n",
      "heal\n",
      "health-care\n",
      "health-promoting\n",
      "health-related\n",
      "healthcare\n",
      "healthy\n",
      "hear\n",
      "heart\n",
      "heat\n",
      "heavily\n",
      "hedge\n",
      "helix\n",
      "helper\n",
      "helpful\n",
      "hematologic\n",
      "hematological\n",
      "hemisphere\n",
      "hemodynamic\n",
      "hemoglobin\n",
      "hemorrhage\n",
      "hemorrhagic\n",
      "heparin\n",
      "hepatectomy\n",
      "hepatic\n",
      "hepatocellular\n",
      "herd\n",
      "hereditary\n",
      "heritability\n",
      "hernia\n",
      "herpes\n",
      "heterogeneity\n",
      "heterogeneous\n",
      "heterogenous\n",
      "heterologous\n",
      "heterozygous\n",
      "heuristic\n",
      "hide\n",
      "hierarchical\n",
      "hierarchy\n",
      "high-dimensional\n",
      "high-dose\n",
      "high-flow\n",
      "high-frequency\n",
      "high-income\n",
      "high-level\n",
      "high-performance\n",
      "high-quality\n",
      "high-resolution\n",
      "high-risk\n",
      "high-throughput\n",
      "high-volume\n",
      "higher-order\n",
      "highlight\n",
      "highly\n",
      "hijack\n",
      "hind\n",
      "hint\n",
      "hip\n",
      "histologic\n",
      "histological\n",
      "histologically\n",
      "histology\n",
      "histopathologic\n",
      "histopathological\n",
      "histopathology\n",
      "historical\n",
      "historically\n",
      "history\n",
      "hold\n",
      "hole\n",
      "holiday\n",
      "home\n",
      "home-based\n",
      "homeostasis\n",
      "homeostatic\n",
      "homogenate\n",
      "homogeneity\n",
      "homogeneous\n",
      "homogenous\n",
      "homologous\n",
      "homozygous\n",
      "hong\n",
      "honorarium\n",
      "hop\n",
      "hope\n",
      "hopefully\n",
      "horizon\n",
      "horizontal\n",
      "horse\n",
      "hospital-acquired\n",
      "hospitalisation\n",
      "hospitalization\n",
      "hospitalize\n",
      "host\n",
      "host-pathogen\n",
      "host-virus\n",
      "hostile\n",
      "hot\n",
      "hotspot\n",
      "hourly\n",
      "house\n",
      "household\n",
      "http\n",
      "https\n",
      "hub\n",
      "huge\n",
      "humanitarian\n",
      "humoral\n",
      "hunt\n",
      "hurdle\n",
      "hurricane\n",
      "husbandry\n",
      "hybrid\n",
      "hybridization\n",
      "hybridize\n",
      "hydrolysis\n",
      "hydrolyze\n",
      "hydrophilic\n",
      "hydrophobic\n",
      "hydrophobicity\n",
      "hydroxychloroquine\n",
      "hygiene\n",
      "hyperglycemia\n",
      "hypertension\n",
      "hypotension\n",
      "hypothermia\n",
      "hypothesis\n",
      "hypothesise\n",
      "hypothesize\n",
      "hypothetical\n",
      "hypoxemia\n",
      "hypoxia\n",
      "hysterectomy\n",
      "iatrogenic\n",
      "ic50\n",
      "ideal\n",
      "ideally\n",
      "identical\n",
      "identifiable\n",
      "identification\n",
      "identifier\n",
      "ideological\n",
      "idiopathic\n",
      "ignore\n",
      "il-10\n",
      "illegal\n",
      "illicit\n",
      "illness\n",
      "illuminate\n",
      "illumination\n",
      "illustrate\n",
      "illustration\n",
      "illustrative\n",
      "image\n",
      "imagine\n",
      "imbalance\n",
      "imbalanced\n",
      "immediate\n",
      "immediately\n",
      "immense\n",
      "immigrant\n",
      "immigration\n",
      "immobilize\n",
      "immune\n",
      "immune-mediated\n",
      "immunization\n",
      "immunoassay\n",
      "immunocompetent\n",
      "immunocompromised\n",
      "immunodeficiency\n",
      "immunofluorescence\n",
      "immunogenic\n",
      "immunogenicity\n",
      "immunoglobulin\n",
      "immunohistochemical\n",
      "immunohistochemistry\n",
      "immunologic\n",
      "immunological\n",
      "immunologically\n",
      "immunology\n",
      "immunomodulatory\n",
      "immunosorbent\n",
      "immunosuppression\n",
      "immunosuppressive\n",
      "immunotherapy\n",
      "impact\n",
      "impactful\n",
      "impair\n",
      "impairment\n",
      "impart\n",
      "impedance\n",
      "impede\n",
      "impend\n",
      "imperative\n",
      "imperfect\n",
      "implant\n",
      "implantation\n",
      "implement\n",
      "implementation\n",
      "implicate\n",
      "implication\n",
      "imply\n",
      "import\n",
      "importance\n",
      "important\n",
      "importantly\n",
      "importation\n",
      "impose\n",
      "impossible\n",
      "imprecise\n",
      "imprecision\n",
      "impression\n",
      "impressive\n",
      "improve\n",
      "improvement\n",
      "impulsive\n",
      "in-hospital\n",
      "in-person\n",
      "inability\n",
      "inaccessible\n",
      "inaccurate\n",
      "inactivate\n",
      "inactivation\n",
      "inactivity\n",
      "inadequacy\n",
      "inadequate\n",
      "inadequately\n",
      "inadvertent\n",
      "inappropriate\n",
      "inbred\n",
      "incentive\n",
      "inception\n",
      "incidence\n",
      "incidental\n",
      "incidentally\n",
      "include\n",
      "inclusion\n",
      "inclusive\n",
      "income\n",
      "incompatible\n",
      "incomplete\n",
      "incompletely\n",
      "inconclusive\n",
      "inconsistency\n",
      "inconsistent\n",
      "incorporate\n",
      "incorporation\n",
      "incorrect\n",
      "increasingly\n",
      "increment\n",
      "incremental\n",
      "incubation\n",
      "incur\n",
      "independence\n",
      "independent\n",
      "independently\n",
      "indeterminate\n",
      "indian\n",
      "indication\n",
      "indicative\n",
      "indicator\n",
      "indigenous\n",
      "indirectly\n",
      "indispensable\n",
      "indistinguishable\n",
      "individual\n",
      "individualize\n",
      "individually\n",
      "indoor\n",
      "induce\n",
      "induced\n",
      "inducer\n",
      "inducible\n",
      "induction\n",
      "inductive\n",
      "industrial\n",
      "industrialization\n",
      "industrialize\n",
      "ineffective\n",
      "inefficiency\n",
      "inefficient\n",
      "inequality\n",
      "inert\n",
      "inevitable\n",
      "inevitably\n",
      "inexpensive\n",
      "infancy\n",
      "infant\n",
      "infarct\n",
      "infarction\n",
      "infect\n",
      "infectious\n",
      "infectiousness\n",
      "infective\n",
      "infectivity\n",
      "infer\n",
      "inference\n",
      "inferential\n",
      "infertility\n",
      "infiltrate\n",
      "infiltration\n",
      "inflammation\n",
      "inflammatory\n",
      "inflict\n",
      "inflow\n",
      "influential\n",
      "influx\n",
      "inform\n",
      "informant\n",
      "informatics\n",
      "information\n",
      "informational\n",
      "informative\n",
      "infrastructure\n",
      "infrequent\n",
      "infrequently\n",
      "infuse\n",
      "ingest\n",
      "ingestion\n",
      "ingredient\n",
      "inhabit\n",
      "inhabitant\n",
      "inhalation\n",
      "inhale\n",
      "inherently\n",
      "inherit\n",
      "inheritance\n",
      "inhibit\n",
      "inhibition\n",
      "inhibitor\n",
      "inhibitory\n",
      "initial\n",
      "initially\n",
      "initiation\n",
      "initiative\n",
      "initiator\n",
      "inject\n",
      "injection\n",
      "injure\n",
      "inlet\n",
      "innate\n",
      "inner\n",
      "innovation\n",
      "innovative\n",
      "inoculate\n",
      "inoculation\n",
      "inorganic\n",
      "inpatient\n",
      "input\n",
      "inquiry\n",
      "insect\n",
      "insecurity\n",
      "insert\n",
      "insertion\n",
      "inside\n",
      "insight\n",
      "insignificant\n",
      "insoluble\n",
      "inspect\n",
      "inspection\n",
      "inspiration\n",
      "inspiratory\n",
      "instability\n",
      "instal\n",
      "installation\n",
      "instant\n",
      "instantaneous\n",
      "instead\n",
      "institute\n",
      "institution\n",
      "institutional\n",
      "instruction\n",
      "instructional\n",
      "instrument\n",
      "instrumental\n",
      "instrumentation\n",
      "insufficiency\n",
      "insufficient\n",
      "insufficiently\n",
      "insulin\n",
      "insult\n",
      "insurance\n",
      "intake\n",
      "integer\n",
      "integrate\n",
      "integration\n",
      "integrative\n",
      "integrity\n",
      "intellectual\n",
      "intelligence\n",
      "intelligent\n",
      "intend\n",
      "intense\n",
      "intensification\n",
      "intensify\n",
      "intensity\n",
      "intensive\n",
      "intensively\n",
      "intent\n",
      "intention\n",
      "intention-to-treat\n",
      "intentional\n",
      "intentionally\n",
      "inter-rater\n",
      "interaction\n",
      "interactive\n",
      "intercept\n",
      "interconnect\n",
      "interconnection\n",
      "interdependence\n",
      "interdependent\n",
      "interdisciplinary\n",
      "interestingly\n",
      "interface\n",
      "interfacial\n",
      "interfere\n",
      "interference\n",
      "interferon\n",
      "intergenerational\n",
      "intergroup\n",
      "interim\n",
      "interlink\n",
      "intermediary\n",
      "intermediate\n",
      "intermittent\n",
      "internalization\n",
      "internalize\n",
      "internally\n",
      "international\n",
      "internationalization\n",
      "internationally\n",
      "internet-based\n",
      "interobserver\n",
      "interoperability\n",
      "interpersonal\n",
      "interplay\n",
      "interpolation\n",
      "interpret\n",
      "interpretability\n",
      "interpretable\n",
      "interpretation\n",
      "interprofessional\n",
      "interquartile\n",
      "interrelate\n",
      "interrelationship\n",
      "interrogate\n",
      "interrupt\n",
      "interruption\n",
      "intersect\n",
      "intersection\n",
      "interspecific\n",
      "interstitial\n",
      "intertwine\n",
      "interval-valued\n",
      "intervene\n",
      "intervention\n",
      "interventional\n",
      "interview\n",
      "intestinal\n",
      "intestine\n",
      "intimately\n",
      "intra\n",
      "intraclass\n",
      "intracorporeal\n",
      "intracranial\n",
      "intractable\n",
      "intranasal\n",
      "intraoperative\n",
      "intraoperatively\n",
      "intraperitoneal\n",
      "intravascular\n",
      "intravenous\n",
      "intravenously\n",
      "intricate\n",
      "intriguingly\n",
      "intrinsic\n",
      "intrinsically\n",
      "introduce\n",
      "introduction\n",
      "introductory\n",
      "intubation\n",
      "intuitive\n",
      "intuitively\n",
      "invade\n",
      "invaluable\n",
      "invariably\n",
      "invariance\n",
      "invariant\n",
      "invasiveness\n",
      "inverse\n",
      "inversely\n",
      "inversion\n",
      "invert\n",
      "invest\n",
      "investigate\n",
      "investigation\n",
      "investigational\n",
      "investigator\n",
      "investment\n",
      "invisible\n",
      "invite\n",
      "invoke\n",
      "involve\n",
      "involvement\n",
      "ion\n",
      "ionic\n",
      "ionize\n",
      "iran\n",
      "irradiate\n",
      "irradiation\n",
      "irregular\n",
      "irrelevant\n",
      "irrespective\n",
      "irreversible\n",
      "irrigation\n",
      "ischaemic\n",
      "isolate\n",
      "isolation\n",
      "isothermal\n",
      "issue\n",
      "iteration\n",
      "iterative\n",
      "iteratively\n",
      "jan\n",
      "janssen\n",
      "january\n",
      "japan\n",
      "jeopardize\n",
      "jet\n",
      "job\n",
      "john\n",
      "join\n",
      "joint\n",
      "jointly\n",
      "journal\n",
      "journey\n",
      "judge\n",
      "judicious\n",
      "july\n",
      "jump\n",
      "june\n",
      "jurisdiction\n",
      "justice\n",
      "justification\n",
      "justify\n",
      "kaplan\n",
      "kaplan-meier\n",
      "kappa\n",
      "kcal\n",
      "kda\n",
      "kernel\n",
      "key\n",
      "kidney\n",
      "kill\n",
      "kind\n",
      "kinetic\n",
      "kingdom\n",
      "knee\n",
      "knot\n",
      "know\n",
      "knowledge\n",
      "kong\n",
      "korea\n",
      "kpa\n",
      "label\n",
      "label-free\n",
      "labor\n",
      "laboratory\n",
      "labour\n",
      "lack\n",
      "lactate\n",
      "lake\n",
      "lamb\n",
      "land\n",
      "landscape\n",
      "laparoscopic\n",
      "laparoscopically\n",
      "laparoscopy\n",
      "laparotomy\n",
      "large\n",
      "large-scale\n",
      "largely\n",
      "lastly\n",
      "late\n",
      "late-onset\n",
      "latency\n",
      "latent\n",
      "laterality\n",
      "latin\n",
      "lattice\n",
      "launch\n",
      "lavage\n",
      "law\n",
      "lay\n",
      "layer\n",
      "layout\n",
      "lc-ms\n",
      "lead\n",
      "leader\n",
      "leadership\n",
      "leaf\n",
      "league\n",
      "leak\n",
      "lean\n",
      "learn\n",
      "learner\n",
      "learning-based\n",
      "leave\n",
      "lecture\n",
      "leg\n",
      "legacy\n",
      "legal\n",
      "legally\n",
      "legislative\n",
      "legitimacy\n",
      "legitimate\n",
      "leishmania\n",
      "leisure\n",
      "lend\n",
      "lengthy\n",
      "lens\n",
      "lesion\n",
      "lessen\n",
      "lesson\n",
      "let\n",
      "lethal\n",
      "lethality\n",
      "letter\n",
      "leukocyte\n",
      "lexical\n",
      "liability\n",
      "liberal\n",
      "liberation\n",
      "library\n",
      "license\n",
      "lie\n",
      "life-saving\n",
      "life-threatening\n",
      "lifecycle\n",
      "lifesaving\n",
      "lifestyle\n",
      "lift\n",
      "ligand\n",
      "light\n",
      "lightweight\n",
      "like\n",
      "likelihood\n",
      "likely\n",
      "likert\n",
      "limb\n",
      "limit\n",
      "limitation\n",
      "lineage\n",
      "linear\n",
      "linearity\n",
      "linguistic\n",
      "link\n",
      "linkage\n",
      "linked\n",
      "linker\n",
      "lipid\n",
      "liquid\n",
      "liss\n",
      "list\n",
      "listen\n",
      "liter\n",
      "literature\n",
      "litter\n",
      "little\n",
      "live\n",
      "live-attenuated\n",
      "livelihood\n",
      "liver\n",
      "livestock\n",
      "loan\n",
      "lobectomy\n",
      "local\n",
      "localization\n",
      "locally\n",
      "locate\n",
      "lock\n",
      "locus\n",
      "log\n",
      "logic\n",
      "logical\n",
      "logistical\n",
      "logistics\n",
      "london\n",
      "loneliness\n",
      "long\n",
      "long-distance\n",
      "long-lasting\n",
      "long-lived\n",
      "long-range\n",
      "long-standing\n",
      "long-term\n",
      "longer-term\n",
      "longevity\n",
      "longitudinal\n",
      "longstanding\n",
      "look\n",
      "loop\n",
      "lose\n",
      "lot\n",
      "love\n",
      "low-certainty\n",
      "low-grade\n",
      "low-income\n",
      "low-level\n",
      "low-quality\n",
      "low-resource\n",
      "lps-induced\n",
      "lumbar\n",
      "lymph\n",
      "lymphocyte\n",
      "lysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lytic\n",
      "macaque\n",
      "machine\n",
      "machinery\n",
      "macro\n",
      "macromolecule\n",
      "macrophage\n",
      "macroscopic\n",
      "magnify\n",
      "magnitude\n",
      "mail\n",
      "main\n",
      "mainly\n",
      "mainstream\n",
      "maintenance\n",
      "major\n",
      "make\n",
      "maker\n",
      "maladaptive\n",
      "malformation\n",
      "malfunction\n",
      "malicious\n",
      "malignancy\n",
      "malignant\n",
      "maltreatment\n",
      "mammal\n",
      "man\n",
      "manage\n",
      "manageable\n",
      "management\n",
      "manager\n",
      "managerial\n",
      "mandate\n",
      "mandatory\n",
      "manifestation\n",
      "manipulate\n",
      "manipulation\n",
      "mankind\n",
      "mann-whitney\n",
      "manner\n",
      "manual\n",
      "manufacture\n",
      "manuscript\n",
      "march\n",
      "margin\n",
      "marginalize\n",
      "marginally\n",
      "marine\n",
      "marital\n",
      "mark\n",
      "marker\n",
      "market\n",
      "markov\n",
      "marrow\n",
      "mask\n",
      "massive\n",
      "massively\n",
      "master\n",
      "match\n",
      "math\n",
      "mathematical\n",
      "mathematically\n",
      "mathematics\n",
      "maturation\n",
      "mature\n",
      "maximal\n",
      "maximally\n",
      "maybe\n",
      "meal\n",
      "mean-field\n",
      "meaningful\n",
      "measles\n",
      "measurable\n",
      "measure\n",
      "measurement\n",
      "mechanical\n",
      "mechanically\n",
      "mechanism\n",
      "mechanistic\n",
      "mechanistically\n",
      "mediate\n",
      "mediation\n",
      "medically\n",
      "medication\n",
      "medicinal\n",
      "medium\n",
      "medium-sized\n",
      "medium-term\n",
      "meet\n",
      "meier\n",
      "melt\n",
      "member\n",
      "membership\n",
      "meningitis\n",
      "mental\n",
      "mention\n",
      "mentor\n",
      "merck\n",
      "mere\n",
      "merely\n",
      "merge\n",
      "merit\n",
      "message\n",
      "meta-analyses\n",
      "meta-analysis\n",
      "metabolic\n",
      "metabolism\n",
      "metabolite\n",
      "metal\n",
      "metastasis\n",
      "metastatic\n",
      "meteorological\n",
      "meter\n",
      "method\n",
      "methodological\n",
      "methodology\n",
      "methylation\n",
      "meticulous\n",
      "metric\n",
      "metropolitan\n",
      "mexico\n",
      "micro\n",
      "micro-organism\n",
      "microarray\n",
      "microarrays\n",
      "microbe\n",
      "microbiological\n",
      "microbiology\n",
      "microbiome\n",
      "microbiomes\n",
      "microbiota\n",
      "microenvironment\n",
      "microfluidic\n",
      "microfluidics\n",
      "microglia\n",
      "micromolar\n",
      "microorganism\n",
      "microscope\n",
      "microscopic\n",
      "microscopy\n",
      "microstructure\n",
      "microvascular\n",
      "microwave\n",
      "mid-march\n",
      "middle\n",
      "middle-aged\n",
      "middle-income\n",
      "midst\n",
      "midterm\n",
      "migrant\n",
      "migrate\n",
      "migration\n",
      "migratory\n",
      "mild-to-moderate\n",
      "mildly\n",
      "milestone\n",
      "milieu\n",
      "milk\n",
      "millennium\n",
      "mimicry\n",
      "min-1\n",
      "mind\n",
      "miniaturize\n",
      "minimal\n",
      "minimally\n",
      "minimization\n",
      "minimum\n",
      "minor\n",
      "minute\n",
      "mirror\n",
      "mis-c\n",
      "misconception\n",
      "misinformation\n",
      "mislead\n",
      "miss\n",
      "mission\n",
      "mistake\n",
      "misunderstand\n",
      "misuse\n",
      "mitigation\n",
      "mitochondrial\n",
      "mitochondrion\n",
      "mixture\n",
      "ml-1\n",
      "mmol\n",
      "mobile\n",
      "mobilization\n",
      "modal\n",
      "mode\n",
      "moderate-quality\n",
      "moderate-to-severe\n",
      "moderately\n",
      "moderation\n",
      "moderator\n",
      "modern\n",
      "modest\n",
      "modifiable\n",
      "modification\n",
      "modify\n",
      "modular\n",
      "modulation\n",
      "modulator\n",
      "module\n",
      "modulus\n",
      "moiety\n",
      "molar\n",
      "molecular\n",
      "molecularly\n",
      "moment\n",
      "money\n",
      "monitor\n",
      "monkey\n",
      "monoclonal\n",
      "monolayer\n",
      "monophyletic\n",
      "monotherapy\n",
      "monovalent\n",
      "monte\n",
      "monthly\n",
      "mood\n",
      "moral\n",
      "morbid\n",
      "morbidity\n",
      "morning\n",
      "morphologic\n",
      "morphological\n",
      "morphologically\n",
      "morphology\n",
      "mosaic\n",
      "mother\n",
      "motif\n",
      "motion\n",
      "motivation\n",
      "motor\n",
      "mount\n",
      "mouse\n",
      "movie\n",
      "mucosa\n",
      "mucosal\n",
      "multi\n",
      "multi-agent\n",
      "multi-disciplinary\n",
      "multi-organ\n",
      "multicenter\n",
      "multidimensional\n",
      "multidisciplinary\n",
      "multidrug-resistant\n",
      "multifaceted\n",
      "multifactorial\n",
      "multifocal\n",
      "multifunctional\n",
      "multilevel\n",
      "multilingual\n",
      "multimedia\n",
      "multimodal\n",
      "multimorbidity\n",
      "multinational\n",
      "multiplex\n",
      "multiplexed\n",
      "multiplication\n",
      "multiplicity\n",
      "multiport\n",
      "multitude\n",
      "multivalent\n",
      "multivariable\n",
      "multivariate\n",
      "municipal\n",
      "museum\n",
      "music\n",
      "mutagenesis\n",
      "mutant\n",
      "mutual\n",
      "mycobacterium\n",
      "mycoplasma\n",
      "myocardial\n",
      "myocarditis\n",
      "myriad\n",
      "n95\n",
      "nadir\n",
      "naive\n",
      "naked\n",
      "nanocomposite\n",
      "nanofibers\n",
      "nanomaterials\n",
      "nanomedicine\n",
      "nanomedicines\n",
      "nanomolar\n",
      "nanoparticle\n",
      "nanoparticles\n",
      "nanoscale\n",
      "nanostructured\n",
      "nanostructures\n",
      "nanotechnology\n",
      "narrative\n",
      "narratively\n",
      "narrow\n",
      "nascent\n",
      "nasopharyngeal\n",
      "nation\n",
      "nationalism\n",
      "nationally\n",
      "nationwide\n",
      "native\n",
      "natural\n",
      "naturally\n",
      "nature\n",
      "navigation\n",
      "near\n",
      "nearby\n",
      "nearly\n",
      "necessarily\n",
      "necessary\n",
      "necessitate\n",
      "necessity\n",
      "need\n",
      "needle\n",
      "negatively\n",
      "neglect\n",
      "negligible\n",
      "negotiate\n",
      "negotiation\n",
      "neighborhood\n",
      "neighbour\n",
      "neighbourhood\n",
      "neoadjuvant\n",
      "neonate\n",
      "nephrectomy\n",
      "nerve\n",
      "nervous\n",
      "netherlands\n",
      "network-based\n",
      "neural\n",
      "neurodegenerative\n",
      "neurodevelopmental\n",
      "neurologic\n",
      "neurological\n",
      "neuromuscular\n",
      "neuron\n",
      "neuroprotective\n",
      "neuropsychiatric\n",
      "neurosurgical\n",
      "neurotropic\n",
      "neutralization\n",
      "neutralize\n",
      "neutrophil\n",
      "new-onset\n",
      "newborn\n",
      "newly\n",
      "next-generation\n",
      "nexus\n",
      "niche\n",
      "nigeria\n",
      "night\n",
      "ninety\n",
      "nmol\n",
      "nocturnal\n",
      "nodule\n",
      "noise\n",
      "noisy\n",
      "nominal\n",
      "non\n",
      "non-covid-19\n",
      "non-human\n",
      "non-infected\n",
      "non-infectious\n",
      "non-inferiority\n",
      "non-invasive\n",
      "non-linear\n",
      "non-pathogenic\n",
      "non-pharmaceutical\n",
      "non-pharmacological\n",
      "non-severe\n",
      "non-significant\n",
      "non-specific\n",
      "non-survivors\n",
      "non-toxic\n",
      "non-trivial\n",
      "nonetheless\n",
      "noninfectious\n",
      "noninvasive\n",
      "nonlinear\n",
      "nonparametric\n",
      "nonsignificant\n",
      "nonspecific\n",
      "norm\n",
      "normalization\n",
      "normalize\n",
      "normative\n",
      "norovirus\n",
      "north\n",
      "nosocomial\n",
      "notable\n",
      "notably\n",
      "note\n",
      "noteworthy\n",
      "notice\n",
      "noticeable\n",
      "notification\n",
      "notify\n",
      "notion\n",
      "notwithstanding\n",
      "novel\n",
      "novelty\n",
      "novice\n",
      "novo\n",
      "np-hard\n",
      "nuance\n",
      "nuanced\n",
      "nuclear\n",
      "nucleic\n",
      "nucleotide\n",
      "nucleus\n",
      "null\n",
      "number\n",
      "numerical\n",
      "numerically\n",
      "nurture\n",
      "nutritional\n",
      "obese\n",
      "obesity\n",
      "object\n",
      "objective\n",
      "objectively\n",
      "objectivity\n",
      "obligation\n",
      "obligatory\n",
      "obliteration\n",
      "obscure\n",
      "observable\n",
      "observation\n",
      "observational\n",
      "observe\n",
      "obstetric\n",
      "obstruction\n",
      "obstructive\n",
      "obtain\n",
      "obviate\n",
      "obvious\n",
      "obviously\n",
      "occasional\n",
      "occasionally\n",
      "occlude\n",
      "occult\n",
      "occupancy\n",
      "occupation\n",
      "occupational\n",
      "occupy\n",
      "occur\n",
      "occurrence\n",
      "october\n",
      "ocular\n",
      "odds\n",
      "offer\n",
      "office\n",
      "officially\n",
      "offline\n",
      "offset\n",
      "offspring\n",
      "oil\n",
      "olfactory\n",
      "oligodendrocyte\n",
      "omics\n",
      "omit\n",
      "on-demand\n",
      "on-line\n",
      "on-site\n",
      "oncologic\n",
      "oncological\n",
      "one-third\n",
      "one-to-one\n",
      "one-way\n",
      "ongoing\n",
      "online\n",
      "onset\n",
      "ontological\n",
      "onward\n",
      "onwards\n",
      "open-ended\n",
      "open-label\n",
      "open-source\n",
      "operate\n",
      "operation\n",
      "operational\n",
      "operative\n",
      "ophthalmic\n",
      "opinion\n",
      "opioid\n",
      "opioids\n",
      "opportunistic\n",
      "opportunity\n",
      "oppose\n",
      "opposition\n",
      "optic\n",
      "optical\n",
      "optimally\n",
      "optimisation\n",
      "optimistic\n",
      "optimization\n",
      "optimum\n",
      "orchestrate\n",
      "ordinal\n",
      "ordination\n",
      "organ\n",
      "organelle\n",
      "organic\n",
      "organisation\n",
      "organisational\n",
      "organization\n",
      "organizational\n",
      "orient\n",
      "orientation\n",
      "origin\n",
      "originally\n",
      "originate\n",
      "orthopaedic\n",
      "orthopedic\n",
      "oscillatory\n",
      "oseltamivir\n",
      "outdoor\n",
      "outlet\n",
      "outlier\n",
      "outline\n",
      "outlook\n",
      "outpatient\n",
      "outperform\n",
      "output\n",
      "outside\n",
      "outsource\n",
      "outstanding\n",
      "ovarian\n",
      "overall\n",
      "overcrowd\n",
      "overestimate\n",
      "overlap\n",
      "overlay\n",
      "overlie\n",
      "overnight\n",
      "oversight\n",
      "overt\n",
      "overuse\n",
      "overwhelm\n",
      "owe\n",
      "owner\n",
      "ownership\n",
      "oxidation\n",
      "oxidative\n",
      "oxidize\n",
      "oxygenation\n",
      "ozone\n",
      "p-value\n",
      "p53\n",
      "pace\n",
      "pack\n",
      "package\n",
      "paco\n",
      "paediatric\n",
      "painful\n",
      "pair\n",
      "palliative\n",
      "pancreatic\n",
      "pancreatitis\n",
      "pandemic-related\n",
      "panel\n",
      "panic\n",
      "paper\n",
      "paper-based\n",
      "paracrine\n",
      "paradox\n",
      "paradoxical\n",
      "paradoxically\n",
      "parallelism\n",
      "parameterized\n",
      "parametric\n",
      "paramount\n",
      "parasitic\n",
      "parenchyma\n",
      "parent\n",
      "parenteral\n",
      "parity\n",
      "park\n",
      "parkinson\n",
      "parse\n",
      "partial\n",
      "partially\n",
      "participant\n",
      "participate\n",
      "participation\n",
      "participatory\n",
      "particular\n",
      "particularly\n",
      "particulate\n",
      "partition\n",
      "partly\n",
      "partner\n",
      "partnership\n",
      "party\n",
      "pass\n",
      "passage\n",
      "passenger\n",
      "passive\n",
      "passively\n",
      "past\n",
      "pasteur\n",
      "patch\n",
      "patchy\n",
      "patent\n",
      "path\n",
      "pathogen-free\n",
      "pathogenesis\n",
      "pathogenetic\n",
      "pathogenic\n",
      "pathogenicity\n",
      "pathologic\n",
      "pathological\n",
      "pathologically\n",
      "pathology\n",
      "pathophysiologic\n",
      "pathophysiological\n",
      "pathophysiology\n",
      "pathway\n",
      "patient-centered\n",
      "patient-days\n",
      "patient-reported\n",
      "patient-specific\n",
      "pattern\n",
      "paucity\n",
      "pause\n",
      "pave\n",
      "pay\n",
      "payer\n",
      "payment\n",
      "pdm09\n",
      "peace\n",
      "pearson\n",
      "pedagogical\n",
      "pediatric\n",
      "pediatrician\n",
      "pediatrics\n",
      "peer\n",
      "peer-reviewed\n",
      "pelvic\n",
      "pending\n",
      "penetrate\n",
      "penetration\n",
      "percentage\n",
      "percentile\n",
      "perception\n",
      "percolation\n",
      "percutaneous\n",
      "perfect\n",
      "perfectly\n",
      "perforate\n",
      "perforation\n",
      "perform\n",
      "performance\n",
      "perfusion\n",
      "peri-operative\n",
      "perinatal\n",
      "periodical\n",
      "periodically\n",
      "periodontal\n",
      "perioperative\n",
      "peripheral\n",
      "periprocedural\n",
      "peritoneal\n",
      "peritonitis\n",
      "permanent\n",
      "permanently\n",
      "permeability\n",
      "permissive\n",
      "permit\n",
      "perpetuate\n",
      "persist\n",
      "persistence\n",
      "persistent\n",
      "persistently\n",
      "person\n",
      "personalise\n",
      "personality\n",
      "personalize\n",
      "personnel\n",
      "perspective\n",
      "pertain\n",
      "pertinent\n",
      "perturb\n",
      "perturbation\n",
      "pertussis\n",
      "pervasive\n",
      "pest\n",
      "petri\n",
      "phage\n",
      "phantom\n",
      "pharmaceutical\n",
      "pharmacist\n",
      "pharmacodynamic\n",
      "pharmacokinetic\n",
      "pharmacologic\n",
      "pharmacological\n",
      "pharmacologically\n",
      "pharmacology\n",
      "phenomenon\n",
      "phenotype\n",
      "phenotypic\n",
      "phenotypically\n",
      "phenotyping\n",
      "philosophical\n",
      "philosophy\n",
      "phosphate\n",
      "phosphorylation\n",
      "photocatalytic\n",
      "photosynthetic\n",
      "phrase\n",
      "phylogenetic\n",
      "phylogenetically\n",
      "phylogeny\n",
      "physic\n",
      "physically\n",
      "physician\n",
      "physicochemical\n",
      "physiologic\n",
      "physiological\n",
      "physiologically\n",
      "physiology\n",
      "pi3k\n",
      "pick\n",
      "picture\n",
      "piece\n",
      "pig\n",
      "piglet\n",
      "pillar\n",
      "pilot\n",
      "pioneer\n",
      "pitfall\n",
      "pivot\n",
      "pivotal\n",
      "place\n",
      "placebo\n",
      "placement\n",
      "plague\n",
      "plain\n",
      "plan\n",
      "planar\n",
      "plane\n",
      "planet\n",
      "plant\n",
      "plaque\n",
      "plasma\n",
      "plasmid\n",
      "plasmonic\n",
      "plastic\n",
      "plasticity\n",
      "plat\n",
      "plate\n",
      "plausibility\n",
      "plausible\n",
      "play\n",
      "player\n",
      "pleiotropic\n",
      "pleural\n",
      "plot\n",
      "plug\n",
      "plume\n",
      "plus\n",
      "pneumococcal\n",
      "pneumoniae\n",
      "pneumonitis\n",
      "pneumothorax\n",
      "pocket\n",
      "point\n",
      "point-of-care\n",
      "poison\n",
      "poisson\n",
      "poland\n",
      "polar\n",
      "polarization\n",
      "police\n",
      "policy\n",
      "poliovirus\n",
      "political\n",
      "politically\n",
      "pollutant\n",
      "pollute\n",
      "pollution\n",
      "polyclonal\n",
      "polygenic\n",
      "polymer\n",
      "polymerase\n",
      "polymerization\n",
      "polymorphic\n",
      "polymorphism\n",
      "polynomial\n",
      "polypeptide\n",
      "pond\n",
      "pool\n",
      "poor\n",
      "poorly\n",
      "popular\n",
      "popularity\n",
      "population\n",
      "population-based\n",
      "porcine\n",
      "pore\n",
      "portability\n",
      "portal\n",
      "portfolio\n",
      "portion\n",
      "portray\n",
      "posit\n",
      "positively\n",
      "positivity\n",
      "posse\n",
      "possess\n",
      "possibility\n",
      "post-challenge\n",
      "post-covid\n",
      "post-covid-19\n",
      "post-exposure\n",
      "post-infection\n",
      "post-intervention\n",
      "post-mortem\n",
      "post-operative\n",
      "post-operatively\n",
      "post-pandemic\n",
      "post-stroke\n",
      "post-surgery\n",
      "post-transplant\n",
      "post-treatment\n",
      "post-vaccination\n",
      "poster\n",
      "posterior\n",
      "postgraduate\n",
      "postinfection\n",
      "postintervention\n",
      "postnatal\n",
      "postoperative\n",
      "postoperatively\n",
      "postpartum\n",
      "postprandial\n",
      "postprocedure\n",
      "posttransplant\n",
      "posttreatment\n",
      "postulate\n",
      "posture\n",
      "potency\n",
      "potent\n",
      "potential\n",
      "potentially\n",
      "potentiate\n",
      "poultry\n",
      "poverty\n",
      "powder\n",
      "power-law\n",
      "practical\n",
      "practicality\n",
      "practically\n",
      "practitioner\n",
      "pragmatic\n",
      "pre-clinical\n",
      "pre-covid-19\n",
      "pre-defined\n",
      "pre-existing\n",
      "pre-lockdown\n",
      "pre-operative\n",
      "pre-pandemic\n",
      "pre-specified\n",
      "pre-trained\n",
      "pre-treatment\n",
      "precaution\n",
      "precautionary\n",
      "precede\n",
      "precipitate\n",
      "precipitation\n",
      "precise\n",
      "precisely\n",
      "precision\n",
      "preclinical\n",
      "precondition\n",
      "precursor\n",
      "predefined\n",
      "predetermine\n",
      "predicate\n",
      "predict\n",
      "predictability\n",
      "predictable\n",
      "prediction\n",
      "predictive\n",
      "predictor\n",
      "predispose\n",
      "predisposition\n",
      "predominance\n",
      "predominant\n",
      "predominantly\n",
      "predominate\n",
      "preemptive\n",
      "prefer\n",
      "preferable\n",
      "preferably\n",
      "preference\n",
      "preferential\n",
      "preferentially\n",
      "preform\n",
      "prehospital\n",
      "preliminary\n",
      "premature\n",
      "prematurely\n",
      "premise\n",
      "premorbid\n",
      "preoperative\n",
      "preoperatively\n",
      "preparation\n",
      "prepare\n",
      "preparedness\n",
      "preprocessing\n",
      "prerequisite\n",
      "preschool\n",
      "prescribe\n",
      "prescription\n",
      "present\n",
      "presentation\n",
      "presently\n",
      "preservation\n",
      "president\n",
      "presidential\n",
      "prespecified\n",
      "presumably\n",
      "presume\n",
      "presumptive\n",
      "pretreated\n",
      "pretreatment\n",
      "prevail\n",
      "prevalence\n",
      "prevalent\n",
      "preventable\n",
      "preventative\n",
      "prevention\n",
      "preventive\n",
      "previously\n",
      "prey\n",
      "price\n",
      "prim\n",
      "primarily\n",
      "primary\n",
      "primate\n",
      "prime\n",
      "prime-boost\n",
      "primer\n",
      "primitive\n",
      "principal\n",
      "principally\n",
      "principle\n",
      "printer\n",
      "prior\n",
      "priori\n",
      "prioritise\n",
      "prioritization\n",
      "prioritize\n",
      "privacy\n",
      "private\n",
      "privilege\n",
      "prize\n",
      "pro-inflammatory\n",
      "proactive\n",
      "proactively\n",
      "probabilistic\n",
      "probability\n",
      "probiotic\n",
      "problematic\n",
      "procedural\n",
      "procedure\n",
      "procedure-related\n",
      "proceed\n",
      "processor\n",
      "procurement\n",
      "produce\n",
      "product\n",
      "production\n",
      "productive\n",
      "productivity\n",
      "prof\n",
      "profession\n",
      "professional\n",
      "professionalism\n",
      "proficiency\n",
      "profile\n",
      "profit\n",
      "profoundly\n",
      "progeny\n",
      "prognosis\n",
      "prognostic\n",
      "programme\n",
      "progression\n",
      "progression-free\n",
      "progressive\n",
      "progressively\n",
      "proinflammatory\n",
      "project\n",
      "projection\n",
      "proliferation\n",
      "proliferative\n",
      "prolong\n",
      "prolongation\n",
      "prominent\n",
      "prominently\n",
      "promise\n",
      "promote\n",
      "promotion\n",
      "prompt\n",
      "prone\n",
      "pronounce\n",
      "proof\n",
      "proof-of-concept\n",
      "propagate\n",
      "propagation\n",
      "propensity\n",
      "proper\n",
      "prophylactic\n",
      "prophylaxis\n",
      "proportion\n",
      "proportional\n",
      "propose\n",
      "proposition\n",
      "prospective\n",
      "prospectively\n",
      "prosthesis\n",
      "prosthetic\n",
      "protect\n",
      "protection\n",
      "protective\n",
      "protest\n",
      "prototype\n",
      "prove\n",
      "provide\n",
      "provision\n",
      "provisional\n",
      "provoke\n",
      "proximity\n",
      "proxy\n",
      "pseudoknot\n",
      "pseudomonas\n",
      "psoriasis\n",
      "psychiatric\n",
      "psychological\n",
      "psychology\n",
      "psychometric\n",
      "psychosocial\n",
      "public\n",
      "publication\n",
      "publish\n",
      "pubmed\n",
      "pull\n",
      "pulmonary\n",
      "pulse\n",
      "pump\n",
      "pup\n",
      "pure\n",
      "purely\n",
      "purification\n",
      "purify\n",
      "purity\n",
      "purpose\n",
      "pursue\n",
      "pursuit\n",
      "push\n",
      "puzzle\n",
      "pylorus\n",
      "qrt-pcr\n",
      "quadratic\n",
      "qualify\n",
      "qualitative\n",
      "qualitatively\n",
      "quality-adjusted\n",
      "quality-of-life\n",
      "quantification\n",
      "quantitation\n",
      "quantitative\n",
      "quantitatively\n",
      "quantum\n",
      "quarantine\n",
      "quarterly\n",
      "quench\n",
      "query\n",
      "quest\n",
      "questionable\n",
      "questionnaire\n",
      "quick\n",
      "quickly\n",
      "quit\n",
      "quite\n",
      "quo\n",
      "rabbit\n",
      "rabies\n",
      "racial\n",
      "racism\n",
      "radial\n",
      "radical\n",
      "radio\n",
      "radioactive\n",
      "radiofrequency\n",
      "radiograph\n",
      "radiographic\n",
      "radiolabeled\n",
      "radiologic\n",
      "radiological\n",
      "radiologist\n",
      "radiotherapy\n",
      "radius\n",
      "rain\n",
      "raise\n",
      "ramification\n",
      "random\n",
      "random-effects\n",
      "randomise\n",
      "randomization\n",
      "randomize\n",
      "randomness\n",
      "rank\n",
      "rankin\n",
      "rapid\n",
      "rapidly\n",
      "rarely\n",
      "rarity\n",
      "rat\n",
      "ration\n",
      "rationale\n",
      "rationality\n",
      "rationally\n",
      "ray\n",
      "re-emerge\n",
      "re-emergence\n",
      "reachability\n",
      "react\n",
      "reactivate\n",
      "reactivation\n",
      "reactivity\n",
      "reactor\n",
      "reader\n",
      "readily\n",
      "readiness\n",
      "readmission\n",
      "ready\n",
      "reagent\n",
      "real-life\n",
      "real-time\n",
      "real-world\n",
      "realise\n",
      "realistic\n",
      "realization\n",
      "realize\n",
      "really\n",
      "realm\n",
      "rear\n",
      "rearrangement\n",
      "reason\n",
      "reasonable\n",
      "reasonably\n",
      "rebound\n",
      "recall\n",
      "recanalization\n",
      "recapitulate\n",
      "receipt\n",
      "receive\n",
      "recent\n",
      "recession\n",
      "recipient\n",
      "reciprocal\n",
      "recognise\n",
      "recognition\n",
      "recognize\n",
      "recombinant\n",
      "recombination\n",
      "recommend\n",
      "recommendation\n",
      "reconcile\n",
      "reconsider\n",
      "reconstitute\n",
      "reconstitution\n",
      "reconstruct\n",
      "reconstruction\n",
      "reconstructive\n",
      "record\n",
      "recover\n",
      "recreation\n",
      "recreational\n",
      "recruit\n",
      "recruitment\n",
      "rectal\n",
      "rectum\n",
      "recur\n",
      "recurrence\n",
      "recurrence-free\n",
      "recurrent\n",
      "recursive\n",
      "recycle\n",
      "redistribution\n",
      "redox\n",
      "reduce\n",
      "reduction\n",
      "redundancy\n",
      "redundant\n",
      "reemergence\n",
      "reemerging\n",
      "ref\n",
      "refer\n",
      "reference\n",
      "refine\n",
      "refinement\n",
      "reflect\n",
      "reflection\n",
      "reflective\n",
      "reflex\n",
      "reflux\n",
      "reform\n",
      "refractory\n",
      "refugee\n",
      "refusal\n",
      "refuse\n",
      "regard\n",
      "regardless\n",
      "regenerate\n",
      "regeneration\n",
      "regenerative\n",
      "regime\n",
      "regimen\n",
      "registration\n",
      "regress\n",
      "regression\n",
      "regular\n",
      "regularly\n",
      "regulation\n",
      "regulator\n",
      "regulatory\n",
      "rehabilitation\n",
      "reimbursement\n",
      "reinforce\n",
      "reinforcement\n",
      "reject\n",
      "rejection\n",
      "relapse\n",
      "relate\n",
      "related\n",
      "relatedness\n",
      "relational\n",
      "relationship\n",
      "relatively\n",
      "relax\n",
      "relaxation\n",
      "release\n",
      "relevance\n",
      "reliability\n",
      "relief\n",
      "relieve\n",
      "religious\n",
      "relocation\n",
      "reluctance\n",
      "reluctant\n",
      "rely\n",
      "remain\n",
      "remainder\n",
      "remark\n",
      "remarkable\n",
      "remarkably\n",
      "remdesivir\n",
      "remedy\n",
      "remind\n",
      "remission\n",
      "remnant\n",
      "remodel\n",
      "remote\n",
      "removal\n",
      "remove\n",
      "renal\n",
      "render\n",
      "renew\n",
      "renewable\n",
      "reopen\n",
      "reoperation\n",
      "reorganization\n",
      "repeat\n",
      "repeatability\n",
      "repeatedly\n",
      "repercussion\n",
      "reperfusion\n",
      "repertoire\n",
      "repetition\n",
      "repetitive\n",
      "replace\n",
      "replacement\n",
      "replicate\n",
      "replication\n",
      "report\n",
      "reportedly\n",
      "reposition\n",
      "repository\n",
      "represent\n",
      "representation\n",
      "representative\n",
      "repress\n",
      "reproduce\n",
      "reproducibility\n",
      "reproducible\n",
      "reproduction\n",
      "reproductive\n",
      "repurposed\n",
      "repurposing\n",
      "request\n",
      "require\n",
      "requirement\n",
      "rescue\n",
      "researcher\n",
      "resect\n",
      "reserve\n",
      "reshape\n",
      "reside\n",
      "residence\n",
      "residency\n",
      "residential\n",
      "resilience\n",
      "resiliency\n",
      "resilient\n",
      "resist\n",
      "resistance\n",
      "resistive\n",
      "resolution\n",
      "resolve\n",
      "resonance\n",
      "resort\n",
      "resource-limited\n",
      "resource-poor\n",
      "respect\n",
      "respective\n",
      "respectively\n",
      "respirator\n",
      "respiratory\n",
      "respond\n",
      "respondent\n",
      "responder\n",
      "responsibility\n",
      "responsible\n",
      "responsive\n",
      "responsiveness\n",
      "restart\n",
      "restoration\n",
      "restore\n",
      "restraint\n",
      "restriction\n",
      "restrictive\n",
      "restructure\n",
      "result\n",
      "resultant\n",
      "results\n",
      "resume\n",
      "resumption\n",
      "resurgence\n",
      "resuscitation\n",
      "retail\n",
      "retain\n",
      "retention\n",
      "rethink\n",
      "retinal\n",
      "retract\n",
      "retreat\n",
      "retriever\n",
      "retrospective\n",
      "retrospectively\n",
      "retrovirus\n",
      "return\n",
      "reuse\n",
      "revascularization\n",
      "reveal\n",
      "revenue\n",
      "reverse\n",
      "reversible\n",
      "reversion\n",
      "revert\n",
      "review\n",
      "revise\n",
      "revisit\n",
      "revolution\n",
      "revolutionize\n",
      "reward\n",
      "rewire\n",
      "rewrite\n",
      "rheumatic\n",
      "rhinovirus\n",
      "rho\n",
      "rhythm\n",
      "ribavirin\n",
      "ribosomal\n",
      "rice\n",
      "rig-i\n",
      "right\n",
      "rigid\n",
      "rigorously\n",
      "ring\n",
      "rise\n",
      "risky\n",
      "river\n",
      "road\n",
      "roadmap\n",
      "robot\n",
      "robot-assisted\n",
      "robotic\n",
      "robotic-assisted\n",
      "robust\n",
      "robustness\n",
      "roche\n",
      "rod\n",
      "rodent\n",
      "role\n",
      "roll\n",
      "roost\n",
      "root\n",
      "rotate\n",
      "rotavirus\n",
      "rough\n",
      "roughly\n",
      "round\n",
      "rout\n",
      "route\n",
      "routine\n",
      "routinely\n",
      "row\n",
      "rrt-pcr\n",
      "rt-pcr\n",
      "rt-qpcr\n",
      "rule\n",
      "rule-based\n",
      "rumor\n",
      "rumour\n",
      "run\n",
      "runtime\n",
      "rupture\n",
      "rural\n",
      "russia\n",
      "sacrifice\n",
      "safeguard\n",
      "safely\n",
      "safety\n",
      "salary\n",
      "sale\n",
      "salient\n",
      "saliva\n",
      "salmonella\n",
      "salvage\n",
      "sample\n",
      "sampler\n",
      "sand\n",
      "sanitation\n",
      "sanitization\n",
      "sanofi\n",
      "sars-cov-2\n",
      "sars-cov2\n",
      "satellite\n",
      "satisfaction\n",
      "satisfactory\n",
      "satisfiability\n",
      "satisfy\n",
      "saturation\n",
      "saudi\n",
      "save\n",
      "saw\n",
      "say\n",
      "scalability\n",
      "scalar\n",
      "scale-free\n",
      "scant\n",
      "scarce\n",
      "scatter\n",
      "scene\n",
      "schema\n",
      "scheme\n",
      "schizophrenia\n",
      "scholar\n",
      "scholarly\n",
      "scholarship\n",
      "school\n",
      "school-based\n",
      "scientific\n",
      "scientifically\n",
      "scientist\n",
      "sclerosis\n",
      "scoping\n",
      "scopus\n",
      "screen\n",
      "script\n",
      "scrutinize\n",
      "seal\n",
      "search\n",
      "season\n",
      "seasonality\n",
      "seat\n",
      "second\n",
      "secondary\n",
      "secret\n",
      "secrete\n",
      "secretion\n",
      "secretory\n",
      "section\n",
      "sector\n",
      "secure\n",
      "sedentary\n",
      "seed\n",
      "seemingly\n",
      "segment\n",
      "segmentation\n",
      "segregate\n",
      "segregation\n",
      "seizure\n",
      "seldom\n",
      "select\n",
      "selection\n",
      "selective\n",
      "selectively\n",
      "selectivity\n",
      "self\n",
      "self-assembly\n",
      "self-care\n",
      "self-isolation\n",
      "self-limiting\n",
      "self-reported\n",
      "sell\n",
      "semantically\n",
      "semantics\n",
      "semi-quantitative\n",
      "semi-structured\n",
      "semi-supervised\n",
      "seminal\n",
      "semistructured\n",
      "send\n",
      "sense\n",
      "sensitivity\n",
      "sensitize\n",
      "sensor\n",
      "sensory\n",
      "sentiment\n",
      "separately\n",
      "separation\n",
      "sepsis\n",
      "september\n",
      "septic\n",
      "sequela\n",
      "sequential\n",
      "sequentially\n",
      "sequester\n",
      "serf\n",
      "serial\n",
      "series\n",
      "seriously\n",
      "seriousness\n",
      "serologic\n",
      "serological\n",
      "serologically\n",
      "seropositive\n",
      "seropositivity\n",
      "seroprevalence\n",
      "serotypes\n",
      "serve\n",
      "server\n",
      "service\n",
      "session\n",
      "settle\n",
      "settlement\n",
      "setup\n",
      "seven\n",
      "seventeen\n",
      "seventh\n",
      "seventy\n",
      "sever\n",
      "sewage\n",
      "sex\n",
      "sex-specific\n",
      "sexual\n",
      "shadow\n",
      "shake\n",
      "shall\n",
      "shallow\n",
      "sham\n",
      "shannon\n",
      "share\n",
      "shareholder\n",
      "sharp\n",
      "sharply\n",
      "shear\n",
      "shed\n",
      "sheep\n",
      "sheet\n",
      "shelf\n",
      "shell\n",
      "shelter\n",
      "shelter-in-place\n",
      "shield\n",
      "shift\n",
      "ship\n",
      "shock\n",
      "shoot\n",
      "shop\n",
      "short\n",
      "short-lived\n",
      "short-term\n",
      "shortcoming\n",
      "shorten\n",
      "shortly\n",
      "shrink\n",
      "shrinkage\n",
      "sibling\n",
      "sick\n",
      "sight\n",
      "signal\n",
      "signature\n",
      "significance\n",
      "significant\n",
      "significantly\n",
      "signify\n",
      "silence\n",
      "silent\n",
      "silico\n",
      "silver\n",
      "similar\n",
      "similarity\n",
      "similarly\n",
      "simplex\n",
      "simplicity\n",
      "simplification\n",
      "simply\n",
      "simulation\n",
      "simulation-based\n",
      "simulator\n",
      "simultaneous\n",
      "simultaneously\n",
      "sing\n",
      "singapore\n",
      "single\n",
      "single-cell\n",
      "single-center\n",
      "single-incision\n",
      "single-port\n",
      "single-site\n",
      "single-use\n",
      "sink\n",
      "sinus\n",
      "sister\n",
      "situ\n",
      "situate\n",
      "situation\n",
      "situational\n",
      "six-month\n",
      "sixteen\n",
      "sixth\n",
      "sixty-one\n",
      "sketch\n",
      "skew\n",
      "skill\n",
      "skilled\n",
      "skin\n",
      "skull\n",
      "sleep\n",
      "sleeve\n",
      "slice\n",
      "slide\n",
      "slight\n",
      "slope\n",
      "slow\n",
      "slowly\n",
      "sludge\n",
      "small-molecule\n",
      "smartphone\n",
      "smartphones\n",
      "smell\n",
      "smoke\n",
      "smoker\n",
      "smooth\n",
      "so-called\n",
      "society\n",
      "socio-demographic\n",
      "socio-ecological\n",
      "socio-economic\n",
      "sociodemographic\n",
      "socioeconomic\n",
      "sociological\n",
      "sociology\n",
      "sodium\n",
      "soil\n",
      "solar\n",
      "sole\n",
      "solely\n",
      "solid\n",
      "solidarity\n",
      "solitaire\n",
      "solubility\n",
      "soluble\n",
      "solve\n",
      "solvent\n",
      "solver\n",
      "somatic\n",
      "son\n",
      "soon\n",
      "sophisticate\n",
      "sort\n",
      "soundness\n",
      "source\n",
      "south\n",
      "southeast\n",
      "sovereignty\n",
      "sow\n",
      "sp-d\n",
      "span\n",
      "spanish\n",
      "spar\n",
      "spark\n",
      "sparse\n",
      "spatially\n",
      "spatio-temporal\n",
      "spatiotemporal\n",
      "speak\n",
      "speaker\n",
      "special\n",
      "specialise\n",
      "specialist\n",
      "specialization\n",
      "specialize\n",
      "specially\n",
      "specialty\n",
      "specie\n",
      "specifically\n",
      "specification\n",
      "specificity\n",
      "specify\n",
      "spectrometry\n",
      "spectroscopy\n",
      "speculate\n",
      "speech\n",
      "speed\n",
      "speedup\n",
      "spend\n",
      "spherical\n",
      "spike\n",
      "spill\n",
      "spillover\n",
      "spinal\n",
      "spiral\n",
      "spiritual\n",
      "spirituality\n",
      "spite\n",
      "spleen\n",
      "splenectomy\n",
      "splenic\n",
      "split\n",
      "spo2\n",
      "sponsor\n",
      "spontaneous\n",
      "spontaneously\n",
      "sporadically\n",
      "spore\n",
      "sport\n",
      "spot\n",
      "spotlight\n",
      "spouse\n",
      "spray\n",
      "spring\n",
      "spur\n",
      "square\n",
      "stability\n",
      "stabilization\n",
      "stabilize\n",
      "stable\n",
      "stably\n",
      "stack\n",
      "staff\n",
      "stage\n",
      "stain\n",
      "stake\n",
      "stakeholder\n",
      "stall\n",
      "stand\n",
      "standardise\n",
      "standardization\n",
      "standardize\n",
      "standpoint\n",
      "staphylococcus\n",
      "staple\n",
      "stark\n",
      "start\n",
      "state\n",
      "state-of-the-art\n",
      "statement\n",
      "statewide\n",
      "static\n",
      "station\n",
      "stationary\n",
      "statistic\n",
      "statistical\n",
      "statistically\n",
      "stay\n",
      "stay-at-home\n",
      "steady\n",
      "steady-state\n",
      "steep\n",
      "steer\n",
      "step-by-step\n",
      "sterility\n",
      "sterilization\n",
      "sterilize\n",
      "steroid\n",
      "stewardship\n",
      "stick\n",
      "stiffness\n",
      "stimulate\n",
      "stimulation\n",
      "stimulatory\n",
      "stochastic\n",
      "stock\n",
      "stockpile\n",
      "stomach\n",
      "stone\n",
      "stool\n",
      "stop\n",
      "storage\n",
      "store\n",
      "storm\n",
      "story\n",
      "straightforward\n",
      "strain-specific\n",
      "strand\n",
      "strange\n",
      "strategic\n",
      "strategically\n",
      "stratification\n",
      "stratum\n",
      "stream\n",
      "streamline\n",
      "street\n",
      "strengthen\n",
      "streptococcus\n",
      "stressful\n",
      "stretch\n",
      "strict\n",
      "strike\n",
      "strikingly\n",
      "string\n",
      "stringent\n",
      "strive\n",
      "stroke\n",
      "structural\n",
      "structurally\n",
      "structure\n",
      "student\n",
      "study\n",
      "style\n",
      "sub\n",
      "sub-saharan\n",
      "subarachnoid\n",
      "subclinical\n",
      "subcutaneous\n",
      "subdivide\n",
      "subject\n",
      "subjective\n",
      "subjectively\n",
      "submission\n",
      "submit\n",
      "suboptimal\n",
      "subscales\n",
      "subsequent\n",
      "subsequently\n",
      "subset\n",
      "subsp\n",
      "substance\n",
      "substantial\n",
      "substantially\n",
      "substantiate\n",
      "substantive\n",
      "substitute\n",
      "substitution\n",
      "substrate\n",
      "subtle\n",
      "subtropical\n",
      "subtype\n",
      "subunit\n",
      "subvert\n",
      "succeed\n",
      "success\n",
      "successful\n",
      "successfully\n",
      "succession\n",
      "successive\n",
      "succumb\n",
      "suckle\n",
      "suction\n",
      "sudden\n",
      "suffer\n",
      "sufficient\n",
      "sufficiently\n",
      "sugar\n",
      "suggest\n",
      "suggestion\n",
      "suggestive\n",
      "suicide\n",
      "suit\n",
      "suitability\n",
      "suite\n",
      "sulfate\n",
      "sum\n",
      "summarise\n",
      "summarization\n",
      "summarize\n",
      "summer\n",
      "sun\n",
      "superfamily\n",
      "superficial\n",
      "superimpose\n",
      "superiority\n",
      "supernatant\n",
      "supervise\n",
      "supervision\n",
      "supine\n",
      "supplement\n",
      "supplemental\n",
      "supplementary\n",
      "supplementation\n",
      "supply\n",
      "supportive\n",
      "suppose\n",
      "suppression\n",
      "suppressive\n",
      "suppressor\n",
      "surface\n",
      "surfactant\n",
      "surge\n",
      "surgeon\n",
      "surgically\n",
      "surpass\n",
      "surprisingly\n",
      "surrogate\n",
      "surveillance\n",
      "survey\n",
      "survive\n",
      "survivorship\n",
      "susceptibility\n",
      "susceptible\n",
      "suspect\n",
      "suspend\n",
      "suspension\n",
      "suspicion\n",
      "suspicious\n",
      "sustain\n",
      "sustainability\n",
      "sustainable\n",
      "suture\n",
      "swallow\n",
      "swarm\n",
      "sweden\n",
      "sweep\n",
      "swell\n",
      "swiftly\n",
      "swim\n",
      "swine\n",
      "switch\n",
      "symbiosis\n",
      "symbiotic\n",
      "symbol\n",
      "symmetric\n",
      "symposium\n",
      "symptom\n",
      "symptomatic\n",
      "synchronization\n",
      "synchronize\n",
      "synchronous\n",
      "synchrony\n",
      "syncytial\n",
      "syndromic\n",
      "synergism\n",
      "synergistic\n",
      "synergistically\n",
      "synergy\n",
      "synonym\n",
      "synonymous\n",
      "synthesis\n",
      "synthesise\n",
      "synthesize\n",
      "synthetic\n",
      "systematic\n",
      "systematically\n",
      "systemically\n",
      "t-cell\n",
      "t-test\n",
      "table\n",
      "tablet\n",
      "tack\n",
      "tackle\n",
      "tactic\n",
      "tactile\n",
      "tail\n",
      "tailor\n",
      "talk\n",
      "tandem\n",
      "tank\n",
      "taper\n",
      "taste\n",
      "tax\n",
      "taxon\n",
      "taxonomic\n",
      "tcid\n",
      "teach\n",
      "teacher\n",
      "tear\n",
      "technical\n",
      "technically\n",
      "technique\n",
      "technological\n",
      "technology\n",
      "telehealth\n",
      "telemedicine\n",
      "telephone\n",
      "television\n",
      "tell\n",
      "temperate\n",
      "temperature\n",
      "temporally\n",
      "temporarily\n",
      "temporary\n",
      "tend\n",
      "tenet\n",
      "tensile\n",
      "tensor\n",
      "tentatively\n",
      "terminate\n",
      "termination\n",
      "terminology\n",
      "terrestrial\n",
      "territorial\n",
      "territory\n",
      "terrorism\n",
      "terrorist\n",
      "test-retest\n",
      "text\n",
      "textile\n",
      "textual\n",
      "thank\n",
      "thematically\n",
      "theme\n",
      "theorem\n",
      "theoretical\n",
      "theoretically\n",
      "therapeutic\n",
      "therapeutically\n",
      "thereof\n",
      "thermal\n",
      "thermodynamic\n",
      "thesis\n",
      "thicken\n",
      "thickness\n",
      "thing\n",
      "think\n",
      "thirdly\n",
      "thirty\n",
      "thirty-eight\n",
      "thirty-five\n",
      "thirty-four\n",
      "thirty-nine\n",
      "thirty-one\n",
      "thirty-seven\n",
      "thirty-six\n",
      "thirty-three\n",
      "thirty-two\n",
      "thoracoscopic\n",
      "thoracotomy\n",
      "thoroughly\n",
      "thread\n",
      "threat\n",
      "three-dimensional\n",
      "three-year\n",
      "thrive\n",
      "thrombectomy\n",
      "thrombocytopenia\n",
      "thromboembolic\n",
      "thrombolysis\n",
      "thrombosis\n",
      "thrombotic\n",
      "throughput\n",
      "thyroid\n",
      "tidal\n",
      "tier\n",
      "tight\n",
      "tightly\n",
      "time-consuming\n",
      "time-dependent\n",
      "time-series\n",
      "time-varying\n",
      "timeframe\n",
      "timeliness\n",
      "timely\n",
      "tiny\n",
      "tip\n",
      "titer\n",
      "title\n",
      "titrate\n",
      "titration\n",
      "titre\n",
      "tocilizumab\n",
      "today\n",
      "tolerability\n",
      "toll\n",
      "tomography\n",
      "tone\n",
      "toolbox\n",
      "topic\n",
      "topical\n",
      "topological\n",
      "total\n",
      "totally\n",
      "touch\n",
      "tourism\n",
      "tourist\n",
      "toxic\n",
      "toxin\n",
      "trace\n",
      "tracer\n",
      "tracheostomy\n",
      "tractable\n",
      "trade\n",
      "trade-off\n",
      "tradition\n",
      "traditional\n",
      "traditionally\n",
      "traffic\n",
      "train\n",
      "trainee\n",
      "trait\n",
      "trajectory\n",
      "transaction\n",
      "transcribe\n",
      "transcript\n",
      "transcription\n",
      "transdisciplinary\n",
      "transduce\n",
      "transducer\n",
      "transduction\n",
      "transform\n",
      "transformation\n",
      "transformative\n",
      "transfusion\n",
      "transgenic\n",
      "transient\n",
      "transiently\n",
      "transit\n",
      "transition\n",
      "transitional\n",
      "transitivity\n",
      "translate\n",
      "translation\n",
      "translational\n",
      "transmembrane\n",
      "transmissibility\n",
      "transmissible\n",
      "transmission\n",
      "transnational\n",
      "transparency\n",
      "transparent\n",
      "transplant\n",
      "transplantation\n",
      "transport\n",
      "transportation\n",
      "trauma\n",
      "traumatic\n",
      "travel\n",
      "traveller\n",
      "treat\n",
      "treatable\n",
      "treated\n",
      "treaty\n",
      "tremendous\n",
      "triad\n",
      "triage\n",
      "trial\n",
      "triangulation\n",
      "trigger\n",
      "triple\n",
      "trivial\n",
      "trophic\n",
      "tropism\n",
      "trouble\n",
      "trough\n",
      "true\n",
      "truly\n",
      "trump\n",
      "trust\n",
      "trustworthy\n",
      "truth\n",
      "try\n",
      "trypsin\n",
      "tuberculosis\n",
      "tumour\n",
      "tune\n",
      "tunnel\n",
      "turbulent\n",
      "turing\n",
      "turkey\n",
      "turn\n",
      "turnaround\n",
      "tutor\n",
      "tweet\n",
      "twentieth\n",
      "twenty-eight\n",
      "twenty-first\n",
      "twenty-five\n",
      "twenty-four\n",
      "twenty-nine\n",
      "twenty-one\n",
      "twenty-seven\n",
      "twenty-six\n",
      "twenty-three\n",
      "twenty-two\n",
      "twice\n",
      "twin\n",
      "twitter\n",
      "two-dimensional\n",
      "two-stage\n",
      "two-thirds\n",
      "two-way\n",
      "twofold\n",
      "type\n",
      "typical\n",
      "typically\n",
      "ubiquitous\n",
      "ubiquitously\n",
      "uganda\n",
      "ulcer\n",
      "ultimately\n",
      "ultrasonic\n",
      "ultrasound\n",
      "ultraviolet\n",
      "umbilical\n",
      "unable\n",
      "unadjusted\n",
      "unaffected\n",
      "unanswered\n",
      "unanticipated\n",
      "unavailable\n",
      "unavoidable\n",
      "unaware\n",
      "uncertain\n",
      "uncertainty\n",
      "unchanged\n",
      "uncharacterized\n",
      "unclear\n",
      "uncoating\n",
      "uncomfortable\n",
      "uncomplicated\n",
      "uncontrolled\n",
      "unconventional\n",
      "uncover\n",
      "und\n",
      "undefined\n",
      "underestimate\n",
      "undergo\n",
      "undergraduate\n",
      "underline\n",
      "undermine\n",
      "underscore\n",
      "underserved\n",
      "understand\n",
      "understudy\n",
      "undertake\n",
      "underwater\n",
      "undesirable\n",
      "undetectable\n",
      "undetected\n",
      "undetermined\n",
      "undoubtedly\n",
      "unemployment\n",
      "unequal\n",
      "uneven\n",
      "uneventful\n",
      "unexpected\n",
      "unexpectedly\n",
      "unexplained\n",
      "unexplored\n",
      "unfamiliar\n",
      "unfavorable\n",
      "unfavourable\n",
      "unfold\n",
      "unfortunately\n",
      "unhealthy\n",
      "unidentified\n",
      "unidirectional\n",
      "uniform\n",
      "uniformity\n",
      "uniformly\n",
      "unify\n",
      "uninfected\n",
      "unintended\n",
      "uninterrupted\n",
      "union\n",
      "unique\n",
      "uniqueness\n",
      "unit\n",
      "unite\n",
      "unity\n",
      "univariable\n",
      "univariate\n",
      "universal\n",
      "universally\n",
      "university\n",
      "unknown\n",
      "unlabeled\n",
      "unlabelled\n",
      "unless\n",
      "unlike\n",
      "unmet\n",
      "unmodified\n",
      "unnecessary\n",
      "unplanned\n",
      "unprecedented\n",
      "unpredictable\n",
      "unprepared\n",
      "unprotected\n",
      "unpublished\n",
      "unravel\n",
      "unrecognized\n",
      "unrelated\n",
      "unreliable\n",
      "unreported\n",
      "unresolved\n",
      "unruptured\n",
      "unsafe\n",
      "unsatisfactory\n",
      "unselected\n",
      "unstructured\n",
      "unsuccessful\n",
      "unsuitable\n",
      "unsupervised\n",
      "untreated\n",
      "unusual\n",
      "unusually\n",
      "unvaccinated\n",
      "unveil\n",
      "up-to-date\n",
      "update\n",
      "upgrade\n",
      "upload\n",
      "upper\n",
      "upregulated\n",
      "upward\n",
      "urban\n",
      "urbanization\n",
      "urge\n",
      "urgency\n",
      "urinary\n",
      "urine\n",
      "url\n",
      "usability\n",
      "usable\n",
      "usage\n",
      "useful\n",
      "usefulness\n",
      "user-friendly\n",
      "usual\n",
      "usually\n",
      "uterine\n",
      "utilisation\n",
      "utilise\n",
      "utility\n",
      "utilization\n",
      "utilize\n",
      "utmost\n",
      "uv-c\n",
      "v-loc\n",
      "vaccinate\n",
      "vaccination\n",
      "vaginal\n",
      "vague\n",
      "valid\n",
      "validation\n",
      "valuation\n",
      "valve\n",
      "van\n",
      "vapor\n",
      "variability\n",
      "variant\n",
      "variety\n",
      "various\n",
      "vary\n",
      "vasculitis\n",
      "vast\n",
      "vastly\n",
      "vector\n",
      "vegetation\n",
      "vegetative\n",
      "vehicle\n",
      "venous\n",
      "ventilate\n",
      "ventilation\n",
      "ventilator\n",
      "ventilator-free\n",
      "ventilatory\n",
      "ventricular\n",
      "venue\n",
      "verbal\n",
      "verification\n",
      "verify\n",
      "vero\n",
      "versa\n",
      "versatile\n",
      "versatility\n",
      "version\n",
      "versus\n",
      "vertebrate\n",
      "vertex\n",
      "vesicle\n",
      "vessel\n",
      "veteran\n",
      "veterinary\n",
      "viability\n",
      "viable\n",
      "vibration\n",
      "vice\n",
      "victim\n",
      "video\n",
      "vietnam\n",
      "view\n",
      "viewpoint\n",
      "vigilance\n",
      "vigor\n",
      "viiv\n",
      "village\n",
      "violate\n",
      "violation\n",
      "violent\n",
      "virion\n",
      "virological\n",
      "virtual\n",
      "virtually\n",
      "virtue\n",
      "virulence\n",
      "virus-induced\n",
      "viscoelastic\n",
      "viscosity\n",
      "viscous\n",
      "visibility\n",
      "visible\n",
      "vision\n",
      "visitor\n",
      "visual\n",
      "visualisation\n",
      "visualization\n",
      "visualize\n",
      "vital\n",
      "vitamin\n",
      "vitro\n",
      "vivo\n",
      "viz\n",
      "vocabulary\n",
      "voice\n",
      "volatility\n",
      "voltage\n",
      "voluntarily\n",
      "voluntary\n",
      "volunteer\n",
      "vomit\n",
      "vote\n",
      "vulnerability\n",
      "vulnerable\n",
      "wage\n",
      "wait\n",
      "wake\n",
      "walk\n",
      "wall\n",
      "want\n",
      "war\n",
      "ward\n",
      "warfare\n",
      "warm\n",
      "warn\n",
      "warrant\n",
      "washington\n",
      "washout\n",
      "wastewater\n",
      "watch\n",
      "waterborne\n",
      "wave\n",
      "wavelength\n",
      "way\n",
      "weak\n",
      "weaken\n",
      "weakly\n",
      "wealth\n",
      "wealthy\n",
      "wean\n",
      "weapon\n",
      "wear\n",
      "weather\n",
      "web-based\n",
      "website\n",
      "week\n",
      "weekend\n",
      "weekly\n",
      "weigh\n",
      "welcome\n",
      "welfare\n",
      "well-being\n",
      "well-characterized\n",
      "well-defined\n",
      "well-designed\n",
      "well-documented\n",
      "well-established\n",
      "well-known\n",
      "well-studied\n",
      "well-tolerated\n",
      "wellbeing\n",
      "west\n",
      "western\n",
      "wet\n",
      "wetland\n",
      "wheeze\n",
      "whilst\n",
      "white\n",
      "wide\n",
      "widely\n",
      "widen\n",
      "widespread\n",
      "width\n",
      "wild\n",
      "wild-type\n",
      "wiley\n",
      "willingness\n",
      "win\n",
      "window\n",
      "wire\n",
      "wisdom\n",
      "wish\n",
      "withdrawal\n",
      "within-host\n",
      "withstand\n",
      "witness\n",
      "woman\n",
      "word\n",
      "work\n",
      "work-related\n",
      "worker\n",
      "workforce\n",
      "workplace\n",
      "world\n",
      "world-wide\n",
      "worldwide\n",
      "worry\n",
      "worsen\n",
      "worth\n",
      "worthwhile\n",
      "worthy\n",
      "wrap\n",
      "write\n",
      "wrong\n",
      "wuhan\n",
      "x-ray\n",
      "xenograft\n",
      "yearly\n",
      "yeast\n",
      "yellow\n",
      "yield\n",
      "york\n",
      "young\n",
      "youth\n",
      "zealand\n",
      "zero\n",
      "zika\n",
      "zinc\n",
      "zone\n",
      "zoom\n"
     ]
    }
   ],
   "source": [
    "# check tokens of the dictionary (Vocabulary)\n",
    "for v in sorted(count_vect.vocabulary_):\n",
    "    if len(v)!= len(\"c0014507\"):\n",
    "        print(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize the train and test documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = count_vect.build_tokenizer()\n",
    "texts = [word_tokenizer(doc) for doc in agg_df['prep_text']]\n",
    "test_texts = [word_tokenizer(doc) for doc in test_df['prep_text']]\n",
    "\n",
    "# tokenized_test_20 = [word_tokenizer(doc.lower()) for doc in twenty_test.data]\n",
    "# texts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build dataset and the topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhamzeia/Thesis/BiomedicalTopicModelling/contextualized_topic_models/models/ctm.py:51: DeprecationWarning: Direct call to CTM is deprecated and will be removed in version 2, use CombinedTM or ZeroShotTM\n",
      "  warnings.warn(\"Direct call to CTM is deprecated and will be removed in version 2, use CombinedTM or ZeroShotTM\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: \n",
      "                   N Components: 10\n",
      "                   Topic Prior Mean: 0.0\n",
      "                   Topic Prior Variance: 0.9\n",
      "                   Model Type: prodLDA\n",
      "                   Hidden Sizes: (100, 100)\n",
      "                   Activation: softplus\n",
      "                   Dropout: 0.2\n",
      "                   Learn Priors: True\n",
      "                   Learning Rate: 0.002\n",
      "                   Momentum: 0.99\n",
      "                   Reduce On Plateau: False\n",
      "                   Save Dir: saved_models/vocab_10k/\n",
      "Epoch: [1/250]\tSamples: [36726/9181500]\tTrain Loss: 670.7034984369044\tTime: 0:00:04.400834\n",
      "Epoch: [2/250]\tSamples: [73452/9181500]\tTrain Loss: 650.6053928075478\tTime: 0:00:04.357050\n",
      "Epoch: [3/250]\tSamples: [110178/9181500]\tTrain Loss: 648.2454989738196\tTime: 0:00:04.392791\n",
      "Epoch: [4/250]\tSamples: [146904/9181500]\tTrain Loss: 646.8304346503498\tTime: 0:00:04.439948\n",
      "Epoch: [5/250]\tSamples: [183630/9181500]\tTrain Loss: 646.16915234609\tTime: 0:00:04.439132\n",
      "Epoch: [6/250]\tSamples: [220356/9181500]\tTrain Loss: 645.4761491773539\tTime: 0:00:04.521132\n",
      "Epoch: [7/250]\tSamples: [257082/9181500]\tTrain Loss: 644.9481172653569\tTime: 0:00:04.468294\n",
      "Epoch: [8/250]\tSamples: [293808/9181500]\tTrain Loss: 644.8215529997482\tTime: 0:00:04.447111\n",
      "Epoch: [9/250]\tSamples: [330534/9181500]\tTrain Loss: 644.5962181296969\tTime: 0:00:04.492993\n",
      "Epoch: [10/250]\tSamples: [367260/9181500]\tTrain Loss: 644.3089185794805\tTime: 0:00:04.444305\n",
      "Epoch: [11/250]\tSamples: [403986/9181500]\tTrain Loss: 643.9159119220716\tTime: 0:00:04.441805\n",
      "Epoch: [12/250]\tSamples: [440712/9181500]\tTrain Loss: 643.901347989163\tTime: 0:00:04.359254\n",
      "Epoch: [13/250]\tSamples: [477438/9181500]\tTrain Loss: 643.759779877505\tTime: 0:00:04.444697\n",
      "Epoch: [14/250]\tSamples: [514164/9181500]\tTrain Loss: 643.7049738902616\tTime: 0:00:04.458359\n",
      "Epoch: [15/250]\tSamples: [550890/9181500]\tTrain Loss: 643.1873539650044\tTime: 0:00:04.539761\n",
      "Epoch: [16/250]\tSamples: [587616/9181500]\tTrain Loss: 643.2836488880494\tTime: 0:00:04.608658\n",
      "Epoch: [17/250]\tSamples: [624342/9181500]\tTrain Loss: 643.2126794964739\tTime: 0:00:04.582217\n",
      "Epoch: [18/250]\tSamples: [661068/9181500]\tTrain Loss: 643.1049388546262\tTime: 0:00:04.607915\n",
      "Epoch: [19/250]\tSamples: [697794/9181500]\tTrain Loss: 642.9546975618431\tTime: 0:00:04.641284\n",
      "Epoch: [20/250]\tSamples: [734520/9181500]\tTrain Loss: 643.1286982059712\tTime: 0:00:04.562887\n",
      "Epoch: [21/250]\tSamples: [771246/9181500]\tTrain Loss: 643.0669807110426\tTime: 0:00:04.587196\n",
      "Epoch: [22/250]\tSamples: [807972/9181500]\tTrain Loss: 643.1145962031327\tTime: 0:00:04.715811\n",
      "Epoch: [23/250]\tSamples: [844698/9181500]\tTrain Loss: 642.7819498618145\tTime: 0:00:04.638529\n",
      "Epoch: [24/250]\tSamples: [881424/9181500]\tTrain Loss: 642.848728804185\tTime: 0:00:04.638893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:14.489248 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [25/250]\tSamples: [918150/9181500]\tTrain Loss: 642.7324733541973\tTime: 0:00:04.690334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:15.313133 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:27:15.996283 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:27:16.803380 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:27:17.336635 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:27:17.337257 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:27:18.084076 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:27:18.744285 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:27:19.535193 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:27:20.051189 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:27:20.051871 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:27:20.797117 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:27:21.466742 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:27:22.262298 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:27:22.774314 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:27:22.775017 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:27:23.544736 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:27:24.213860 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:27:25.008578 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:27:25.525262 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:27:25.542191 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:27:26.108634 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (-36121 virtual)\n",
      "I0228 01:27:26.684826 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (-214280 virtual)\n",
      "I0228 01:27:26.715870 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (-224022 virtual)\n",
      "I0228 01:27:27.141390 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (-525873 virtual)\n",
      "I0228 01:27:27.145735 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (-524627 virtual)\n",
      "I0228 01:27:27.195168 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (-532060 virtual)\n",
      "I0228 01:27:27.323230 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (-560469 virtual)\n",
      "I0228 01:27:27.329210 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (-558641 virtual)\n",
      "I0228 01:27:27.337899 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (-556469 virtual)\n",
      "I0228 01:27:27.341289 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (-554796 virtual)\n",
      "I0228 01:27:27.345212 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (-553175 virtual)\n",
      "I0228 01:27:28.361543 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:27:28.364348 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:27:28.366230 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:27:28.365775 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:27:28.366927 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:27:28.376647 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:27:28.371643 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:27:28.369457 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:27:28.382815 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:27:28.368071 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:27:28.791134 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:27:28.813584 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 230241 virtual documents\n",
      "I0228 01:27:31.124369 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 1000 documents\n",
      "I0228 01:27:31.141760 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 2000 documents\n",
      "I0228 01:27:31.159109 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 3000 documents\n",
      "I0228 01:27:31.176498 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 4000 documents\n",
      "I0228 01:27:31.191956 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 5000 documents\n",
      "I0228 01:27:31.206573 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 6000 documents\n",
      "I0228 01:27:31.220944 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 7000 documents\n",
      "I0228 01:27:31.235487 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 8000 documents\n",
      "I0228 01:27:31.251547 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 9000 documents\n",
      "I0228 01:27:31.267910 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 10000 documents\n",
      "I0228 01:27:31.285525 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 11000 documents\n",
      "I0228 01:27:31.300367 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 12000 documents\n",
      "I0228 01:27:31.313521 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 13000 documents\n",
      "I0228 01:27:31.327047 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 14000 documents\n",
      "I0228 01:27:31.340581 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 15000 documents\n",
      "I0228 01:27:31.355108 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 16000 documents\n",
      "I0228 01:27:31.367666 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 17000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:31.380962 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 18000 documents\n",
      "I0228 01:27:31.394298 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 19000 documents\n",
      "I0228 01:27:31.408857 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 20000 documents\n",
      "I0228 01:27:31.424784 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 21000 documents\n",
      "I0228 01:27:31.441360 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 22000 documents\n",
      "I0228 01:27:31.459140 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 23000 documents\n",
      "I0228 01:27:31.484410 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 24000 documents\n",
      "I0228 01:27:31.501925 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 25000 documents\n",
      "I0228 01:27:31.519025 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 26000 documents\n",
      "I0228 01:27:31.537003 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 27000 documents\n",
      "I0228 01:27:31.554659 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 28000 documents\n",
      "I0228 01:27:31.572387 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 29000 documents\n",
      "I0228 01:27:31.590233 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 30000 documents\n",
      "I0228 01:27:31.609958 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 31000 documents\n",
      "I0228 01:27:31.627940 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 32000 documents\n",
      "I0228 01:27:31.645822 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 33000 documents\n",
      "I0228 01:27:31.664024 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 34000 documents\n",
      "I0228 01:27:31.681984 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 35000 documents\n",
      "I0228 01:27:31.699217 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 36000 documents\n",
      "I0228 01:27:31.864549 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:27:32.234764 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (4980 virtual)\n",
      "I0228 01:27:32.239069 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10307 virtual)\n",
      "I0228 01:27:32.242855 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16328 virtual)\n",
      "I0228 01:27:32.246688 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21117 virtual)\n",
      "I0228 01:27:32.250603 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (26828 virtual)\n",
      "I0228 01:27:32.258924 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32057 virtual)\n",
      "I0228 01:27:32.262298 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37474 virtual)\n",
      "I0228 01:27:32.264936 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43337 virtual)\n",
      "I0228 01:27:32.267338 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48538 virtual)\n",
      "I0228 01:27:32.269827 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54118 virtual)\n",
      "I0228 01:27:32.308028 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (59944 virtual)\n",
      "I0228 01:27:32.310404 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65104 virtual)\n",
      "I0228 01:27:32.315674 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70375 virtual)\n",
      "I0228 01:27:32.330726 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (75654 virtual)\n",
      "I0228 01:27:32.336554 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81266 virtual)\n",
      "I0228 01:27:32.381164 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87394 virtual)\n",
      "I0228 01:27:32.384435 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93162 virtual)\n",
      "I0228 01:27:32.391266 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (98645 virtual)\n",
      "I0228 01:27:32.399391 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (103770 virtual)\n",
      "I0228 01:27:32.411227 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109258 virtual)\n",
      "I0228 01:27:32.452690 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (114661 virtual)\n",
      "I0228 01:27:32.456972 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (119945 virtual)\n",
      "I0228 01:27:32.461322 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (125648 virtual)\n",
      "I0228 01:27:32.471280 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (131251 virtual)\n",
      "I0228 01:27:32.485085 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (136778 virtual)\n",
      "I0228 01:27:32.529978 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (142176 virtual)\n",
      "I0228 01:27:32.534660 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (147786 virtual)\n",
      "I0228 01:27:32.539643 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (153858 virtual)\n",
      "I0228 01:27:32.544250 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (159612 virtual)\n",
      "I0228 01:27:32.577251 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (165183 virtual)\n",
      "I0228 01:27:32.596602 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (170663 virtual)\n",
      "I0228 01:27:32.600920 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176074 virtual)\n",
      "I0228 01:27:32.606832 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (181423 virtual)\n",
      "I0228 01:27:32.610133 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (186490 virtual)\n",
      "I0228 01:27:32.629739 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (192226 virtual)\n",
      "I0228 01:27:32.666723 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (197644 virtual)\n",
      "I0228 01:27:32.676928 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (202979 virtual)\n",
      "I0228 01:27:32.684889 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (208604 virtual)\n",
      "I0228 01:27:32.687861 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (214108 virtual)\n",
      "I0228 01:27:32.702382 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (219746 virtual)\n",
      "I0228 01:27:32.736366 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (225131 virtual)\n",
      "I0228 01:27:32.747520 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (232679 virtual)\n",
      "I0228 01:27:32.751606 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (238163 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:32.756186 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (244106 virtual)\n",
      "I0228 01:27:32.778901 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (249622 virtual)\n",
      "I0228 01:27:32.805317 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (254903 virtual)\n",
      "I0228 01:27:32.818345 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (260245 virtual)\n",
      "I0228 01:27:32.823799 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (265398 virtual)\n",
      "I0228 01:27:32.830979 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (270931 virtual)\n",
      "I0228 01:27:32.851892 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (276222 virtual)\n",
      "I0228 01:27:32.872336 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (281676 virtual)\n",
      "I0228 01:27:32.895068 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (287805 virtual)\n",
      "I0228 01:27:32.901328 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (292575 virtual)\n",
      "I0228 01:27:32.907725 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (298322 virtual)\n",
      "I0228 01:27:32.923178 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (303827 virtual)\n",
      "I0228 01:27:32.938543 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (308972 virtual)\n",
      "I0228 01:27:32.964721 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (314486 virtual)\n",
      "I0228 01:27:32.969867 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (320186 virtual)\n",
      "I0228 01:27:32.981985 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (325400 virtual)\n",
      "I0228 01:27:32.993173 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (331723 virtual)\n",
      "I0228 01:27:33.015765 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (337477 virtual)\n",
      "I0228 01:27:33.033136 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (343319 virtual)\n",
      "I0228 01:27:33.040915 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (348781 virtual)\n",
      "I0228 01:27:33.055302 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (354833 virtual)\n",
      "I0228 01:27:33.066244 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (360556 virtual)\n",
      "I0228 01:27:33.075210 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (366062 virtual)\n",
      "I0228 01:27:33.104922 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (371796 virtual)\n",
      "I0228 01:27:33.115339 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (377710 virtual)\n",
      "I0228 01:27:33.125173 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (383555 virtual)\n",
      "I0228 01:27:33.145865 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (388922 virtual)\n",
      "I0228 01:27:33.150846 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (393533 virtual)\n",
      "I0228 01:27:33.180953 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (397985 virtual)\n",
      "I0228 01:27:33.189436 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (401528 virtual)\n",
      "I0228 01:27:33.203620 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (405263 virtual)\n",
      "I0228 01:27:33.216753 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (409936 virtual)\n",
      "I0228 01:27:33.221982 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (414315 virtual)\n",
      "I0228 01:27:33.251828 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (418639 virtual)\n",
      "I0228 01:27:33.268888 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (422939 virtual)\n",
      "I0228 01:27:33.273958 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (427226 virtual)\n",
      "I0228 01:27:33.282353 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (431500 virtual)\n",
      "I0228 01:27:33.285581 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (436415 virtual)\n",
      "I0228 01:27:33.309436 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (441228 virtual)\n",
      "I0228 01:27:33.316503 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (445865 virtual)\n",
      "I0228 01:27:33.323134 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (449812 virtual)\n",
      "I0228 01:27:33.334094 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (453941 virtual)\n",
      "I0228 01:27:33.341263 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (457595 virtual)\n",
      "I0228 01:27:33.364420 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (461194 virtual)\n",
      "I0228 01:27:33.370157 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (464939 virtual)\n",
      "I0228 01:27:33.378384 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (469125 virtual)\n",
      "I0228 01:27:33.387876 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (473936 virtual)\n",
      "I0228 01:27:33.401521 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (478664 virtual)\n",
      "I0228 01:27:33.423561 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (483164 virtual)\n",
      "I0228 01:27:33.427741 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (487666 virtual)\n",
      "I0228 01:27:33.431893 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (492494 virtual)\n",
      "I0228 01:27:33.438306 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (497176 virtual)\n",
      "I0228 01:27:33.445635 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (501476 virtual)\n",
      "I0228 01:27:33.467861 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (505756 virtual)\n",
      "I0228 01:27:33.474081 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (511136 virtual)\n",
      "I0228 01:27:33.481652 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (515760 virtual)\n",
      "I0228 01:27:33.498703 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (520560 virtual)\n",
      "I0228 01:27:33.505911 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (524704 virtual)\n",
      "I0228 01:27:33.522893 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (529449 virtual)\n",
      "I0228 01:27:33.532469 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (533203 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:33.542142 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (537839 virtual)\n",
      "I0228 01:27:33.557428 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (541405 virtual)\n",
      "I0228 01:27:33.562647 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (545917 virtual)\n",
      "I0228 01:27:33.578239 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (550458 virtual)\n",
      "I0228 01:27:33.600852 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (554648 virtual)\n",
      "I0228 01:27:33.605378 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (559305 virtual)\n",
      "I0228 01:27:33.615453 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (563344 virtual)\n",
      "I0228 01:27:33.619201 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (567865 virtual)\n",
      "I0228 01:27:33.639632 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (571997 virtual)\n",
      "I0228 01:27:33.647498 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (576026 virtual)\n",
      "I0228 01:27:33.659633 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (579894 virtual)\n",
      "I0228 01:27:33.662208 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (582957 virtual)\n",
      "I0228 01:27:33.676131 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (587011 virtual)\n",
      "I0228 01:27:33.695830 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (592298 virtual)\n",
      "I0228 01:27:33.701654 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (598010 virtual)\n",
      "I0228 01:27:33.713386 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (603152 virtual)\n",
      "I0228 01:27:33.717745 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (607954 virtual)\n",
      "I0228 01:27:33.730614 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (613534 virtual)\n",
      "I0228 01:27:33.750932 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (618623 virtual)\n",
      "I0228 01:27:33.755986 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (623726 virtual)\n",
      "I0228 01:27:33.762038 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (628169 virtual)\n",
      "I0228 01:27:33.766724 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (632508 virtual)\n",
      "I0228 01:27:33.782949 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (636975 virtual)\n",
      "I0228 01:27:33.816304 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (642090 virtual)\n",
      "I0228 01:27:33.822181 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (647626 virtual)\n",
      "I0228 01:27:33.826442 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (652926 virtual)\n",
      "I0228 01:27:33.829604 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (658383 virtual)\n",
      "I0228 01:27:33.851183 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (662852 virtual)\n",
      "I0228 01:27:33.879379 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (667943 virtual)\n",
      "I0228 01:27:33.884157 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (673110 virtual)\n",
      "I0228 01:27:33.888269 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (678526 virtual)\n",
      "I0228 01:27:33.891583 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (683542 virtual)\n",
      "I0228 01:27:33.908509 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (688338 virtual)\n",
      "I0228 01:27:33.940603 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (693702 virtual)\n",
      "I0228 01:27:33.944882 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (698529 virtual)\n",
      "I0228 01:27:33.952716 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (703884 virtual)\n",
      "I0228 01:27:33.956664 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (708955 virtual)\n",
      "I0228 01:27:33.959661 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (714316 virtual)\n",
      "I0228 01:27:34.004624 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (719473 virtual)\n",
      "I0228 01:27:34.009969 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (724674 virtual)\n",
      "I0228 01:27:34.022299 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (729887 virtual)\n",
      "I0228 01:27:34.024584 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (735389 virtual)\n",
      "I0228 01:27:34.028844 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (740880 virtual)\n",
      "I0228 01:27:34.072924 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (746007 virtual)\n",
      "I0228 01:27:34.080609 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (750441 virtual)\n",
      "I0228 01:27:34.085424 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (756124 virtual)\n",
      "I0228 01:27:34.088467 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (761309 virtual)\n",
      "I0228 01:27:34.091309 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (766623 virtual)\n",
      "I0228 01:27:34.136615 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (772194 virtual)\n",
      "I0228 01:27:34.145669 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (778018 virtual)\n",
      "I0228 01:27:34.148974 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (783878 virtual)\n",
      "I0228 01:27:34.152223 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (789847 virtual)\n",
      "I0228 01:27:34.162961 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (795709 virtual)\n",
      "I0228 01:27:34.193835 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (801299 virtual)\n",
      "I0228 01:27:34.198143 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (806841 virtual)\n",
      "I0228 01:27:34.211521 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (812318 virtual)\n",
      "I0228 01:27:34.218772 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (818146 virtual)\n",
      "I0228 01:27:34.231098 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (823740 virtual)\n",
      "I0228 01:27:34.264349 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (830074 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:34.271824 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (835312 virtual)\n",
      "I0228 01:27:34.286003 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (840687 virtual)\n",
      "I0228 01:27:34.294167 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (846190 virtual)\n",
      "I0228 01:27:34.301474 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (852385 virtual)\n",
      "I0228 01:27:34.332295 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (858351 virtual)\n",
      "I0228 01:27:34.340760 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (864061 virtual)\n",
      "I0228 01:27:34.353111 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (869917 virtual)\n",
      "I0228 01:27:34.367639 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (875718 virtual)\n",
      "I0228 01:27:34.373632 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (881555 virtual)\n",
      "I0228 01:27:34.407316 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (887541 virtual)\n",
      "I0228 01:27:34.413147 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (893637 virtual)\n",
      "I0228 01:27:34.420898 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (899141 virtual)\n",
      "I0228 01:27:34.434009 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (904983 virtual)\n",
      "I0228 01:27:34.447933 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (912120 virtual)\n",
      "I0228 01:27:34.481350 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (917395 virtual)\n",
      "I0228 01:27:34.485041 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (921381 virtual)\n",
      "I0228 01:27:34.488391 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (924286 virtual)\n",
      "I0228 01:27:34.505742 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (927536 virtual)\n",
      "I0228 01:27:34.518386 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (934378 virtual)\n",
      "I0228 01:27:34.553378 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (937976 virtual)\n",
      "I0228 01:27:34.557721 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (941791 virtual)\n",
      "I0228 01:27:34.562104 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (945255 virtual)\n",
      "I0228 01:27:34.573825 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (949144 virtual)\n",
      "I0228 01:27:34.596129 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (952691 virtual)\n",
      "I0228 01:27:34.603911 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (955958 virtual)\n",
      "I0228 01:27:34.609037 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (959631 virtual)\n",
      "I0228 01:27:34.617382 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (963487 virtual)\n",
      "I0228 01:27:34.621890 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (966797 virtual)\n",
      "I0228 01:27:34.649341 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (971064 virtual)\n",
      "I0228 01:27:34.653825 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (974543 virtual)\n",
      "I0228 01:27:34.660999 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (978207 virtual)\n",
      "I0228 01:27:34.668454 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (981749 virtual)\n",
      "I0228 01:27:34.683202 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (985508 virtual)\n",
      "I0228 01:27:34.695116 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (988976 virtual)\n",
      "I0228 01:27:34.699498 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (992722 virtual)\n",
      "I0228 01:27:34.705586 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (996418 virtual)\n",
      "I0228 01:27:34.718995 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1000026 virtual)\n",
      "I0228 01:27:34.727333 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1003840 virtual)\n",
      "I0228 01:27:34.742088 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1007974 virtual)\n",
      "I0228 01:27:34.747626 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1011457 virtual)\n",
      "I0228 01:27:34.751509 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1014991 virtual)\n",
      "I0228 01:27:34.764263 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1018666 virtual)\n",
      "I0228 01:27:34.776485 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1022954 virtual)\n",
      "I0228 01:27:34.785009 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1028134 virtual)\n",
      "I0228 01:27:34.792747 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1031844 virtual)\n",
      "I0228 01:27:34.795622 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1035740 virtual)\n",
      "I0228 01:27:34.809935 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1039491 virtual)\n",
      "I0228 01:27:34.824169 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1043288 virtual)\n",
      "I0228 01:27:34.834848 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1048181 virtual)\n",
      "I0228 01:27:34.839007 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1051677 virtual)\n",
      "I0228 01:27:34.843366 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1056020 virtual)\n",
      "I0228 01:27:34.854769 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1059831 virtual)\n",
      "I0228 01:27:34.879701 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1063844 virtual)\n",
      "I0228 01:27:34.884063 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1067625 virtual)\n",
      "I0228 01:27:34.889312 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1071289 virtual)\n",
      "I0228 01:27:34.893719 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1075202 virtual)\n",
      "I0228 01:27:34.901942 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1079811 virtual)\n",
      "I0228 01:27:34.931089 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1083820 virtual)\n",
      "I0228 01:27:34.935593 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1088018 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:34.940293 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1092959 virtual)\n",
      "I0228 01:27:34.946841 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1096911 virtual)\n",
      "I0228 01:27:34.949478 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1100672 virtual)\n",
      "I0228 01:27:34.978621 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1104139 virtual)\n",
      "I0228 01:27:34.982822 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1108588 virtual)\n",
      "I0228 01:27:34.986771 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1112463 virtual)\n",
      "I0228 01:27:34.994330 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1116298 virtual)\n",
      "I0228 01:27:35.002179 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1120717 virtual)\n",
      "I0228 01:27:35.031498 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1124898 virtual)\n",
      "I0228 01:27:35.036315 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1129876 virtual)\n",
      "I0228 01:27:35.038913 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1133357 virtual)\n",
      "I0228 01:27:35.041698 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1138143 virtual)\n",
      "I0228 01:27:35.051847 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1142595 virtual)\n",
      "I0228 01:27:35.066756 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1147404 virtual)\n",
      "I0228 01:27:35.080084 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1152440 virtual)\n",
      "I0228 01:27:35.084769 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1156277 virtual)\n",
      "I0228 01:27:35.088484 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1159852 virtual)\n",
      "I0228 01:27:35.106088 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1163833 virtual)\n",
      "I0228 01:27:35.122540 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1168105 virtual)\n",
      "I0228 01:27:35.126387 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1171751 virtual)\n",
      "I0228 01:27:35.142261 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1176640 virtual)\n",
      "I0228 01:27:35.146445 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1181252 virtual)\n",
      "I0228 01:27:35.158181 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1185006 virtual)\n",
      "I0228 01:27:35.179104 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1189704 virtual)\n",
      "I0228 01:27:35.189118 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1194413 virtual)\n",
      "I0228 01:27:35.193457 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1198312 virtual)\n",
      "I0228 01:27:35.198321 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1203102 virtual)\n",
      "I0228 01:27:35.204932 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1206769 virtual)\n",
      "I0228 01:27:35.232009 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1211315 virtual)\n",
      "I0228 01:27:35.236577 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1215140 virtual)\n",
      "I0228 01:27:35.243954 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1219366 virtual)\n",
      "I0228 01:27:35.247307 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1223294 virtual)\n",
      "I0228 01:27:35.249973 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1227538 virtual)\n",
      "I0228 01:27:35.283857 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1231926 virtual)\n",
      "I0228 01:27:35.287740 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1235633 virtual)\n",
      "I0228 01:27:35.291485 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1239332 virtual)\n",
      "I0228 01:27:35.295003 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1242184 virtual)\n",
      "I0228 01:27:35.302731 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1245647 virtual)\n",
      "I0228 01:27:35.326692 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1249504 virtual)\n",
      "I0228 01:27:35.333065 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1253249 virtual)\n",
      "I0228 01:27:35.337921 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1257881 virtual)\n",
      "I0228 01:27:35.341044 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1263048 virtual)\n",
      "I0228 01:27:35.352509 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1267103 virtual)\n",
      "I0228 01:27:35.367758 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1271089 virtual)\n",
      "I0228 01:27:35.377224 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1275273 virtual)\n",
      "I0228 01:27:35.381240 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1279404 virtual)\n",
      "I0228 01:27:35.385068 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1283263 virtual)\n",
      "I0228 01:27:35.392888 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1287106 virtual)\n",
      "I0228 01:27:35.410851 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1291068 virtual)\n",
      "I0228 01:27:35.415810 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1295881 virtual)\n",
      "I0228 01:27:35.436410 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1299513 virtual)\n",
      "I0228 01:27:35.441890 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1303815 virtual)\n",
      "I0228 01:27:35.447713 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1308455 virtual)\n",
      "I0228 01:27:35.457429 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1311710 virtual)\n",
      "I0228 01:27:35.460060 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1315320 virtual)\n",
      "I0228 01:27:35.483078 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1320065 virtual)\n",
      "I0228 01:27:35.489055 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1324773 virtual)\n",
      "I0228 01:27:35.494055 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1328367 virtual)\n",
      "I0228 01:27:35.499388 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1332293 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:35.518428 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1336490 virtual)\n",
      "I0228 01:27:35.522144 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1340041 virtual)\n",
      "I0228 01:27:35.531788 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1343597 virtual)\n",
      "I0228 01:27:35.537418 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1347501 virtual)\n",
      "I0228 01:27:35.547448 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1351792 virtual)\n",
      "I0228 01:27:35.556704 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1356046 virtual)\n",
      "I0228 01:27:35.575832 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1360462 virtual)\n",
      "I0228 01:27:35.581491 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1364569 virtual)\n",
      "I0228 01:27:35.586668 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1369563 virtual)\n",
      "I0228 01:27:35.594192 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1373097 virtual)\n",
      "I0228 01:27:35.604849 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1374869 virtual)\n",
      "I0228 01:27:35.616678 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1380132 virtual)\n",
      "I0228 01:27:35.622792 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1385227 virtual)\n",
      "I0228 01:27:35.627972 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1389503 virtual)\n",
      "I0228 01:27:35.659523 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1394262 virtual)\n",
      "I0228 01:27:35.667519 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1397912 virtual)\n",
      "I0228 01:27:35.670391 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1403013 virtual)\n",
      "I0228 01:27:35.676603 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1406999 virtual)\n",
      "I0228 01:27:35.679384 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1411617 virtual)\n",
      "I0228 01:27:35.688801 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1416530 virtual)\n",
      "I0228 01:27:35.693795 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1420976 virtual)\n",
      "I0228 01:27:35.731188 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1425009 virtual)\n",
      "I0228 01:27:35.735528 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1429711 virtual)\n",
      "I0228 01:27:35.739961 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1434671 virtual)\n",
      "I0228 01:27:35.743390 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1438407 virtual)\n",
      "I0228 01:27:35.746266 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1443252 virtual)\n",
      "I0228 01:27:35.779792 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1447792 virtual)\n",
      "I0228 01:27:35.786269 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1452008 virtual)\n",
      "I0228 01:27:35.794869 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1457195 virtual)\n",
      "I0228 01:27:35.798238 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1462285 virtual)\n",
      "I0228 01:27:35.801732 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1467862 virtual)\n",
      "I0228 01:27:35.829816 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1472657 virtual)\n",
      "I0228 01:27:35.839177 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1478570 virtual)\n",
      "I0228 01:27:35.842865 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1482890 virtual)\n",
      "I0228 01:27:35.853930 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1490127 virtual)\n",
      "I0228 01:27:35.858467 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1497773 virtual)\n",
      "I0228 01:27:35.883448 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1502877 virtual)\n",
      "I0228 01:27:35.888965 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1507639 virtual)\n",
      "I0228 01:27:35.906399 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1512800 virtual)\n",
      "I0228 01:27:35.913822 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1517881 virtual)\n",
      "I0228 01:27:35.931480 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1523234 virtual)\n",
      "I0228 01:27:35.946417 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1527815 virtual)\n",
      "I0228 01:27:35.959827 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1532647 virtual)\n",
      "I0228 01:27:35.961827 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1537658 virtual)\n",
      "I0228 01:27:35.992074 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1547940 virtual)\n",
      "I0228 01:27:36.000261 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1553419 virtual)\n",
      "I0228 01:27:36.013055 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1559261 virtual)\n",
      "I0228 01:27:36.016993 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1564335 virtual)\n",
      "I0228 01:27:36.021682 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1568573 virtual)\n",
      "I0228 01:27:36.053609 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1572843 virtual)\n",
      "I0228 01:27:36.062572 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1577601 virtual)\n",
      "I0228 01:27:36.067961 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1582538 virtual)\n",
      "I0228 01:27:36.073992 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1587071 virtual)\n",
      "I0228 01:27:36.077491 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1592351 virtual)\n",
      "I0228 01:27:36.127862 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1598301 virtual)\n",
      "I0228 01:27:36.132163 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1604071 virtual)\n",
      "I0228 01:27:36.135273 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1610037 virtual)\n",
      "I0228 01:27:36.138128 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1615118 virtual)\n",
      "I0228 01:27:36.175703 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1620916 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:36.181315 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1627139 virtual)\n",
      "I0228 01:27:36.186585 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1632635 virtual)\n",
      "I0228 01:27:36.189360 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1636757 virtual)\n",
      "I0228 01:27:36.196030 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1642288 virtual)\n",
      "I0228 01:27:36.238261 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1647184 virtual)\n",
      "I0228 01:27:36.253458 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1652434 virtual)\n",
      "I0228 01:27:36.258471 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1658210 virtual)\n",
      "I0228 01:27:36.262538 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1663210 virtual)\n",
      "I0228 01:27:36.265748 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1668494 virtual)\n",
      "I0228 01:27:36.309234 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1673876 virtual)\n",
      "I0228 01:27:36.313626 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1679127 virtual)\n",
      "I0228 01:27:36.319193 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1685931 virtual)\n",
      "I0228 01:27:36.324083 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1694159 virtual)\n",
      "I0228 01:27:36.327698 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1702731 virtual)\n",
      "I0228 01:27:36.371484 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1710804 virtual)\n",
      "I0228 01:27:36.376661 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1718825 virtual)\n",
      "I0228 01:27:36.382006 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1723914 virtual)\n",
      "I0228 01:27:36.384868 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1729364 virtual)\n",
      "I0228 01:27:36.390403 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1735342 virtual)\n",
      "I0228 01:27:36.435044 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1740928 virtual)\n",
      "I0228 01:27:36.439301 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1747155 virtual)\n",
      "I0228 01:27:36.470890 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1752977 virtual)\n",
      "I0228 01:27:36.485638 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1758611 virtual)\n",
      "I0228 01:27:36.494527 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1763421 virtual)\n",
      "I0228 01:27:36.536756 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1769148 virtual)\n",
      "I0228 01:27:36.540916 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1774086 virtual)\n",
      "I0228 01:27:36.545829 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1779286 virtual)\n",
      "I0228 01:27:36.551770 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1783940 virtual)\n",
      "I0228 01:27:36.569292 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1789163 virtual)\n",
      "I0228 01:27:36.602852 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1794670 virtual)\n",
      "I0228 01:27:36.613164 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1799955 virtual)\n",
      "I0228 01:27:36.617963 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1804664 virtual)\n",
      "I0228 01:27:36.622108 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1809503 virtual)\n",
      "I0228 01:27:36.625120 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1814331 virtual)\n",
      "I0228 01:27:36.675560 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1819570 virtual)\n",
      "I0228 01:27:36.679761 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1824719 virtual)\n",
      "I0228 01:27:36.683002 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1829810 virtual)\n",
      "I0228 01:27:36.686229 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1834915 virtual)\n",
      "I0228 01:27:36.694195 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1840005 virtual)\n",
      "I0228 01:27:36.740624 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1845214 virtual)\n",
      "I0228 01:27:36.745712 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1850183 virtual)\n",
      "I0228 01:27:36.752406 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1854938 virtual)\n",
      "I0228 01:27:36.755831 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1860294 virtual)\n",
      "I0228 01:27:36.758746 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1865279 virtual)\n",
      "I0228 01:27:36.803879 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1870948 virtual)\n",
      "I0228 01:27:36.807856 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1875665 virtual)\n",
      "I0228 01:27:36.812024 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1880483 virtual)\n",
      "I0228 01:27:36.815653 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1885116 virtual)\n",
      "I0228 01:27:36.822897 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1890491 virtual)\n",
      "I0228 01:27:36.865743 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1895404 virtual)\n",
      "I0228 01:27:36.870075 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1900369 virtual)\n",
      "I0228 01:27:36.874588 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1905171 virtual)\n",
      "I0228 01:27:36.879422 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1910438 virtual)\n",
      "I0228 01:27:36.887050 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1916009 virtual)\n",
      "I0228 01:27:36.928145 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1921059 virtual)\n",
      "I0228 01:27:36.934727 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1925818 virtual)\n",
      "I0228 01:27:36.939755 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1931200 virtual)\n",
      "I0228 01:27:36.942638 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1936305 virtual)\n",
      "I0228 01:27:36.954363 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1941224 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:36.989110 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1945769 virtual)\n",
      "I0228 01:27:36.998570 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1950572 virtual)\n",
      "I0228 01:27:37.003220 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1955629 virtual)\n",
      "I0228 01:27:37.007896 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (1960731 virtual)\n",
      "I0228 01:27:37.023083 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (1965723 virtual)\n",
      "I0228 01:27:37.052964 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (1970860 virtual)\n",
      "I0228 01:27:37.057985 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (1975467 virtual)\n",
      "I0228 01:27:37.063855 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (1980966 virtual)\n",
      "I0228 01:27:37.067416 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (1986159 virtual)\n",
      "I0228 01:27:37.086052 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (1991181 virtual)\n",
      "I0228 01:27:37.110592 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (1995845 virtual)\n",
      "I0228 01:27:37.120761 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2000920 virtual)\n",
      "I0228 01:27:37.132158 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2006296 virtual)\n",
      "I0228 01:27:37.135638 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2011346 virtual)\n",
      "I0228 01:27:37.149827 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2016562 virtual)\n",
      "I0228 01:27:37.176135 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2022056 virtual)\n",
      "I0228 01:27:37.181434 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2027295 virtual)\n",
      "I0228 01:27:37.219385 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2032389 virtual)\n",
      "I0228 01:27:37.222549 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2037707 virtual)\n",
      "I0228 01:27:37.225578 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2042909 virtual)\n",
      "I0228 01:27:37.240725 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2048014 virtual)\n",
      "I0228 01:27:37.245913 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2052827 virtual)\n",
      "I0228 01:27:37.261169 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2057669 virtual)\n",
      "I0228 01:27:37.267083 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2062510 virtual)\n",
      "I0228 01:27:37.282456 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2067202 virtual)\n",
      "I0228 01:27:37.304956 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2071785 virtual)\n",
      "I0228 01:27:37.312083 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2076587 virtual)\n",
      "I0228 01:27:37.327504 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2081446 virtual)\n",
      "I0228 01:27:37.335289 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2086861 virtual)\n",
      "I0228 01:27:37.350435 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2091685 virtual)\n",
      "I0228 01:27:37.369489 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2096924 virtual)\n",
      "I0228 01:27:37.374553 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2102107 virtual)\n",
      "I0228 01:27:37.388582 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2107321 virtual)\n",
      "I0228 01:27:37.400752 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2112655 virtual)\n",
      "I0228 01:27:37.412747 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2117815 virtual)\n",
      "I0228 01:27:37.429364 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2123274 virtual)\n",
      "I0228 01:27:37.434441 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2127817 virtual)\n",
      "I0228 01:27:37.450034 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2132786 virtual)\n",
      "I0228 01:27:37.469578 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2137587 virtual)\n",
      "I0228 01:27:37.473710 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2142551 virtual)\n",
      "I0228 01:27:37.491404 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2146975 virtual)\n",
      "I0228 01:27:37.502484 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2152236 virtual)\n",
      "I0228 01:27:37.513175 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2157549 virtual)\n",
      "I0228 01:27:37.537915 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2162900 virtual)\n",
      "I0228 01:27:37.542520 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2168020 virtual)\n",
      "I0228 01:27:37.559900 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2173269 virtual)\n",
      "I0228 01:27:37.564039 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2178359 virtual)\n",
      "I0228 01:27:37.576102 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2183118 virtual)\n",
      "I0228 01:27:37.599347 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2188082 virtual)\n",
      "I0228 01:27:37.603776 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2193406 virtual)\n",
      "I0228 01:27:37.617109 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2198289 virtual)\n",
      "I0228 01:27:37.630888 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2203130 virtual)\n",
      "I0228 01:27:37.644793 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2207721 virtual)\n",
      "I0228 01:27:37.668160 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2213203 virtual)\n",
      "I0228 01:27:37.678407 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2218005 virtual)\n",
      "I0228 01:27:37.681888 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2223223 virtual)\n",
      "I0228 01:27:37.702079 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2228861 virtual)\n",
      "I0228 01:27:37.705086 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2233697 virtual)\n",
      "I0228 01:27:37.732042 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2238913 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:37.739174 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2244032 virtual)\n",
      "I0228 01:27:37.743761 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2248772 virtual)\n",
      "I0228 01:27:37.763776 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2253791 virtual)\n",
      "I0228 01:27:37.768854 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2259114 virtual)\n",
      "I0228 01:27:37.801606 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2264057 virtual)\n",
      "I0228 01:27:37.805762 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2269298 virtual)\n",
      "I0228 01:27:37.809864 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2273997 virtual)\n",
      "I0228 01:27:37.825321 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2279384 virtual)\n",
      "I0228 01:27:37.835497 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2284177 virtual)\n",
      "I0228 01:27:37.864871 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2289152 virtual)\n",
      "I0228 01:27:37.870122 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2294160 virtual)\n",
      "I0228 01:27:37.875898 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2299141 virtual)\n",
      "I0228 01:27:37.889114 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2304090 virtual)\n",
      "I0228 01:27:37.904375 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2308962 virtual)\n",
      "I0228 01:27:37.925522 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2313823 virtual)\n",
      "I0228 01:27:37.930104 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2319150 virtual)\n",
      "I0228 01:27:37.934152 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2324289 virtual)\n",
      "I0228 01:27:37.954782 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2329350 virtual)\n",
      "I0228 01:27:37.966025 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2334643 virtual)\n",
      "I0228 01:27:37.986884 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2339692 virtual)\n",
      "I0228 01:27:37.994467 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2344877 virtual)\n",
      "I0228 01:27:37.999070 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2350029 virtual)\n",
      "I0228 01:27:38.020486 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2354850 virtual)\n",
      "I0228 01:27:38.032375 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2360259 virtual)\n",
      "I0228 01:27:38.051645 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2365614 virtual)\n",
      "I0228 01:27:38.059812 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2370737 virtual)\n",
      "I0228 01:27:38.063207 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2375527 virtual)\n",
      "I0228 01:27:38.082974 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2380243 virtual)\n",
      "I0228 01:27:38.100207 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2384922 virtual)\n",
      "I0228 01:27:38.112034 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2390511 virtual)\n",
      "I0228 01:27:38.124366 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2395241 virtual)\n",
      "I0228 01:27:38.137150 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2400270 virtual)\n",
      "I0228 01:27:38.144349 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2405753 virtual)\n",
      "I0228 01:27:38.166804 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2410910 virtual)\n",
      "I0228 01:27:38.180916 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2416012 virtual)\n",
      "I0228 01:27:38.189883 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2420845 virtual)\n",
      "I0228 01:27:38.193730 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2425758 virtual)\n",
      "I0228 01:27:38.203856 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2430647 virtual)\n",
      "I0228 01:27:38.228284 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2435460 virtual)\n",
      "I0228 01:27:38.251051 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2440728 virtual)\n",
      "I0228 01:27:38.255256 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2446252 virtual)\n",
      "I0228 01:27:38.259385 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2451544 virtual)\n",
      "I0228 01:27:38.273321 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2456346 virtual)\n",
      "I0228 01:27:38.294442 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2461416 virtual)\n",
      "I0228 01:27:38.314616 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2466654 virtual)\n",
      "I0228 01:27:38.319845 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2471543 virtual)\n",
      "I0228 01:27:38.324072 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2476304 virtual)\n",
      "I0228 01:27:38.332788 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2481503 virtual)\n",
      "I0228 01:27:38.357475 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2486437 virtual)\n",
      "I0228 01:27:38.380216 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2491688 virtual)\n",
      "I0228 01:27:38.389604 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2497300 virtual)\n",
      "I0228 01:27:38.394005 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2502428 virtual)\n",
      "I0228 01:27:38.398422 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2507662 virtual)\n",
      "I0228 01:27:38.421947 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2512415 virtual)\n",
      "I0228 01:27:38.443494 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2516899 virtual)\n",
      "I0228 01:27:38.450586 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2521610 virtual)\n",
      "I0228 01:27:38.455862 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2526978 virtual)\n",
      "I0228 01:27:38.459849 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2532200 virtual)\n",
      "I0228 01:27:38.486304 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2537538 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:38.506716 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2542458 virtual)\n",
      "I0228 01:27:38.519267 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2547273 virtual)\n",
      "I0228 01:27:38.525751 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2552776 virtual)\n",
      "I0228 01:27:38.530946 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2557593 virtual)\n",
      "I0228 01:27:38.544528 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2562282 virtual)\n",
      "I0228 01:27:38.562261 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2567323 virtual)\n",
      "I0228 01:27:38.578169 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2572279 virtual)\n",
      "I0228 01:27:38.590339 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2577233 virtual)\n",
      "I0228 01:27:38.595255 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2582612 virtual)\n",
      "I0228 01:27:38.610331 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2587452 virtual)\n",
      "I0228 01:27:38.623241 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2592705 virtual)\n",
      "I0228 01:27:38.642280 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2597927 virtual)\n",
      "I0228 01:27:38.655112 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2602734 virtual)\n",
      "I0228 01:27:38.660495 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2607825 virtual)\n",
      "I0228 01:27:38.667499 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2613337 virtual)\n",
      "I0228 01:27:38.685893 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2618745 virtual)\n",
      "I0228 01:27:38.702939 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2623848 virtual)\n",
      "I0228 01:27:38.719145 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2629193 virtual)\n",
      "I0228 01:27:38.726377 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2634622 virtual)\n",
      "I0228 01:27:38.731771 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2639955 virtual)\n",
      "I0228 01:27:38.773754 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2644614 virtual)\n",
      "I0228 01:27:38.776844 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2649815 virtual)\n",
      "I0228 01:27:38.783422 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2654573 virtual)\n",
      "I0228 01:27:38.793019 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2659823 virtual)\n",
      "I0228 01:27:38.802026 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2664558 virtual)\n",
      "I0228 01:27:38.819720 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2669658 virtual)\n",
      "I0228 01:27:38.837234 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2674339 virtual)\n",
      "I0228 01:27:38.850124 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2679389 virtual)\n",
      "I0228 01:27:38.861857 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2684651 virtual)\n",
      "I0228 01:27:38.869698 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2689903 virtual)\n",
      "I0228 01:27:38.878123 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2695402 virtual)\n",
      "I0228 01:27:38.903646 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2700487 virtual)\n",
      "I0228 01:27:38.909853 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2705977 virtual)\n",
      "I0228 01:27:38.928607 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2711002 virtual)\n",
      "I0228 01:27:38.932735 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2716398 virtual)\n",
      "I0228 01:27:38.941788 140370323834688 text_analysis.py:506] 552 batches submitted to accumulate stats from 35328 documents (2721543 virtual)\n",
      "I0228 01:27:38.962073 140370323834688 text_analysis.py:506] 553 batches submitted to accumulate stats from 35392 documents (2726524 virtual)\n",
      "I0228 01:27:38.976225 140370323834688 text_analysis.py:506] 554 batches submitted to accumulate stats from 35456 documents (2731527 virtual)\n",
      "I0228 01:27:38.994475 140370323834688 text_analysis.py:506] 555 batches submitted to accumulate stats from 35520 documents (2736614 virtual)\n",
      "I0228 01:27:38.998693 140370323834688 text_analysis.py:506] 556 batches submitted to accumulate stats from 35584 documents (2741973 virtual)\n",
      "I0228 01:27:39.011113 140370323834688 text_analysis.py:506] 557 batches submitted to accumulate stats from 35648 documents (2747487 virtual)\n",
      "I0228 01:27:39.027302 140370323834688 text_analysis.py:506] 558 batches submitted to accumulate stats from 35712 documents (2753102 virtual)\n",
      "I0228 01:27:39.044654 140370323834688 text_analysis.py:506] 559 batches submitted to accumulate stats from 35776 documents (2758299 virtual)\n",
      "I0228 01:27:39.060831 140370323834688 text_analysis.py:506] 560 batches submitted to accumulate stats from 35840 documents (2763361 virtual)\n",
      "I0228 01:27:39.064311 140370323834688 text_analysis.py:506] 561 batches submitted to accumulate stats from 35904 documents (2768438 virtual)\n",
      "I0228 01:27:39.084096 140370323834688 text_analysis.py:506] 562 batches submitted to accumulate stats from 35968 documents (2773424 virtual)\n",
      "I0228 01:27:39.094087 140370323834688 text_analysis.py:506] 563 batches submitted to accumulate stats from 36032 documents (2778548 virtual)\n",
      "I0228 01:27:39.110669 140370323834688 text_analysis.py:506] 564 batches submitted to accumulate stats from 36096 documents (2784123 virtual)\n",
      "I0228 01:27:39.126209 140370323834688 text_analysis.py:506] 565 batches submitted to accumulate stats from 36160 documents (2789016 virtual)\n",
      "I0228 01:27:39.131479 140370323834688 text_analysis.py:506] 566 batches submitted to accumulate stats from 36224 documents (2793951 virtual)\n",
      "I0228 01:27:39.158030 140370323834688 text_analysis.py:506] 567 batches submitted to accumulate stats from 36288 documents (2795719 virtual)\n",
      "I0228 01:27:39.228100 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:27:39.249359 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:27:39.251504 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:27:39.252161 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:27:39.262713 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:27:39.262666 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:27:39.232268 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:27:39.253229 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:27:39.254664 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:27:39.271011 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:27:39.672957 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:39.696004 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2796164 virtual documents\n",
      "I0228 01:27:39.779548 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:27:40.145944 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (4980 virtual)\n",
      "I0228 01:27:40.150787 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10307 virtual)\n",
      "I0228 01:27:40.154548 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16328 virtual)\n",
      "I0228 01:27:40.157735 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21117 virtual)\n",
      "I0228 01:27:40.161345 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (26828 virtual)\n",
      "I0228 01:27:40.176139 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32057 virtual)\n",
      "I0228 01:27:40.178466 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37474 virtual)\n",
      "I0228 01:27:40.180648 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43337 virtual)\n",
      "I0228 01:27:40.182669 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48538 virtual)\n",
      "I0228 01:27:40.184807 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54118 virtual)\n",
      "I0228 01:27:40.219878 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (59944 virtual)\n",
      "I0228 01:27:40.225976 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65104 virtual)\n",
      "I0228 01:27:40.239432 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70375 virtual)\n",
      "I0228 01:27:40.242246 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (75654 virtual)\n",
      "I0228 01:27:40.249466 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81266 virtual)\n",
      "I0228 01:27:40.291299 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87394 virtual)\n",
      "I0228 01:27:40.303487 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93162 virtual)\n",
      "I0228 01:27:40.308678 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (98645 virtual)\n",
      "I0228 01:27:40.317201 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (103770 virtual)\n",
      "I0228 01:27:40.325945 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109258 virtual)\n",
      "I0228 01:27:40.364938 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (114661 virtual)\n",
      "I0228 01:27:40.369684 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (119945 virtual)\n",
      "I0228 01:27:40.377607 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (125648 virtual)\n",
      "I0228 01:27:40.395091 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (131251 virtual)\n",
      "I0228 01:27:40.400380 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (136778 virtual)\n",
      "I0228 01:27:40.441869 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (142176 virtual)\n",
      "I0228 01:27:40.447472 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (147786 virtual)\n",
      "I0228 01:27:40.453034 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (153858 virtual)\n",
      "I0228 01:27:40.461666 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (159612 virtual)\n",
      "I0228 01:27:40.471394 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (165183 virtual)\n",
      "I0228 01:27:40.510674 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (170663 virtual)\n",
      "I0228 01:27:40.516615 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176074 virtual)\n",
      "I0228 01:27:40.520509 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (181423 virtual)\n",
      "I0228 01:27:40.533110 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (186490 virtual)\n",
      "I0228 01:27:40.542602 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (192226 virtual)\n",
      "I0228 01:27:40.582060 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (197644 virtual)\n",
      "I0228 01:27:40.587275 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (202979 virtual)\n",
      "I0228 01:27:40.597152 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (208604 virtual)\n",
      "I0228 01:27:40.607349 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (214108 virtual)\n",
      "I0228 01:27:40.615163 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (219746 virtual)\n",
      "I0228 01:27:40.652028 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (225131 virtual)\n",
      "I0228 01:27:40.658030 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (232679 virtual)\n",
      "I0228 01:27:40.664364 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (238163 virtual)\n",
      "I0228 01:27:40.679163 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (244106 virtual)\n",
      "I0228 01:27:40.691429 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (249622 virtual)\n",
      "I0228 01:27:40.722393 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (254903 virtual)\n",
      "I0228 01:27:40.728860 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (260245 virtual)\n",
      "I0228 01:27:40.737357 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (265398 virtual)\n",
      "I0228 01:27:40.751885 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (270931 virtual)\n",
      "I0228 01:27:40.764898 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (276222 virtual)\n",
      "I0228 01:27:40.791018 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (281676 virtual)\n",
      "I0228 01:27:40.809934 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (287805 virtual)\n",
      "I0228 01:27:40.814665 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (292575 virtual)\n",
      "I0228 01:27:40.828004 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (298322 virtual)\n",
      "I0228 01:27:40.835145 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (303827 virtual)\n",
      "I0228 01:27:40.858305 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (308972 virtual)\n",
      "I0228 01:27:40.879938 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (314486 virtual)\n",
      "I0228 01:27:40.885485 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (320186 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:40.901595 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (325400 virtual)\n",
      "I0228 01:27:40.908173 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (331723 virtual)\n",
      "I0228 01:27:40.929718 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (337477 virtual)\n",
      "I0228 01:27:40.943598 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (343319 virtual)\n",
      "I0228 01:27:40.958209 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (348781 virtual)\n",
      "I0228 01:27:40.977086 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (354833 virtual)\n",
      "I0228 01:27:40.990280 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (360556 virtual)\n",
      "I0228 01:27:41.005983 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (366062 virtual)\n",
      "I0228 01:27:41.014735 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (371796 virtual)\n",
      "I0228 01:27:41.027945 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (377710 virtual)\n",
      "I0228 01:27:41.042661 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (383555 virtual)\n",
      "I0228 01:27:41.059828 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (388922 virtual)\n",
      "I0228 01:27:41.074740 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (393533 virtual)\n",
      "I0228 01:27:41.090548 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (397985 virtual)\n",
      "I0228 01:27:41.099426 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (401528 virtual)\n",
      "I0228 01:27:41.119289 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (405263 virtual)\n",
      "I0228 01:27:41.133662 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (409936 virtual)\n",
      "I0228 01:27:41.138771 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (414315 virtual)\n",
      "I0228 01:27:41.161228 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (418639 virtual)\n",
      "I0228 01:27:41.179109 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (422939 virtual)\n",
      "I0228 01:27:41.192581 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (427226 virtual)\n",
      "I0228 01:27:41.196466 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (431500 virtual)\n",
      "I0228 01:27:41.202780 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (436415 virtual)\n",
      "I0228 01:27:41.218944 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (441228 virtual)\n",
      "I0228 01:27:41.226930 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (445865 virtual)\n",
      "I0228 01:27:41.240740 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (449812 virtual)\n",
      "I0228 01:27:41.253589 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (453941 virtual)\n",
      "I0228 01:27:41.258143 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (457595 virtual)\n",
      "I0228 01:27:41.273600 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (461194 virtual)\n",
      "I0228 01:27:41.280873 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (464939 virtual)\n",
      "I0228 01:27:41.295009 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (469125 virtual)\n",
      "I0228 01:27:41.308169 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (473936 virtual)\n",
      "I0228 01:27:41.315082 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (478664 virtual)\n",
      "I0228 01:27:41.331951 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (483164 virtual)\n",
      "I0228 01:27:41.336982 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (487666 virtual)\n",
      "I0228 01:27:41.345406 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (492494 virtual)\n",
      "I0228 01:27:41.359225 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (497176 virtual)\n",
      "I0228 01:27:41.362871 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (501476 virtual)\n",
      "I0228 01:27:41.375874 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (505756 virtual)\n",
      "I0228 01:27:41.382505 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (511136 virtual)\n",
      "I0228 01:27:41.396092 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (515760 virtual)\n",
      "I0228 01:27:41.418875 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (520560 virtual)\n",
      "I0228 01:27:41.423472 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (524704 virtual)\n",
      "I0228 01:27:41.430314 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (529449 virtual)\n",
      "I0228 01:27:41.440154 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (533203 virtual)\n",
      "I0228 01:27:41.455878 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (537839 virtual)\n",
      "I0228 01:27:41.476023 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (541405 virtual)\n",
      "I0228 01:27:41.480805 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (545917 virtual)\n",
      "I0228 01:27:41.485437 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (550458 virtual)\n",
      "I0228 01:27:41.508782 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (554648 virtual)\n",
      "I0228 01:27:41.514260 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (559305 virtual)\n",
      "I0228 01:27:41.532587 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (563344 virtual)\n",
      "I0228 01:27:41.540015 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (567865 virtual)\n",
      "I0228 01:27:41.545349 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (571997 virtual)\n",
      "I0228 01:27:41.554944 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (576026 virtual)\n",
      "I0228 01:27:41.571122 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (579894 virtual)\n",
      "I0228 01:27:41.576925 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (582957 virtual)\n",
      "I0228 01:27:41.598515 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (587011 virtual)\n",
      "I0228 01:27:41.602997 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (592298 virtual)\n",
      "I0228 01:27:41.607420 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (598010 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:41.628530 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (603152 virtual)\n",
      "I0228 01:27:41.632786 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (607954 virtual)\n",
      "I0228 01:27:41.654260 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (613534 virtual)\n",
      "I0228 01:27:41.658529 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (618623 virtual)\n",
      "I0228 01:27:41.662486 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (623726 virtual)\n",
      "I0228 01:27:41.675611 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (628169 virtual)\n",
      "I0228 01:27:41.679460 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (632508 virtual)\n",
      "I0228 01:27:41.708598 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (636975 virtual)\n",
      "I0228 01:27:41.727741 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (642090 virtual)\n",
      "I0228 01:27:41.733351 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (647626 virtual)\n",
      "I0228 01:27:41.739064 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (652926 virtual)\n",
      "I0228 01:27:41.743823 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (658383 virtual)\n",
      "I0228 01:27:41.782752 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (662852 virtual)\n",
      "I0228 01:27:41.791494 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (667943 virtual)\n",
      "I0228 01:27:41.795900 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (673110 virtual)\n",
      "I0228 01:27:41.799888 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (678526 virtual)\n",
      "I0228 01:27:41.803084 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (683542 virtual)\n",
      "I0228 01:27:41.846081 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (688338 virtual)\n",
      "I0228 01:27:41.872033 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (693702 virtual)\n",
      "I0228 01:27:41.879358 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (698529 virtual)\n",
      "I0228 01:27:41.885211 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (703884 virtual)\n",
      "I0228 01:27:41.888552 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (708955 virtual)\n",
      "I0228 01:27:41.908811 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (714316 virtual)\n",
      "I0228 01:27:41.917468 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (719473 virtual)\n",
      "I0228 01:27:41.921013 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (724674 virtual)\n",
      "I0228 01:27:41.925940 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (729887 virtual)\n",
      "I0228 01:27:41.930176 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (735389 virtual)\n",
      "I0228 01:27:41.973570 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (740880 virtual)\n",
      "I0228 01:27:41.983226 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (746007 virtual)\n",
      "I0228 01:27:41.987505 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (750441 virtual)\n",
      "I0228 01:27:41.992055 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (756124 virtual)\n",
      "I0228 01:27:41.996522 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (761309 virtual)\n",
      "I0228 01:27:42.043678 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (766623 virtual)\n",
      "I0228 01:27:42.047899 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (772194 virtual)\n",
      "I0228 01:27:42.052379 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (778018 virtual)\n",
      "I0228 01:27:42.055530 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (783878 virtual)\n",
      "I0228 01:27:42.058587 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (789847 virtual)\n",
      "I0228 01:27:42.099775 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (795709 virtual)\n",
      "I0228 01:27:42.105377 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (801299 virtual)\n",
      "I0228 01:27:42.116466 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (806841 virtual)\n",
      "I0228 01:27:42.120226 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (812318 virtual)\n",
      "I0228 01:27:42.124166 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (818146 virtual)\n",
      "I0228 01:27:42.177057 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (823740 virtual)\n",
      "I0228 01:27:42.182013 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (830074 virtual)\n",
      "I0228 01:27:42.191119 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (835312 virtual)\n",
      "I0228 01:27:42.195776 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (840687 virtual)\n",
      "I0228 01:27:42.199302 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (846190 virtual)\n",
      "I0228 01:27:42.239628 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (852385 virtual)\n",
      "I0228 01:27:42.243652 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (858351 virtual)\n",
      "I0228 01:27:42.261613 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (864061 virtual)\n",
      "I0228 01:27:42.266541 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (869917 virtual)\n",
      "I0228 01:27:42.271252 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (875718 virtual)\n",
      "I0228 01:27:42.310045 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (881555 virtual)\n",
      "I0228 01:27:42.320271 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (887541 virtual)\n",
      "I0228 01:27:42.329418 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (893637 virtual)\n",
      "I0228 01:27:42.333218 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (899141 virtual)\n",
      "I0228 01:27:42.337012 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (904983 virtual)\n",
      "I0228 01:27:42.382504 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (912120 virtual)\n",
      "I0228 01:27:42.393892 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (917395 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:42.400215 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (921381 virtual)\n",
      "I0228 01:27:42.403680 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (924286 virtual)\n",
      "I0228 01:27:42.407390 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (927536 virtual)\n",
      "I0228 01:27:42.452231 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (934378 virtual)\n",
      "I0228 01:27:42.465454 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (937976 virtual)\n",
      "I0228 01:27:42.470547 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (941791 virtual)\n",
      "I0228 01:27:42.474187 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (945255 virtual)\n",
      "I0228 01:27:42.477098 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (949144 virtual)\n",
      "I0228 01:27:42.512812 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (952691 virtual)\n",
      "I0228 01:27:42.518930 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (955958 virtual)\n",
      "I0228 01:27:42.522664 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (959631 virtual)\n",
      "I0228 01:27:42.532820 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (963487 virtual)\n",
      "I0228 01:27:42.536975 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (966797 virtual)\n",
      "I0228 01:27:42.564387 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (971064 virtual)\n",
      "I0228 01:27:42.568523 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (974543 virtual)\n",
      "I0228 01:27:42.577135 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (978207 virtual)\n",
      "I0228 01:27:42.582613 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (981749 virtual)\n",
      "I0228 01:27:42.599001 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (985508 virtual)\n",
      "I0228 01:27:42.609987 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (988976 virtual)\n",
      "I0228 01:27:42.614755 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (992722 virtual)\n",
      "I0228 01:27:42.622380 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (996418 virtual)\n",
      "I0228 01:27:42.631549 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1000026 virtual)\n",
      "I0228 01:27:42.642115 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1003840 virtual)\n",
      "I0228 01:27:42.655143 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1007974 virtual)\n",
      "I0228 01:27:42.662271 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1011457 virtual)\n",
      "I0228 01:27:42.667242 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1014991 virtual)\n",
      "I0228 01:27:42.676559 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1018666 virtual)\n",
      "I0228 01:27:42.691093 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1022954 virtual)\n",
      "I0228 01:27:42.697287 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1028134 virtual)\n",
      "I0228 01:27:42.708342 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1031844 virtual)\n",
      "I0228 01:27:42.712799 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1035740 virtual)\n",
      "I0228 01:27:42.721226 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1039491 virtual)\n",
      "I0228 01:27:42.738512 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1043288 virtual)\n",
      "I0228 01:27:42.746749 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1048181 virtual)\n",
      "I0228 01:27:42.751233 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1051677 virtual)\n",
      "I0228 01:27:42.757742 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1056020 virtual)\n",
      "I0228 01:27:42.766100 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1059831 virtual)\n",
      "I0228 01:27:42.793364 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1063844 virtual)\n",
      "I0228 01:27:42.797255 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1067625 virtual)\n",
      "I0228 01:27:42.804681 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1071289 virtual)\n",
      "I0228 01:27:42.808981 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1075202 virtual)\n",
      "I0228 01:27:42.812039 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1079811 virtual)\n",
      "I0228 01:27:42.843888 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1083820 virtual)\n",
      "I0228 01:27:42.848124 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1088018 virtual)\n",
      "I0228 01:27:42.854988 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1092959 virtual)\n",
      "I0228 01:27:42.857840 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1096911 virtual)\n",
      "I0228 01:27:42.860851 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1100672 virtual)\n",
      "I0228 01:27:42.892293 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1104139 virtual)\n",
      "I0228 01:27:42.896207 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1108588 virtual)\n",
      "I0228 01:27:42.900882 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1112463 virtual)\n",
      "I0228 01:27:42.905225 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1116298 virtual)\n",
      "I0228 01:27:42.915153 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1120717 virtual)\n",
      "I0228 01:27:42.943307 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1124898 virtual)\n",
      "I0228 01:27:42.947607 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1129876 virtual)\n",
      "I0228 01:27:42.951230 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1133357 virtual)\n",
      "I0228 01:27:42.955373 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1138143 virtual)\n",
      "I0228 01:27:42.963711 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1142595 virtual)\n",
      "I0228 01:27:42.981874 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1147404 virtual)\n",
      "I0228 01:27:42.993530 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1152440 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:42.997717 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1156277 virtual)\n",
      "I0228 01:27:43.001889 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1159852 virtual)\n",
      "I0228 01:27:43.017083 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1163833 virtual)\n",
      "I0228 01:27:43.034153 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1168105 virtual)\n",
      "I0228 01:27:43.038084 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1171751 virtual)\n",
      "I0228 01:27:43.055869 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1176640 virtual)\n",
      "I0228 01:27:43.059758 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1181252 virtual)\n",
      "I0228 01:27:43.068962 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1185006 virtual)\n",
      "I0228 01:27:43.095468 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1189704 virtual)\n",
      "I0228 01:27:43.101639 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1194413 virtual)\n",
      "I0228 01:27:43.105844 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1198312 virtual)\n",
      "I0228 01:27:43.109246 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1203102 virtual)\n",
      "I0228 01:27:43.115447 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1206769 virtual)\n",
      "I0228 01:27:43.145428 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1211315 virtual)\n",
      "I0228 01:27:43.149236 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1215140 virtual)\n",
      "I0228 01:27:43.159411 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1219366 virtual)\n",
      "I0228 01:27:43.165437 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1223294 virtual)\n",
      "I0228 01:27:43.168405 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1227538 virtual)\n",
      "I0228 01:27:43.198149 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1231926 virtual)\n",
      "I0228 01:27:43.201851 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1235633 virtual)\n",
      "I0228 01:27:43.205552 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1239332 virtual)\n",
      "I0228 01:27:43.208890 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1242184 virtual)\n",
      "I0228 01:27:43.214534 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1245647 virtual)\n",
      "I0228 01:27:43.238847 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1249504 virtual)\n",
      "I0228 01:27:43.247644 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1253249 virtual)\n",
      "I0228 01:27:43.251657 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1257881 virtual)\n",
      "I0228 01:27:43.255487 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1263048 virtual)\n",
      "I0228 01:27:43.263613 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1267103 virtual)\n",
      "I0228 01:27:43.282842 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1271089 virtual)\n",
      "I0228 01:27:43.289532 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1275273 virtual)\n",
      "I0228 01:27:43.292620 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1279404 virtual)\n",
      "I0228 01:27:43.295746 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1283263 virtual)\n",
      "I0228 01:27:43.303390 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1287106 virtual)\n",
      "I0228 01:27:43.325774 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1291068 virtual)\n",
      "I0228 01:27:43.329969 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1295881 virtual)\n",
      "I0228 01:27:43.348799 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1299513 virtual)\n",
      "I0228 01:27:43.352992 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1303815 virtual)\n",
      "I0228 01:27:43.357261 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1308455 virtual)\n",
      "I0228 01:27:43.373077 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1311710 virtual)\n",
      "I0228 01:27:43.376821 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1315320 virtual)\n",
      "I0228 01:27:43.395005 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1320065 virtual)\n",
      "I0228 01:27:43.399615 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1324773 virtual)\n",
      "I0228 01:27:43.414716 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1328367 virtual)\n",
      "I0228 01:27:43.430560 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1332293 virtual)\n",
      "I0228 01:27:43.432753 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1336490 virtual)\n",
      "I0228 01:27:43.434391 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1340041 virtual)\n",
      "I0228 01:27:43.442961 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1343597 virtual)\n",
      "I0228 01:27:43.454357 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1347501 virtual)\n",
      "I0228 01:27:43.470425 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1351792 virtual)\n",
      "I0228 01:27:43.473474 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1356046 virtual)\n",
      "I0228 01:27:43.487370 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1360462 virtual)\n",
      "I0228 01:27:43.497473 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1364569 virtual)\n",
      "I0228 01:27:43.501909 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1369563 virtual)\n",
      "I0228 01:27:43.518999 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1373097 virtual)\n",
      "I0228 01:27:43.521345 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1374869 virtual)\n",
      "I0228 01:27:43.528645 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1380132 virtual)\n",
      "I0228 01:27:43.537899 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1385227 virtual)\n",
      "I0228 01:27:43.540839 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1389503 virtual)\n",
      "I0228 01:27:43.572073 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1394262 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:43.576115 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1397912 virtual)\n",
      "I0228 01:27:43.580337 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1403013 virtual)\n",
      "I0228 01:27:43.583344 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1406999 virtual)\n",
      "I0228 01:27:43.598009 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1411617 virtual)\n",
      "I0228 01:27:43.601493 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1416530 virtual)\n",
      "I0228 01:27:43.617179 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1420976 virtual)\n",
      "I0228 01:27:43.643844 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1425009 virtual)\n",
      "I0228 01:27:43.648592 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1429711 virtual)\n",
      "I0228 01:27:43.652731 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1434671 virtual)\n",
      "I0228 01:27:43.656060 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1438407 virtual)\n",
      "I0228 01:27:43.659574 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1443252 virtual)\n",
      "I0228 01:27:43.693956 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1447792 virtual)\n",
      "I0228 01:27:43.706141 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1452008 virtual)\n",
      "I0228 01:27:43.711608 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1457195 virtual)\n",
      "I0228 01:27:43.717088 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1462285 virtual)\n",
      "I0228 01:27:43.721309 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1467862 virtual)\n",
      "I0228 01:27:43.744718 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1472657 virtual)\n",
      "I0228 01:27:43.752361 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1478570 virtual)\n",
      "I0228 01:27:43.761385 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1482890 virtual)\n",
      "I0228 01:27:43.767921 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1490127 virtual)\n",
      "I0228 01:27:43.772215 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1497773 virtual)\n",
      "I0228 01:27:43.798707 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1502877 virtual)\n",
      "I0228 01:27:43.802928 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1507639 virtual)\n",
      "I0228 01:27:43.827966 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1512800 virtual)\n",
      "I0228 01:27:43.832311 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1517881 virtual)\n",
      "I0228 01:27:43.836432 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1523234 virtual)\n",
      "I0228 01:27:43.856327 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1527815 virtual)\n",
      "I0228 01:27:43.870805 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1532647 virtual)\n",
      "I0228 01:27:43.879397 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1537658 virtual)\n",
      "I0228 01:27:43.907790 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1547940 virtual)\n",
      "I0228 01:27:43.916151 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1553419 virtual)\n",
      "I0228 01:27:43.921993 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1559261 virtual)\n",
      "I0228 01:27:43.926388 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1564335 virtual)\n",
      "I0228 01:27:43.940473 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1568573 virtual)\n",
      "I0228 01:27:43.968542 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1572843 virtual)\n",
      "I0228 01:27:43.976946 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1577601 virtual)\n",
      "I0228 01:27:43.981751 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1582538 virtual)\n",
      "I0228 01:27:43.986537 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1587071 virtual)\n",
      "I0228 01:27:43.999919 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1592351 virtual)\n",
      "I0228 01:27:44.041777 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1598301 virtual)\n",
      "I0228 01:27:44.046308 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1604071 virtual)\n",
      "I0228 01:27:44.050260 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1610037 virtual)\n",
      "I0228 01:27:44.053635 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1615118 virtual)\n",
      "I0228 01:27:44.091585 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1620916 virtual)\n",
      "I0228 01:27:44.096205 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1627139 virtual)\n",
      "I0228 01:27:44.101087 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1632635 virtual)\n",
      "I0228 01:27:44.104157 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1636757 virtual)\n",
      "I0228 01:27:44.107970 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1642288 virtual)\n",
      "I0228 01:27:44.152979 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1647184 virtual)\n",
      "I0228 01:27:44.166380 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1652434 virtual)\n",
      "I0228 01:27:44.170224 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1658210 virtual)\n",
      "I0228 01:27:44.176773 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1663210 virtual)\n",
      "I0228 01:27:44.179809 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1668494 virtual)\n",
      "I0228 01:27:44.225169 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1673876 virtual)\n",
      "I0228 01:27:44.230784 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1679127 virtual)\n",
      "I0228 01:27:44.237600 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1685931 virtual)\n",
      "I0228 01:27:44.241341 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1694159 virtual)\n",
      "I0228 01:27:44.244933 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1702731 virtual)\n",
      "I0228 01:27:44.286178 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1710804 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:44.290410 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1718825 virtual)\n",
      "I0228 01:27:44.298052 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1723914 virtual)\n",
      "I0228 01:27:44.302970 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1729364 virtual)\n",
      "I0228 01:27:44.306318 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1735342 virtual)\n",
      "I0228 01:27:44.349212 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1740928 virtual)\n",
      "I0228 01:27:44.353687 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1747155 virtual)\n",
      "I0228 01:27:44.388381 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1752977 virtual)\n",
      "I0228 01:27:44.404967 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1758611 virtual)\n",
      "I0228 01:27:44.410193 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1763421 virtual)\n",
      "I0228 01:27:44.450590 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1769148 virtual)\n",
      "I0228 01:27:44.455081 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1774086 virtual)\n",
      "I0228 01:27:44.460212 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1779286 virtual)\n",
      "I0228 01:27:44.471485 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1783940 virtual)\n",
      "I0228 01:27:44.482853 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1789163 virtual)\n",
      "I0228 01:27:44.517198 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1794670 virtual)\n",
      "I0228 01:27:44.526320 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1799955 virtual)\n",
      "I0228 01:27:44.531833 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1804664 virtual)\n",
      "I0228 01:27:44.536708 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1809503 virtual)\n",
      "I0228 01:27:44.541411 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1814331 virtual)\n",
      "I0228 01:27:44.588281 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1819570 virtual)\n",
      "I0228 01:27:44.592957 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1824719 virtual)\n",
      "I0228 01:27:44.597998 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1829810 virtual)\n",
      "I0228 01:27:44.602137 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1834915 virtual)\n",
      "I0228 01:27:44.607460 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1840005 virtual)\n",
      "I0228 01:27:44.651974 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1845214 virtual)\n",
      "I0228 01:27:44.656698 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1850183 virtual)\n",
      "I0228 01:27:44.660623 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1854938 virtual)\n",
      "I0228 01:27:44.663707 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1860294 virtual)\n",
      "I0228 01:27:44.674751 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1865279 virtual)\n",
      "I0228 01:27:44.717677 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1870948 virtual)\n",
      "I0228 01:27:44.723865 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1875665 virtual)\n",
      "I0228 01:27:44.729912 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1880483 virtual)\n",
      "I0228 01:27:44.732777 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1885116 virtual)\n",
      "I0228 01:27:44.735781 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1890491 virtual)\n",
      "I0228 01:27:44.778131 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1895404 virtual)\n",
      "I0228 01:27:44.783654 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1900369 virtual)\n",
      "I0228 01:27:44.788348 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1905171 virtual)\n",
      "I0228 01:27:44.793089 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1910438 virtual)\n",
      "I0228 01:27:44.799041 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1916009 virtual)\n",
      "I0228 01:27:44.843564 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1921059 virtual)\n",
      "I0228 01:27:44.848489 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1925818 virtual)\n",
      "I0228 01:27:44.853468 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1931200 virtual)\n",
      "I0228 01:27:44.856980 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1936305 virtual)\n",
      "I0228 01:27:44.866126 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1941224 virtual)\n",
      "I0228 01:27:44.905307 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1945769 virtual)\n",
      "I0228 01:27:44.910254 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1950572 virtual)\n",
      "I0228 01:27:44.914643 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1955629 virtual)\n",
      "I0228 01:27:44.919088 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (1960731 virtual)\n",
      "I0228 01:27:44.953456 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (1965723 virtual)\n",
      "I0228 01:27:44.968380 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (1970860 virtual)\n",
      "I0228 01:27:44.971184 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (1975467 virtual)\n",
      "I0228 01:27:44.975466 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (1980966 virtual)\n",
      "I0228 01:27:44.978495 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (1986159 virtual)\n",
      "I0228 01:27:44.997247 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (1991181 virtual)\n",
      "I0228 01:27:45.027395 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (1995845 virtual)\n",
      "I0228 01:27:45.032046 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2000920 virtual)\n",
      "I0228 01:27:45.040140 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2006296 virtual)\n",
      "I0228 01:27:45.043447 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2011346 virtual)\n",
      "I0228 01:27:45.060385 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2016562 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:45.099742 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2022056 virtual)\n",
      "I0228 01:27:45.104253 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2027295 virtual)\n",
      "I0228 01:27:45.107209 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2032389 virtual)\n",
      "I0228 01:27:45.110275 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2037707 virtual)\n",
      "I0228 01:27:45.123879 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2042909 virtual)\n",
      "I0228 01:27:45.154603 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2048014 virtual)\n",
      "I0228 01:27:45.158896 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2052827 virtual)\n",
      "I0228 01:27:45.174581 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2057669 virtual)\n",
      "I0228 01:27:45.178114 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2062510 virtual)\n",
      "I0228 01:27:45.190742 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2067202 virtual)\n",
      "I0228 01:27:45.222561 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2071785 virtual)\n",
      "I0228 01:27:45.228316 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2076587 virtual)\n",
      "I0228 01:27:45.240360 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2081446 virtual)\n",
      "I0228 01:27:45.243538 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2086861 virtual)\n",
      "I0228 01:27:45.257607 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2091685 virtual)\n",
      "I0228 01:27:45.288182 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2096924 virtual)\n",
      "I0228 01:27:45.293900 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2102107 virtual)\n",
      "I0228 01:27:45.302680 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2107321 virtual)\n",
      "I0228 01:27:45.307963 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2112655 virtual)\n",
      "I0228 01:27:45.319412 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2117815 virtual)\n",
      "I0228 01:27:45.349059 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2123274 virtual)\n",
      "I0228 01:27:45.352981 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2127817 virtual)\n",
      "I0228 01:27:45.364413 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2132786 virtual)\n",
      "I0228 01:27:45.375761 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2137587 virtual)\n",
      "I0228 01:27:45.380373 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2142551 virtual)\n",
      "I0228 01:27:45.412418 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2146975 virtual)\n",
      "I0228 01:27:45.417965 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2152236 virtual)\n",
      "I0228 01:27:45.428605 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2157549 virtual)\n",
      "I0228 01:27:45.443207 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2162900 virtual)\n",
      "I0228 01:27:45.448155 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2168020 virtual)\n",
      "I0228 01:27:45.475991 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2173269 virtual)\n",
      "I0228 01:27:45.481705 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2178359 virtual)\n",
      "I0228 01:27:45.491341 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2183118 virtual)\n",
      "I0228 01:27:45.503417 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2188082 virtual)\n",
      "I0228 01:27:45.508536 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2193406 virtual)\n",
      "I0228 01:27:45.533756 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2198289 virtual)\n",
      "I0228 01:27:45.550748 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2203130 virtual)\n",
      "I0228 01:27:45.560154 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2207721 virtual)\n",
      "I0228 01:27:45.570613 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2213203 virtual)\n",
      "I0228 01:27:45.575302 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2218005 virtual)\n",
      "I0228 01:27:45.600267 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2223223 virtual)\n",
      "I0228 01:27:45.616978 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2228861 virtual)\n",
      "I0228 01:27:45.621498 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2233697 virtual)\n",
      "I0228 01:27:45.633129 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2238913 virtual)\n",
      "I0228 01:27:45.641757 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2244032 virtual)\n",
      "I0228 01:27:45.659139 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2248772 virtual)\n",
      "I0228 01:27:45.676706 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2253791 virtual)\n",
      "I0228 01:27:45.682229 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2259114 virtual)\n",
      "I0228 01:27:45.703470 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2264057 virtual)\n",
      "I0228 01:27:45.708400 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2269298 virtual)\n",
      "I0228 01:27:45.722703 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2273997 virtual)\n",
      "I0228 01:27:45.742513 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2279384 virtual)\n",
      "I0228 01:27:45.746637 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2284177 virtual)\n",
      "I0228 01:27:45.767590 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2289152 virtual)\n",
      "I0228 01:27:45.771884 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2294160 virtual)\n",
      "I0228 01:27:45.784844 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2299141 virtual)\n",
      "I0228 01:27:45.807445 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2304090 virtual)\n",
      "I0228 01:27:45.813917 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2308962 virtual)\n",
      "I0228 01:27:45.828703 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2313823 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:45.833493 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2319150 virtual)\n",
      "I0228 01:27:45.846105 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2324289 virtual)\n",
      "I0228 01:27:45.873632 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2329350 virtual)\n",
      "I0228 01:27:45.878266 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2334643 virtual)\n",
      "I0228 01:27:45.890186 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2339692 virtual)\n",
      "I0228 01:27:45.901132 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2344877 virtual)\n",
      "I0228 01:27:45.906301 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2350029 virtual)\n",
      "I0228 01:27:45.940521 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2354850 virtual)\n",
      "I0228 01:27:45.944837 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2360259 virtual)\n",
      "I0228 01:27:45.952817 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2365614 virtual)\n",
      "I0228 01:27:45.970833 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2370737 virtual)\n",
      "I0228 01:27:45.975088 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2375527 virtual)\n",
      "I0228 01:27:46.003512 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2380243 virtual)\n",
      "I0228 01:27:46.008028 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2384922 virtual)\n",
      "I0228 01:27:46.027345 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2390511 virtual)\n",
      "I0228 01:27:46.044580 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2395241 virtual)\n",
      "I0228 01:27:46.046550 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2400270 virtual)\n",
      "I0228 01:27:46.065218 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2405753 virtual)\n",
      "I0228 01:27:46.073524 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2410910 virtual)\n",
      "I0228 01:27:46.106598 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2416012 virtual)\n",
      "I0228 01:27:46.110180 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2420845 virtual)\n",
      "I0228 01:27:46.112184 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2425758 virtual)\n",
      "I0228 01:27:46.125058 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2430647 virtual)\n",
      "I0228 01:27:46.134886 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2435460 virtual)\n",
      "I0228 01:27:46.176432 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2440728 virtual)\n",
      "I0228 01:27:46.179471 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2446252 virtual)\n",
      "I0228 01:27:46.181958 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2451544 virtual)\n",
      "I0228 01:27:46.195226 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2456346 virtual)\n",
      "I0228 01:27:46.200388 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2461416 virtual)\n",
      "I0228 01:27:46.234259 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2466654 virtual)\n",
      "I0228 01:27:46.238681 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2471543 virtual)\n",
      "I0228 01:27:46.246792 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2476304 virtual)\n",
      "I0228 01:27:46.255452 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2481503 virtual)\n",
      "I0228 01:27:46.262808 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2486437 virtual)\n",
      "I0228 01:27:46.300873 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2491688 virtual)\n",
      "I0228 01:27:46.306882 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2497300 virtual)\n",
      "I0228 01:27:46.317245 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2502428 virtual)\n",
      "I0228 01:27:46.320170 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2507662 virtual)\n",
      "I0228 01:27:46.327769 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2512415 virtual)\n",
      "I0228 01:27:46.365759 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2516899 virtual)\n",
      "I0228 01:27:46.369859 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2521610 virtual)\n",
      "I0228 01:27:46.376372 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2526978 virtual)\n",
      "I0228 01:27:46.385888 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2532200 virtual)\n",
      "I0228 01:27:46.391434 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2537538 virtual)\n",
      "I0228 01:27:46.429266 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2542458 virtual)\n",
      "I0228 01:27:46.440174 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2547273 virtual)\n",
      "I0228 01:27:46.445253 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2552776 virtual)\n",
      "I0228 01:27:46.450151 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2557593 virtual)\n",
      "I0228 01:27:46.455342 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2562282 virtual)\n",
      "I0228 01:27:46.487548 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2567323 virtual)\n",
      "I0228 01:27:46.498831 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2572279 virtual)\n",
      "I0228 01:27:46.512514 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2577233 virtual)\n",
      "I0228 01:27:46.515678 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2582612 virtual)\n",
      "I0228 01:27:46.541609 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2587452 virtual)\n",
      "I0228 01:27:46.548097 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2592705 virtual)\n",
      "I0228 01:27:46.563192 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2597927 virtual)\n",
      "I0228 01:27:46.574926 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2602734 virtual)\n",
      "I0228 01:27:46.580835 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2607825 virtual)\n",
      "I0228 01:27:46.600162 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2613337 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:27:46.611358 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2618745 virtual)\n",
      "I0228 01:27:46.622722 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2623848 virtual)\n",
      "I0228 01:27:46.638567 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2629193 virtual)\n",
      "I0228 01:27:46.650912 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2634622 virtual)\n",
      "I0228 01:27:46.662535 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2639955 virtual)\n",
      "I0228 01:27:46.677885 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2644614 virtual)\n",
      "I0228 01:27:46.689487 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2649815 virtual)\n",
      "I0228 01:27:46.700769 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2654573 virtual)\n",
      "I0228 01:27:46.715415 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2659823 virtual)\n",
      "I0228 01:27:46.737116 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2664558 virtual)\n",
      "I0228 01:27:46.745753 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2669658 virtual)\n",
      "I0228 01:27:46.756217 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2674339 virtual)\n",
      "I0228 01:27:46.766228 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2679389 virtual)\n",
      "I0228 01:27:46.784838 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2684651 virtual)\n",
      "I0228 01:27:46.804131 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2689903 virtual)\n",
      "I0228 01:27:46.809321 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2695402 virtual)\n",
      "I0228 01:27:46.821064 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2700487 virtual)\n",
      "I0228 01:27:46.825542 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2705977 virtual)\n",
      "I0228 01:27:46.852661 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2711002 virtual)\n",
      "I0228 01:27:46.863944 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2716398 virtual)\n",
      "I0228 01:27:46.871994 140370323834688 text_analysis.py:506] 552 batches submitted to accumulate stats from 35328 documents (2721543 virtual)\n",
      "I0228 01:27:46.878663 140370323834688 text_analysis.py:506] 553 batches submitted to accumulate stats from 35392 documents (2726524 virtual)\n",
      "I0228 01:27:46.890680 140370323834688 text_analysis.py:506] 554 batches submitted to accumulate stats from 35456 documents (2731527 virtual)\n",
      "I0228 01:27:46.924707 140370323834688 text_analysis.py:506] 555 batches submitted to accumulate stats from 35520 documents (2736614 virtual)\n",
      "I0228 01:27:46.931032 140370323834688 text_analysis.py:506] 556 batches submitted to accumulate stats from 35584 documents (2741973 virtual)\n",
      "I0228 01:27:46.942613 140370323834688 text_analysis.py:506] 557 batches submitted to accumulate stats from 35648 documents (2747487 virtual)\n",
      "I0228 01:27:46.945724 140370323834688 text_analysis.py:506] 558 batches submitted to accumulate stats from 35712 documents (2753102 virtual)\n",
      "I0228 01:27:46.958511 140370323834688 text_analysis.py:506] 559 batches submitted to accumulate stats from 35776 documents (2758299 virtual)\n",
      "I0228 01:27:46.990041 140370323834688 text_analysis.py:506] 560 batches submitted to accumulate stats from 35840 documents (2763361 virtual)\n",
      "I0228 01:27:46.995111 140370323834688 text_analysis.py:506] 561 batches submitted to accumulate stats from 35904 documents (2768438 virtual)\n",
      "I0228 01:27:47.008249 140370323834688 text_analysis.py:506] 562 batches submitted to accumulate stats from 35968 documents (2773424 virtual)\n",
      "I0228 01:27:47.013004 140370323834688 text_analysis.py:506] 563 batches submitted to accumulate stats from 36032 documents (2778548 virtual)\n",
      "I0228 01:27:47.023407 140370323834688 text_analysis.py:506] 564 batches submitted to accumulate stats from 36096 documents (2784123 virtual)\n",
      "I0228 01:27:47.057241 140370323834688 text_analysis.py:506] 565 batches submitted to accumulate stats from 36160 documents (2789016 virtual)\n",
      "I0228 01:27:47.061378 140370323834688 text_analysis.py:506] 566 batches submitted to accumulate stats from 36224 documents (2793951 virtual)\n",
      "I0228 01:27:47.076346 140370323834688 text_analysis.py:506] 567 batches submitted to accumulate stats from 36288 documents (2795719 virtual)\n",
      "I0228 01:27:47.142558 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:27:47.163348 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:27:47.164333 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:27:47.167401 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:27:47.166190 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:27:47.183232 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:27:47.191976 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:27:47.146451 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:27:47.189384 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:27:47.195450 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:27:47.580711 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:27:47.606706 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2796164 virtual documents\n",
      "/home/dhamzeia/Thesis/BiomedicalTopicModelling/contextualized_topic_models/models/ctm.py:511: Warning: This is an experimental feature that we has not been fully tested. Refer to the following issue:https://github.com/MilaNLProc/contextualized-topic-models/issues/38\n",
      "  Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores\n",
      "{'epoch': 24, 'cv': 0.6580009225959259, 'umass': -3.6947330977349706, 'uci': -0.1328511390185115, 'npmi': 0.061833517570755, 'rbo': 1.0, 'td': 0.976, 'train_loss': 642.7324733541973, 'topics': [['c0117438', 'c0605290', 'salivary', 'c0001275', 'c0536858', 'ref', 'c0861880', 'northern', 'c0020933', 'c0851891', 'aeruginosa', 'c1641393', 'c0015024', 'goat', 'c1446219', 'c0026231', 'cow', 'preform', 'disable', 'c0076080', 'c0063355', 'booster', 'insufficiently', 'c0595939', 'c0318483', 'cessation'], ['c0012634', 'c0011900', 'child', 'c1457887', 'clinical', 'c0546788', 'cause', 'c3714514', 'common', 'c0010076', 'c1446409', 'c0019994', 'c0032285', 'c0019993', 'confirm', 'c1306577', 'sars-cov-2', 'c0809949', 'c0015967', 'c0221423', 'associate', 'early', 'present', 'infant', 'c0035648', 'c0021708'], ['c0025080', 'c0543467', 'undergo', 'perform', 'c0009566', 'c0229962', 'postoperative', 'c0005898', 'c0728940', 'technique', 'c0031150', 'procedure', 'complication', 'c1522577', 'c0087111', 'c0038930', 'c0850292', 'operative', 'feasible', 'c0019080', 'c0582175', 'surgical', 'c0221198', 'preoperative', 'conventional', 'c0002940'], ['compare', 'difference', 'c0243095', 'significant', 'significantly', 'associate', 'receive', 'determine', 'assess', 'measure', 'study', 'primary', 'incidence', 'association', 'decrease', 'c0008976', 'reduce', 'c2603343', 'c0430022', 'trial', 'measurement', 'c0199470', 'versus', 'c0005516', 'include', 'c0206035'], ['economic', 'crisis', 'policy', 'political', 'chapter', 'market', 'argue', 'challenge', 'right', 'society', 'way', 'draw', 'opportunity', 'international', 'perspective', 'industry', 'security', 'economy', 'face', 'trade', 'public', 'world', 'inequality', 'bring', 'c2700280', 'disaster'], ['c0042210', 'c1514562', 'c0033684', 'c0042736', 'c1254351', 'c1167622', 'c0017337', 'c0014442', 'c0030956', 'interaction', 'c1148560', 'c0035668', 'c0029224', 'novel', 'c0003241', 'c0678594', 'c0003320', 'c0026882', 'c0574031', 'c0596901', 'c0003250', 'c0003316', 'c0042774', 'potential', 'vitro', 'c0002520'], ['c1171362', 'c0007634', 'c0025929', 'role', 'c0007613', 'induce', 'mechanism', 'c0021368', 'c0079189', 'c0039194', 'c0024109', 'c0017262', 'activation', 'mouse', 'c0040300', 'c3539881', 'c0024432', 'play', 'c0301872', 'suggest', 'induction', 'c0013081', 'demonstrate', 'c0006104', 'activate', 'c0041904'], ['c3161035', 'propose', 'c0002045', 'c0025663', 'performance', 'base', 'application', 'learn', 'paper', 'apply', 'prediction', 'accuracy', 'c0033213', 'solution', 'c0150098', 'feature', 'sensor', 'machine', 'c1710191', 'c0282574', 'representation', 'solve', 'c0679083', 'method', 'automate', 'compute'], ['c1257890', 'need', 'c0679646', 'conduct', 'report', 'c2603343', 'impact', 'c0027361', 'include', 'c0086388', 'train', 'c0018724', 'c0242481', 'search', 'provide', 'c0038951', 'c0282574', 'measure', 'information', 'evidence', 'c0184661', 'c0035168', 'physical', 'c0282443', 'c0025353', 'c0030971'], ['c0017446', 'c0042776', 'c0032098', 'sample', 'c0086418', 'c1705920', 'c0003062', 'pathogen', 'c0442726', 'c0242781', 'c0017428', 'genetic', 'population', 'c0017337', 'c0684063', 'c0012984', 'different', 'c0439663', 'c0004611', 'c0039005', 'host', 'diversity', 'c0014406', 'c1764827', 'c0007452', 'transmission']]}\n",
      "Epoch: [26/250]\tSamples: [954876/9181500]\tTrain Loss: 642.881061675482\tTime: 0:00:04.502603\n",
      "Epoch: [27/250]\tSamples: [991602/9181500]\tTrain Loss: 642.7403102110902\tTime: 0:00:04.441508\n",
      "Epoch: [28/250]\tSamples: [1028328/9181500]\tTrain Loss: 642.5931444812599\tTime: 0:00:04.444656\n",
      "Epoch: [29/250]\tSamples: [1065054/9181500]\tTrain Loss: 642.688645252597\tTime: 0:00:04.543837\n",
      "Epoch: [30/250]\tSamples: [1101780/9181500]\tTrain Loss: 642.6828558105974\tTime: 0:00:04.564128\n",
      "Epoch: [31/250]\tSamples: [1138506/9181500]\tTrain Loss: 642.5655405698619\tTime: 0:00:04.699105\n",
      "Epoch: [32/250]\tSamples: [1175232/9181500]\tTrain Loss: 642.4366989879445\tTime: 0:00:04.669796\n",
      "Epoch: [33/250]\tSamples: [1211958/9181500]\tTrain Loss: 642.5957269500831\tTime: 0:00:04.690638\n",
      "Epoch: [34/250]\tSamples: [1248684/9181500]\tTrain Loss: 642.4992285565744\tTime: 0:00:04.652222\n",
      "Epoch: [35/250]\tSamples: [1285410/9181500]\tTrain Loss: 642.4084943440955\tTime: 0:00:04.705764\n",
      "Epoch: [36/250]\tSamples: [1322136/9181500]\tTrain Loss: 642.1411409619548\tTime: 0:00:04.741190\n",
      "Epoch: [37/250]\tSamples: [1358862/9181500]\tTrain Loss: 642.4569510530687\tTime: 0:00:04.758787\n",
      "Epoch: [38/250]\tSamples: [1395588/9181500]\tTrain Loss: 642.4143836068113\tTime: 0:00:04.746784\n",
      "Epoch: [39/250]\tSamples: [1432314/9181500]\tTrain Loss: 642.4554014654128\tTime: 0:00:04.703439\n",
      "Epoch: [40/250]\tSamples: [1469040/9181500]\tTrain Loss: 642.4117352999237\tTime: 0:00:04.694424\n",
      "Epoch: [41/250]\tSamples: [1505766/9181500]\tTrain Loss: 642.2645162827424\tTime: 0:00:04.727021\n",
      "Epoch: [42/250]\tSamples: [1542492/9181500]\tTrain Loss: 642.3159123049746\tTime: 0:00:04.687841\n",
      "Epoch: [43/250]\tSamples: [1579218/9181500]\tTrain Loss: 642.5574809909873\tTime: 0:00:04.715756\n",
      "Epoch: [44/250]\tSamples: [1615944/9181500]\tTrain Loss: 642.3476775223955\tTime: 0:00:04.709322\n",
      "Epoch: [45/250]\tSamples: [1652670/9181500]\tTrain Loss: 642.410339298964\tTime: 0:00:04.778912\n",
      "Epoch: [46/250]\tSamples: [1689396/9181500]\tTrain Loss: 642.471304389261\tTime: 0:00:04.736699\n",
      "Epoch: [47/250]\tSamples: [1726122/9181500]\tTrain Loss: 642.43948163001\tTime: 0:00:04.722202\n",
      "Epoch: [48/250]\tSamples: [1762848/9181500]\tTrain Loss: 642.4276397128397\tTime: 0:00:04.724315\n",
      "Epoch: [49/250]\tSamples: [1799574/9181500]\tTrain Loss: 642.1863852201097\tTime: 0:00:04.733224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:29:46.588713 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [50/250]\tSamples: [1836300/9181500]\tTrain Loss: 642.4195572661398\tTime: 0:00:04.745931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:29:47.422953 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:29:48.106961 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:29:48.920309 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:29:49.452473 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:29:49.459046 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:29:50.213418 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:29:50.883162 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:29:51.676366 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:29:52.203933 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:29:52.208915 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:29:52.980303 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:29:53.654122 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:29:54.451458 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:29:54.974947 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:29:54.981449 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:29:55.727676 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:29:56.397374 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:29:57.189967 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:29:57.706529 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:29:57.727961 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:29:58.285999 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (-35891 virtual)\n",
      "I0228 01:29:58.815155 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (-212214 virtual)\n",
      "I0228 01:29:59.211808 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (-509952 virtual)\n",
      "I0228 01:29:59.216051 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (-509384 virtual)\n",
      "I0228 01:29:59.252499 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (-516956 virtual)\n",
      "I0228 01:29:59.263316 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (-516221 virtual)\n",
      "I0228 01:29:59.386693 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (-542931 virtual)\n",
      "I0228 01:29:59.391440 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (-540897 virtual)\n",
      "I0228 01:29:59.393745 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (-539131 virtual)\n",
      "I0228 01:29:59.398383 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (-537433 virtual)\n",
      "I0228 01:29:59.401410 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (-537077 virtual)\n",
      "I0228 01:30:00.300368 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:30:00.305184 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:30:00.307435 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:30:00.309152 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:30:00.313151 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:30:00.314573 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:30:00.318217 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:30:00.317266 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:30:00.327406 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:30:00.304619 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:30:00.731145 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:30:00.751132 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 229950 virtual documents\n",
      "I0228 01:30:02.630084 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 1000 documents\n",
      "I0228 01:30:02.647155 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 2000 documents\n",
      "I0228 01:30:02.671196 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 3000 documents\n",
      "I0228 01:30:02.689223 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 4000 documents\n",
      "I0228 01:30:02.705684 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 5000 documents\n",
      "I0228 01:30:02.720633 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 6000 documents\n",
      "I0228 01:30:02.736246 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 7000 documents\n",
      "I0228 01:30:02.752269 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 8000 documents\n",
      "I0228 01:30:02.769162 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 9000 documents\n",
      "I0228 01:30:02.786921 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 10000 documents\n",
      "I0228 01:30:02.805675 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 11000 documents\n",
      "I0228 01:30:02.821573 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 12000 documents\n",
      "I0228 01:30:02.836024 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 13000 documents\n",
      "I0228 01:30:02.850562 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 14000 documents\n",
      "I0228 01:30:02.865545 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 15000 documents\n",
      "I0228 01:30:02.881024 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 16000 documents\n",
      "I0228 01:30:02.894601 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 17000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:02.908136 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 18000 documents\n",
      "I0228 01:30:02.920975 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 19000 documents\n",
      "I0228 01:30:02.935132 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 20000 documents\n",
      "I0228 01:30:02.951447 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 21000 documents\n",
      "I0228 01:30:02.968854 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 22000 documents\n",
      "I0228 01:30:02.987164 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 23000 documents\n",
      "I0228 01:30:03.005268 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 24000 documents\n",
      "I0228 01:30:03.022264 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 25000 documents\n",
      "I0228 01:30:03.045391 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 26000 documents\n",
      "I0228 01:30:03.062839 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 27000 documents\n",
      "I0228 01:30:03.080398 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 28000 documents\n",
      "I0228 01:30:03.097677 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 29000 documents\n",
      "I0228 01:30:03.114831 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 30000 documents\n",
      "I0228 01:30:03.132605 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 31000 documents\n",
      "I0228 01:30:03.150051 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 32000 documents\n",
      "I0228 01:30:03.167575 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 33000 documents\n",
      "I0228 01:30:03.184940 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 34000 documents\n",
      "I0228 01:30:03.202746 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 35000 documents\n",
      "I0228 01:30:03.220447 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 36000 documents\n",
      "I0228 01:30:03.385484 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:30:03.787448 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (4980 virtual)\n",
      "I0228 01:30:03.791893 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10307 virtual)\n",
      "I0228 01:30:03.796179 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16414 virtual)\n",
      "I0228 01:30:03.804282 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21215 virtual)\n",
      "I0228 01:30:03.807299 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27004 virtual)\n",
      "I0228 01:30:03.824346 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32207 virtual)\n",
      "I0228 01:30:03.829722 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37655 virtual)\n",
      "I0228 01:30:03.833914 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43431 virtual)\n",
      "I0228 01:30:03.835982 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48712 virtual)\n",
      "I0228 01:30:03.838119 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54323 virtual)\n",
      "I0228 01:30:03.853588 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60103 virtual)\n",
      "I0228 01:30:03.872582 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65442 virtual)\n",
      "I0228 01:30:03.876019 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70624 virtual)\n",
      "I0228 01:30:03.879091 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (75859 virtual)\n",
      "I0228 01:30:03.885604 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81509 virtual)\n",
      "I0228 01:30:03.924884 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87623 virtual)\n",
      "I0228 01:30:03.936331 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93416 virtual)\n",
      "I0228 01:30:03.942188 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (98773 virtual)\n",
      "I0228 01:30:03.951725 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104047 virtual)\n",
      "I0228 01:30:03.956660 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109514 virtual)\n",
      "I0228 01:30:03.993379 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (114907 virtual)\n",
      "I0228 01:30:04.003306 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120170 virtual)\n",
      "I0228 01:30:04.008189 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (125847 virtual)\n",
      "I0228 01:30:04.019815 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (131538 virtual)\n",
      "I0228 01:30:04.026682 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137175 virtual)\n",
      "I0228 01:30:04.072654 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (142576 virtual)\n",
      "I0228 01:30:04.077233 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148030 virtual)\n",
      "I0228 01:30:04.080488 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154156 virtual)\n",
      "I0228 01:30:04.084251 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160081 virtual)\n",
      "I0228 01:30:04.092022 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (165472 virtual)\n",
      "I0228 01:30:04.135536 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (170939 virtual)\n",
      "I0228 01:30:04.142194 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176352 virtual)\n",
      "I0228 01:30:04.146991 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (181651 virtual)\n",
      "I0228 01:30:04.152798 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (186789 virtual)\n",
      "I0228 01:30:04.159807 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (192564 virtual)\n",
      "I0228 01:30:04.202309 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (197886 virtual)\n",
      "I0228 01:30:04.208073 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (203407 virtual)\n",
      "I0228 01:30:04.218581 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (208963 virtual)\n",
      "I0228 01:30:04.224411 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (214435 virtual)\n",
      "I0228 01:30:04.228557 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220054 virtual)\n",
      "I0228 01:30:04.269953 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (225400 virtual)\n",
      "I0228 01:30:04.279040 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (232909 virtual)\n",
      "I0228 01:30:04.283205 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (238438 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:04.289703 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (244392 virtual)\n",
      "I0228 01:30:04.296139 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (249922 virtual)\n",
      "I0228 01:30:04.334915 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (255158 virtual)\n",
      "I0228 01:30:04.347411 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (260438 virtual)\n",
      "I0228 01:30:04.352932 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (265773 virtual)\n",
      "I0228 01:30:04.363639 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (271299 virtual)\n",
      "I0228 01:30:04.368277 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (276453 virtual)\n",
      "I0228 01:30:04.399444 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (281973 virtual)\n",
      "I0228 01:30:04.421825 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288037 virtual)\n",
      "I0228 01:30:04.428985 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (292873 virtual)\n",
      "I0228 01:30:04.433480 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (298597 virtual)\n",
      "I0228 01:30:04.438132 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304108 virtual)\n",
      "I0228 01:30:04.462046 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (309240 virtual)\n",
      "I0228 01:30:04.487771 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (314799 virtual)\n",
      "I0228 01:30:04.496734 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (320631 virtual)\n",
      "I0228 01:30:04.500980 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (325850 virtual)\n",
      "I0228 01:30:04.505765 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (332130 virtual)\n",
      "I0228 01:30:04.530660 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (337900 virtual)\n",
      "I0228 01:30:04.559017 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (343723 virtual)\n",
      "I0228 01:30:04.563674 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (349304 virtual)\n",
      "I0228 01:30:04.568489 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (355163 virtual)\n",
      "I0228 01:30:04.571676 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (361024 virtual)\n",
      "I0228 01:30:04.593796 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (366537 virtual)\n",
      "I0228 01:30:04.627293 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (372157 virtual)\n",
      "I0228 01:30:04.632122 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (378118 virtual)\n",
      "I0228 01:30:04.636200 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (383990 virtual)\n",
      "I0228 01:30:04.644159 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (389065 virtual)\n",
      "I0228 01:30:04.662156 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (393951 virtual)\n",
      "I0228 01:30:04.698172 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398202 virtual)\n",
      "I0228 01:30:04.702958 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (401664 virtual)\n",
      "I0228 01:30:04.708444 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (405568 virtual)\n",
      "I0228 01:30:04.713870 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (410195 virtual)\n",
      "I0228 01:30:04.728564 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (414564 virtual)\n",
      "I0228 01:30:04.765449 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (418954 virtual)\n",
      "I0228 01:30:04.773025 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (423122 virtual)\n",
      "I0228 01:30:04.777253 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (427529 virtual)\n",
      "I0228 01:30:04.780187 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (431881 virtual)\n",
      "I0228 01:30:04.783243 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (436865 virtual)\n",
      "I0228 01:30:04.815913 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (441715 virtual)\n",
      "I0228 01:30:04.819919 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (446269 virtual)\n",
      "I0228 01:30:04.823559 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (450229 virtual)\n",
      "I0228 01:30:04.829308 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (454348 virtual)\n",
      "I0228 01:30:04.832484 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (457795 virtual)\n",
      "I0228 01:30:04.864993 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (461452 virtual)\n",
      "I0228 01:30:04.868961 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (465384 virtual)\n",
      "I0228 01:30:04.880431 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (469488 virtual)\n",
      "I0228 01:30:04.883834 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (474359 virtual)\n",
      "I0228 01:30:04.886631 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (479001 virtual)\n",
      "I0228 01:30:04.919692 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (483540 virtual)\n",
      "I0228 01:30:04.924578 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (488190 virtual)\n",
      "I0228 01:30:04.928880 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (492919 virtual)\n",
      "I0228 01:30:04.933276 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (497624 virtual)\n",
      "I0228 01:30:04.936883 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (501951 virtual)\n",
      "I0228 01:30:04.962342 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (506220 virtual)\n",
      "I0228 01:30:04.969408 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (511590 virtual)\n",
      "I0228 01:30:04.975711 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (516569 virtual)\n",
      "I0228 01:30:04.989854 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (521086 virtual)\n",
      "I0228 01:30:04.993328 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (525301 virtual)\n",
      "I0228 01:30:05.013887 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (530053 virtual)\n",
      "I0228 01:30:05.027301 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (533956 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:05.032039 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (538376 virtual)\n",
      "I0228 01:30:05.046002 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (542205 virtual)\n",
      "I0228 01:30:05.049399 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (546723 virtual)\n",
      "I0228 01:30:05.065053 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (551110 virtual)\n",
      "I0228 01:30:05.090400 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (555327 virtual)\n",
      "I0228 01:30:05.093018 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (560014 virtual)\n",
      "I0228 01:30:05.103744 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (563839 virtual)\n",
      "I0228 01:30:05.107988 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (568567 virtual)\n",
      "I0228 01:30:05.120394 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (572512 virtual)\n",
      "I0228 01:30:05.137552 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (576636 virtual)\n",
      "I0228 01:30:05.143826 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (580519 virtual)\n",
      "I0228 01:30:05.149835 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (583468 virtual)\n",
      "I0228 01:30:05.164827 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (588107 virtual)\n",
      "I0228 01:30:05.168983 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (593543 virtual)\n",
      "I0228 01:30:05.186077 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (599025 virtual)\n",
      "I0228 01:30:05.195296 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (603868 virtual)\n",
      "I0228 01:30:05.200451 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (609077 virtual)\n",
      "I0228 01:30:05.217253 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (614860 virtual)\n",
      "I0228 01:30:05.222003 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (619866 virtual)\n",
      "I0228 01:30:05.232880 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (624419 virtual)\n",
      "I0228 01:30:05.240370 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (629283 virtual)\n",
      "I0228 01:30:05.244173 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (633200 virtual)\n",
      "I0228 01:30:05.272509 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (638223 virtual)\n",
      "I0228 01:30:05.289676 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (643508 virtual)\n",
      "I0228 01:30:05.297657 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (648945 virtual)\n",
      "I0228 01:30:05.301594 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (654536 virtual)\n",
      "I0228 01:30:05.305534 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (659566 virtual)\n",
      "I0228 01:30:05.340284 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (664092 virtual)\n",
      "I0228 01:30:05.352647 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (669308 virtual)\n",
      "I0228 01:30:05.357227 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (674653 virtual)\n",
      "I0228 01:30:05.361989 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (679797 virtual)\n",
      "I0228 01:30:05.364863 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (684773 virtual)\n",
      "I0228 01:30:05.400150 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (689748 virtual)\n",
      "I0228 01:30:05.414584 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (694886 virtual)\n",
      "I0228 01:30:05.418411 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (699866 virtual)\n",
      "I0228 01:30:05.421389 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (705230 virtual)\n",
      "I0228 01:30:05.429905 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (710399 virtual)\n",
      "I0228 01:30:05.447200 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (715531 virtual)\n",
      "I0228 01:30:05.478049 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (720922 virtual)\n",
      "I0228 01:30:05.482281 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (725769 virtual)\n",
      "I0228 01:30:05.486189 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (731410 virtual)\n",
      "I0228 01:30:05.489904 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (736835 virtual)\n",
      "I0228 01:30:05.504117 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (742492 virtual)\n",
      "I0228 01:30:05.540049 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (747339 virtual)\n",
      "I0228 01:30:05.544979 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (752481 virtual)\n",
      "I0228 01:30:05.550679 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (757532 virtual)\n",
      "I0228 01:30:05.553748 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (763262 virtual)\n",
      "I0228 01:30:05.560938 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (768150 virtual)\n",
      "I0228 01:30:05.601727 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (774090 virtual)\n",
      "I0228 01:30:05.606301 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (779912 virtual)\n",
      "I0228 01:30:05.611369 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (785627 virtual)\n",
      "I0228 01:30:05.614363 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (791566 virtual)\n",
      "I0228 01:30:05.632431 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (797575 virtual)\n",
      "I0228 01:30:05.654069 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (803232 virtual)\n",
      "I0228 01:30:05.660171 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (808620 virtual)\n",
      "I0228 01:30:05.666313 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (814228 virtual)\n",
      "I0228 01:30:05.681301 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (819858 virtual)\n",
      "I0228 01:30:05.688477 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (825916 virtual)\n",
      "I0228 01:30:05.728250 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (831451 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:05.732578 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (837013 virtual)\n",
      "I0228 01:30:05.737037 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (842318 virtual)\n",
      "I0228 01:30:05.748929 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (848419 virtual)\n",
      "I0228 01:30:05.755610 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (854272 virtual)\n",
      "I0228 01:30:05.794163 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (860177 virtual)\n",
      "I0228 01:30:05.798457 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (866041 virtual)\n",
      "I0228 01:30:05.803396 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (871592 virtual)\n",
      "I0228 01:30:05.814637 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (877878 virtual)\n",
      "I0228 01:30:05.825557 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (883501 virtual)\n",
      "I0228 01:30:05.860047 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (889640 virtual)\n",
      "I0228 01:30:05.864718 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (895493 virtual)\n",
      "I0228 01:30:05.869143 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (901050 virtual)\n",
      "I0228 01:30:05.883319 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (907019 virtual)\n",
      "I0228 01:30:05.890634 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (914186 virtual)\n",
      "I0228 01:30:05.928880 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (918418 virtual)\n",
      "I0228 01:30:05.932737 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (922372 virtual)\n",
      "I0228 01:30:05.936814 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (924815 virtual)\n",
      "I0228 01:30:05.953358 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (930910 virtual)\n",
      "I0228 01:30:05.957397 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (935495 virtual)\n",
      "I0228 01:30:05.995439 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (939328 virtual)\n",
      "I0228 01:30:05.999542 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (943039 virtual)\n",
      "I0228 01:30:06.004125 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (946820 virtual)\n",
      "I0228 01:30:06.019687 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (950303 virtual)\n",
      "I0228 01:30:06.029831 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (954050 virtual)\n",
      "I0228 01:30:06.036959 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (957734 virtual)\n",
      "I0228 01:30:06.045444 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (961475 virtual)\n",
      "I0228 01:30:06.048097 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (964579 virtual)\n",
      "I0228 01:30:06.083594 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (968770 virtual)\n",
      "I0228 01:30:06.090278 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (972684 virtual)\n",
      "I0228 01:30:06.092942 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (976322 virtual)\n",
      "I0228 01:30:06.095543 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (979694 virtual)\n",
      "I0228 01:30:06.098190 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (983407 virtual)\n",
      "I0228 01:30:06.119086 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (987111 virtual)\n",
      "I0228 01:30:06.128229 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (990513 virtual)\n",
      "I0228 01:30:06.132573 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (994647 virtual)\n",
      "I0228 01:30:06.136542 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (997974 virtual)\n",
      "I0228 01:30:06.140560 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1001742 virtual)\n",
      "I0228 01:30:06.167134 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1006059 virtual)\n",
      "I0228 01:30:06.170926 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1009660 virtual)\n",
      "I0228 01:30:06.178736 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1013233 virtual)\n",
      "I0228 01:30:06.181350 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1016819 virtual)\n",
      "I0228 01:30:06.184221 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1021428 virtual)\n",
      "I0228 01:30:06.208485 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1026582 virtual)\n",
      "I0228 01:30:06.212373 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1030400 virtual)\n",
      "I0228 01:30:06.216984 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1034108 virtual)\n",
      "I0228 01:30:06.223469 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1038037 virtual)\n",
      "I0228 01:30:06.226088 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1041817 virtual)\n",
      "I0228 01:30:06.252892 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1046782 virtual)\n",
      "I0228 01:30:06.256601 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1050371 virtual)\n",
      "I0228 01:30:06.261089 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1054766 virtual)\n",
      "I0228 01:30:06.265519 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1058528 virtual)\n",
      "I0228 01:30:06.279373 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1062524 virtual)\n",
      "I0228 01:30:06.297357 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1066329 virtual)\n",
      "I0228 01:30:06.301952 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1070200 virtual)\n",
      "I0228 01:30:06.306582 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1074154 virtual)\n",
      "I0228 01:30:06.310958 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1078842 virtual)\n",
      "I0228 01:30:06.324003 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1083059 virtual)\n",
      "I0228 01:30:06.339379 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1087171 virtual)\n",
      "I0228 01:30:06.346101 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1092002 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:06.354431 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1096025 virtual)\n",
      "I0228 01:30:06.357236 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1099826 virtual)\n",
      "I0228 01:30:06.369854 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1103227 virtual)\n",
      "I0228 01:30:06.381479 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1107670 virtual)\n",
      "I0228 01:30:06.389878 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1111416 virtual)\n",
      "I0228 01:30:06.398128 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1115816 virtual)\n",
      "I0228 01:30:06.406523 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1120030 virtual)\n",
      "I0228 01:30:06.418602 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1124740 virtual)\n",
      "I0228 01:30:06.429052 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1129268 virtual)\n",
      "I0228 01:30:06.439040 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1132879 virtual)\n",
      "I0228 01:30:06.444633 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1138140 virtual)\n",
      "I0228 01:30:06.452586 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1141798 virtual)\n",
      "I0228 01:30:06.456798 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1147554 virtual)\n",
      "I0228 01:30:06.475288 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1152145 virtual)\n",
      "I0228 01:30:06.479860 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1155772 virtual)\n",
      "I0228 01:30:06.490904 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1159498 virtual)\n",
      "I0228 01:30:06.501482 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1163568 virtual)\n",
      "I0228 01:30:06.510192 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1168053 virtual)\n",
      "I0228 01:30:06.519367 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1171898 virtual)\n",
      "I0228 01:30:06.527778 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1176768 virtual)\n",
      "I0228 01:30:06.543361 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1181005 virtual)\n",
      "I0228 01:30:06.548555 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1185014 virtual)\n",
      "I0228 01:30:06.570863 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1190004 virtual)\n",
      "I0228 01:30:06.575898 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1194714 virtual)\n",
      "I0228 01:30:06.581310 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1198450 virtual)\n",
      "I0228 01:30:06.584294 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1202746 virtual)\n",
      "I0228 01:30:06.596097 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1207405 virtual)\n",
      "I0228 01:30:06.618020 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1211530 virtual)\n",
      "I0228 01:30:06.622171 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1215556 virtual)\n",
      "I0228 01:30:06.628955 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1219955 virtual)\n",
      "I0228 01:30:06.637063 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1223600 virtual)\n",
      "I0228 01:30:06.640756 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1228226 virtual)\n",
      "I0228 01:30:06.666268 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1232487 virtual)\n",
      "I0228 01:30:06.670207 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1236285 virtual)\n",
      "I0228 01:30:06.677612 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1239199 virtual)\n",
      "I0228 01:30:06.682888 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1242562 virtual)\n",
      "I0228 01:30:06.685563 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1246521 virtual)\n",
      "I0228 01:30:06.705273 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1250522 virtual)\n",
      "I0228 01:30:06.709366 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1255291 virtual)\n",
      "I0228 01:30:06.719384 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1260300 virtual)\n",
      "I0228 01:30:06.723443 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1264756 virtual)\n",
      "I0228 01:30:06.733363 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1268585 virtual)\n",
      "I0228 01:30:06.745558 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1272719 virtual)\n",
      "I0228 01:30:06.750275 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1276828 virtual)\n",
      "I0228 01:30:06.754786 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1280568 virtual)\n",
      "I0228 01:30:06.758514 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1284568 virtual)\n",
      "I0228 01:30:06.776051 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1288591 virtual)\n",
      "I0228 01:30:06.782197 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1293156 virtual)\n",
      "I0228 01:30:06.801446 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1296849 virtual)\n",
      "I0228 01:30:06.807066 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1301436 virtual)\n",
      "I0228 01:30:06.811311 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1305653 virtual)\n",
      "I0228 01:30:06.817859 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1309042 virtual)\n",
      "I0228 01:30:06.823867 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1313179 virtual)\n",
      "I0228 01:30:06.843850 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1318137 virtual)\n",
      "I0228 01:30:06.849074 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1322571 virtual)\n",
      "I0228 01:30:06.853183 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1325857 virtual)\n",
      "I0228 01:30:06.857638 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1330507 virtual)\n",
      "I0228 01:30:06.875437 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1334355 virtual)\n",
      "I0228 01:30:06.886893 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1337780 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:06.890689 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1341636 virtual)\n",
      "I0228 01:30:06.896770 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1345864 virtual)\n",
      "I0228 01:30:06.899436 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1349929 virtual)\n",
      "I0228 01:30:06.914962 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1354621 virtual)\n",
      "I0228 01:30:06.940164 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1358886 virtual)\n",
      "I0228 01:30:06.945990 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1363625 virtual)\n",
      "I0228 01:30:06.964234 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1368179 virtual)\n",
      "I0228 01:30:06.966620 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1370237 virtual)\n",
      "I0228 01:30:06.969666 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1375561 virtual)\n",
      "I0228 01:30:06.972589 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1380617 virtual)\n",
      "I0228 01:30:06.977243 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1384786 virtual)\n",
      "I0228 01:30:06.982422 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1389491 virtual)\n",
      "I0228 01:30:06.996286 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1393425 virtual)\n",
      "I0228 01:30:07.003036 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1398683 virtual)\n",
      "I0228 01:30:07.018374 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1402389 virtual)\n",
      "I0228 01:30:07.022507 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1407208 virtual)\n",
      "I0228 01:30:07.026914 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1412035 virtual)\n",
      "I0228 01:30:07.035604 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1416438 virtual)\n",
      "I0228 01:30:07.064747 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1420604 virtual)\n",
      "I0228 01:30:07.068881 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1425342 virtual)\n",
      "I0228 01:30:07.076175 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1430021 virtual)\n",
      "I0228 01:30:07.079434 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1434038 virtual)\n",
      "I0228 01:30:07.092244 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1438896 virtual)\n",
      "I0228 01:30:07.110393 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1443280 virtual)\n",
      "I0228 01:30:07.122179 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1447579 virtual)\n",
      "I0228 01:30:07.126788 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1453029 virtual)\n",
      "I0228 01:30:07.131248 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1457823 virtual)\n",
      "I0228 01:30:07.135406 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1463930 virtual)\n",
      "I0228 01:30:07.159835 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1468510 virtual)\n",
      "I0228 01:30:07.175945 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1474437 virtual)\n",
      "I0228 01:30:07.179720 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1479157 virtual)\n",
      "I0228 01:30:07.183600 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1486848 virtual)\n",
      "I0228 01:30:07.188166 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1493816 virtual)\n",
      "I0228 01:30:07.207657 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1498724 virtual)\n",
      "I0228 01:30:07.221409 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1503362 virtual)\n",
      "I0228 01:30:07.226323 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1508709 virtual)\n",
      "I0228 01:30:07.233075 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1513981 virtual)\n",
      "I0228 01:30:07.256436 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1519194 virtual)\n",
      "I0228 01:30:07.259694 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1523981 virtual)\n",
      "I0228 01:30:07.285278 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1529084 virtual)\n",
      "I0228 01:30:07.290325 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1537444 virtual)\n",
      "I0228 01:30:07.312456 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1544579 virtual)\n",
      "I0228 01:30:07.316558 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1549789 virtual)\n",
      "I0228 01:30:07.331782 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1555616 virtual)\n",
      "I0228 01:30:07.338114 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1560486 virtual)\n",
      "I0228 01:30:07.349961 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1564770 virtual)\n",
      "I0228 01:30:07.373028 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1569307 virtual)\n",
      "I0228 01:30:07.378367 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1574055 virtual)\n",
      "I0228 01:30:07.383819 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1578775 virtual)\n",
      "I0228 01:30:07.392326 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1583548 virtual)\n",
      "I0228 01:30:07.432724 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1589083 virtual)\n",
      "I0228 01:30:07.444389 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1595363 virtual)\n",
      "I0228 01:30:07.448687 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1600937 virtual)\n",
      "I0228 01:30:07.452842 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1606536 virtual)\n",
      "I0228 01:30:07.456939 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1612004 virtual)\n",
      "I0228 01:30:07.480048 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1617992 virtual)\n",
      "I0228 01:30:07.497140 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1624217 virtual)\n",
      "I0228 01:30:07.502567 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1628811 virtual)\n",
      "I0228 01:30:07.508149 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1633772 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:07.511225 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1638925 virtual)\n",
      "I0228 01:30:07.540942 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1644194 virtual)\n",
      "I0228 01:30:07.562645 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1649422 virtual)\n",
      "I0228 01:30:07.567214 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1654806 virtual)\n",
      "I0228 01:30:07.571286 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1659925 virtual)\n",
      "I0228 01:30:07.574753 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1665648 virtual)\n",
      "I0228 01:30:07.605969 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1670544 virtual)\n",
      "I0228 01:30:07.614902 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1676162 virtual)\n",
      "I0228 01:30:07.621704 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1684269 virtual)\n",
      "I0228 01:30:07.628214 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1692703 virtual)\n",
      "I0228 01:30:07.632016 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1700869 virtual)\n",
      "I0228 01:30:07.662929 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1708967 virtual)\n",
      "I0228 01:30:07.675817 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1715723 virtual)\n",
      "I0228 01:30:07.678733 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1720656 virtual)\n",
      "I0228 01:30:07.689497 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1726863 virtual)\n",
      "I0228 01:30:07.693892 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1732507 virtual)\n",
      "I0228 01:30:07.718870 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1738057 virtual)\n",
      "I0228 01:30:07.737437 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1744271 virtual)\n",
      "I0228 01:30:07.775373 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1749867 virtual)\n",
      "I0228 01:30:07.787345 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1755693 virtual)\n",
      "I0228 01:30:07.792435 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1760692 virtual)\n",
      "I0228 01:30:07.813635 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1765856 virtual)\n",
      "I0228 01:30:07.820664 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1771270 virtual)\n",
      "I0228 01:30:07.834001 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1776128 virtual)\n",
      "I0228 01:30:07.855795 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1781039 virtual)\n",
      "I0228 01:30:07.862380 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1786505 virtual)\n",
      "I0228 01:30:07.874036 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1791795 virtual)\n",
      "I0228 01:30:07.891788 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1796772 virtual)\n",
      "I0228 01:30:07.899026 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1801627 virtual)\n",
      "I0228 01:30:07.917335 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1806734 virtual)\n",
      "I0228 01:30:07.925077 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1811797 virtual)\n",
      "I0228 01:30:07.935993 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1816555 virtual)\n",
      "I0228 01:30:07.954427 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1821994 virtual)\n",
      "I0228 01:30:07.959821 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1827190 virtual)\n",
      "I0228 01:30:07.975915 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1832469 virtual)\n",
      "I0228 01:30:07.987473 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1837457 virtual)\n",
      "I0228 01:30:08.000805 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1842314 virtual)\n",
      "I0228 01:30:08.013532 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1847204 virtual)\n",
      "I0228 01:30:08.018523 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1852579 virtual)\n",
      "I0228 01:30:08.040556 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1857559 virtual)\n",
      "I0228 01:30:08.048289 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1863045 virtual)\n",
      "I0228 01:30:08.052935 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1868321 virtual)\n",
      "I0228 01:30:08.076299 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1872927 virtual)\n",
      "I0228 01:30:08.081876 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1877906 virtual)\n",
      "I0228 01:30:08.102808 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1882681 virtual)\n",
      "I0228 01:30:08.109806 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1887818 virtual)\n",
      "I0228 01:30:08.113137 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1892586 virtual)\n",
      "I0228 01:30:08.134416 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1897610 virtual)\n",
      "I0228 01:30:08.139970 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1902845 virtual)\n",
      "I0228 01:30:08.164076 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1908285 virtual)\n",
      "I0228 01:30:08.176100 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1913688 virtual)\n",
      "I0228 01:30:08.178926 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1918437 virtual)\n",
      "I0228 01:30:08.190277 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1923642 virtual)\n",
      "I0228 01:30:08.197888 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1928971 virtual)\n",
      "I0228 01:30:08.218607 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1933903 virtual)\n",
      "I0228 01:30:08.232895 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1938436 virtual)\n",
      "I0228 01:30:08.240114 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1943251 virtual)\n",
      "I0228 01:30:08.251361 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1948389 virtual)\n",
      "I0228 01:30:08.259027 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1953254 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:08.286251 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1958406 virtual)\n",
      "I0228 01:30:08.295210 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1963559 virtual)\n",
      "I0228 01:30:08.301277 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1968280 virtual)\n",
      "I0228 01:30:08.309589 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (1973534 virtual)\n",
      "I0228 01:30:08.320418 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (1978800 virtual)\n",
      "I0228 01:30:08.347133 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (1983747 virtual)\n",
      "I0228 01:30:08.351997 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (1988637 virtual)\n",
      "I0228 01:30:08.359563 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (1993724 virtual)\n",
      "I0228 01:30:08.371837 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (1998995 virtual)\n",
      "I0228 01:30:08.377767 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2004303 virtual)\n",
      "I0228 01:30:08.408034 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2009536 virtual)\n",
      "I0228 01:30:08.413504 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2014712 virtual)\n",
      "I0228 01:30:08.419192 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2020085 virtual)\n",
      "I0228 01:30:08.434803 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2025427 virtual)\n",
      "I0228 01:30:08.439051 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2030569 virtual)\n",
      "I0228 01:30:08.469831 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2035744 virtual)\n",
      "I0228 01:30:08.474113 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2040875 virtual)\n",
      "I0228 01:30:08.478458 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2045683 virtual)\n",
      "I0228 01:30:08.505949 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2050555 virtual)\n",
      "I0228 01:30:08.509079 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2055627 virtual)\n",
      "I0228 01:30:08.533945 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2060247 virtual)\n",
      "I0228 01:30:08.537492 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2064859 virtual)\n",
      "I0228 01:30:08.543880 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2069475 virtual)\n",
      "I0228 01:30:08.564274 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2074430 virtual)\n",
      "I0228 01:30:08.568538 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2079785 virtual)\n",
      "I0228 01:30:08.596194 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2084623 virtual)\n",
      "I0228 01:30:08.600433 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2089856 virtual)\n",
      "I0228 01:30:08.604597 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2095133 virtual)\n",
      "I0228 01:30:08.623052 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2100307 virtual)\n",
      "I0228 01:30:08.629033 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2105641 virtual)\n",
      "I0228 01:30:08.653967 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2111039 virtual)\n",
      "I0228 01:30:08.658992 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2116410 virtual)\n",
      "I0228 01:30:08.663815 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2120897 virtual)\n",
      "I0228 01:30:08.681676 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2125815 virtual)\n",
      "I0228 01:30:08.691414 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2130769 virtual)\n",
      "I0228 01:30:08.711612 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2135680 virtual)\n",
      "I0228 01:30:08.718301 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2140145 virtual)\n",
      "I0228 01:30:08.731096 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2145216 virtual)\n",
      "I0228 01:30:08.741240 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2150562 virtual)\n",
      "I0228 01:30:08.753910 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2155739 virtual)\n",
      "I0228 01:30:08.774254 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2161290 virtual)\n",
      "I0228 01:30:08.782144 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2166385 virtual)\n",
      "I0228 01:30:08.789887 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2171481 virtual)\n",
      "I0228 01:30:08.802257 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2176371 virtual)\n",
      "I0228 01:30:08.810328 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2181309 virtual)\n",
      "I0228 01:30:08.834386 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2186614 virtual)\n",
      "I0228 01:30:08.839114 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2191634 virtual)\n",
      "I0228 01:30:08.853626 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2196579 virtual)\n",
      "I0228 01:30:08.867207 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2201169 virtual)\n",
      "I0228 01:30:08.871641 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2206506 virtual)\n",
      "I0228 01:30:08.897889 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2211419 virtual)\n",
      "I0228 01:30:08.901880 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2216703 virtual)\n",
      "I0228 01:30:08.917648 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2222306 virtual)\n",
      "I0228 01:30:08.924757 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2227103 virtual)\n",
      "I0228 01:30:08.930572 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2232290 virtual)\n",
      "I0228 01:30:08.960462 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2237384 virtual)\n",
      "I0228 01:30:08.964978 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2242121 virtual)\n",
      "I0228 01:30:08.978110 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2247178 virtual)\n",
      "I0228 01:30:08.982708 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2252538 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:08.995310 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2257475 virtual)\n",
      "I0228 01:30:09.020287 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2262741 virtual)\n",
      "I0228 01:30:09.024459 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2267514 virtual)\n",
      "I0228 01:30:09.038104 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2272843 virtual)\n",
      "I0228 01:30:09.045449 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2277657 virtual)\n",
      "I0228 01:30:09.057223 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2282653 virtual)\n",
      "I0228 01:30:09.079804 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2287597 virtual)\n",
      "I0228 01:30:09.085069 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2292651 virtual)\n",
      "I0228 01:30:09.103891 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2297600 virtual)\n",
      "I0228 01:30:09.108225 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2302472 virtual)\n",
      "I0228 01:30:09.114986 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2307325 virtual)\n",
      "I0228 01:30:09.137377 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2312655 virtual)\n",
      "I0228 01:30:09.142444 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2317820 virtual)\n",
      "I0228 01:30:09.162150 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2322840 virtual)\n",
      "I0228 01:30:09.167109 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2328155 virtual)\n",
      "I0228 01:30:09.174607 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2333370 virtual)\n",
      "I0228 01:30:09.197756 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2338484 virtual)\n",
      "I0228 01:30:09.201900 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2343548 virtual)\n",
      "I0228 01:30:09.225504 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2348536 virtual)\n",
      "I0228 01:30:09.231154 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2353911 virtual)\n",
      "I0228 01:30:09.235892 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2359314 virtual)\n",
      "I0228 01:30:09.261037 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2364358 virtual)\n",
      "I0228 01:30:09.265616 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2369383 virtual)\n",
      "I0228 01:30:09.286720 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2374083 virtual)\n",
      "I0228 01:30:09.292610 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2378907 virtual)\n",
      "I0228 01:30:09.297341 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2384427 virtual)\n",
      "I0228 01:30:09.321587 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2389188 virtual)\n",
      "I0228 01:30:09.325731 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2394207 virtual)\n",
      "I0228 01:30:09.347138 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2399875 virtual)\n",
      "I0228 01:30:09.355813 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2404786 virtual)\n",
      "I0228 01:30:09.361098 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2409661 virtual)\n",
      "I0228 01:30:09.382987 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2414750 virtual)\n",
      "I0228 01:30:09.387354 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2419653 virtual)\n",
      "I0228 01:30:09.405417 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2424578 virtual)\n",
      "I0228 01:30:09.413260 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2429478 virtual)\n",
      "I0228 01:30:09.425309 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2434777 virtual)\n",
      "I0228 01:30:09.442694 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2440217 virtual)\n",
      "I0228 01:30:09.447304 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2445370 virtual)\n",
      "I0228 01:30:09.471490 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2450212 virtual)\n",
      "I0228 01:30:09.475995 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2455236 virtual)\n",
      "I0228 01:30:09.483417 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2460555 virtual)\n",
      "I0228 01:30:09.503551 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2465436 virtual)\n",
      "I0228 01:30:09.508228 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2470329 virtual)\n",
      "I0228 01:30:09.528561 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2475343 virtual)\n",
      "I0228 01:30:09.534688 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2480177 virtual)\n",
      "I0228 01:30:09.547374 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2485600 virtual)\n",
      "I0228 01:30:09.566219 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2491229 virtual)\n",
      "I0228 01:30:09.571654 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2496244 virtual)\n",
      "I0228 01:30:09.585441 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2501583 virtual)\n",
      "I0228 01:30:09.595864 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2506130 virtual)\n",
      "I0228 01:30:09.609262 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2510757 virtual)\n",
      "I0228 01:30:09.625209 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2515570 virtual)\n",
      "I0228 01:30:09.631024 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2521102 virtual)\n",
      "I0228 01:30:09.645836 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2526019 virtual)\n",
      "I0228 01:30:09.654294 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2531474 virtual)\n",
      "I0228 01:30:09.675426 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2536259 virtual)\n",
      "I0228 01:30:09.686594 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2541394 virtual)\n",
      "I0228 01:30:09.693129 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2546702 virtual)\n",
      "I0228 01:30:09.707067 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2551639 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:09.712129 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2556324 virtual)\n",
      "I0228 01:30:09.725913 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2561317 virtual)\n",
      "I0228 01:30:09.743002 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2566393 virtual)\n",
      "I0228 01:30:09.761075 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2571530 virtual)\n",
      "I0228 01:30:09.766400 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2576675 virtual)\n",
      "I0228 01:30:09.772233 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2581593 virtual)\n",
      "I0228 01:30:09.782235 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2586883 virtual)\n",
      "I0228 01:30:09.804949 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2592005 virtual)\n",
      "I0228 01:30:09.822574 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2597074 virtual)\n",
      "I0228 01:30:09.828343 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2602277 virtual)\n",
      "I0228 01:30:09.834542 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2607541 virtual)\n",
      "I0228 01:30:09.840902 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2612990 virtual)\n",
      "I0228 01:30:09.863404 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2618385 virtual)\n",
      "I0228 01:30:09.884082 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2623500 virtual)\n",
      "I0228 01:30:09.889146 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2628900 virtual)\n",
      "I0228 01:30:09.893325 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2634082 virtual)\n",
      "I0228 01:30:09.906291 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2638937 virtual)\n",
      "I0228 01:30:09.922863 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2643807 virtual)\n",
      "I0228 01:30:09.947300 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2649057 virtual)\n",
      "I0228 01:30:09.951638 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2653857 virtual)\n",
      "I0228 01:30:09.955722 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2658910 virtual)\n",
      "I0228 01:30:09.972547 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2663926 virtual)\n",
      "I0228 01:30:09.982842 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2668586 virtual)\n",
      "I0228 01:30:10.008632 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2673721 virtual)\n",
      "I0228 01:30:10.013607 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2678864 virtual)\n",
      "I0228 01:30:10.019109 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2684491 virtual)\n",
      "I0228 01:30:10.051438 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2689738 virtual)\n",
      "I0228 01:30:10.059798 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2694861 virtual)\n",
      "I0228 01:30:10.068369 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2700216 virtual)\n",
      "I0228 01:30:10.072747 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2705338 virtual)\n",
      "I0228 01:30:10.082495 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2710819 virtual)\n",
      "I0228 01:30:10.088267 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2715711 virtual)\n",
      "I0228 01:30:10.096235 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2720896 virtual)\n",
      "I0228 01:30:10.130248 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2725763 virtual)\n",
      "I0228 01:30:10.135320 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2731136 virtual)\n",
      "I0228 01:30:10.149044 140370323834688 text_analysis.py:506] 552 batches submitted to accumulate stats from 35328 documents (2736564 virtual)\n",
      "I0228 01:30:10.152135 140370323834688 text_analysis.py:506] 553 batches submitted to accumulate stats from 35392 documents (2742203 virtual)\n",
      "I0228 01:30:10.156723 140370323834688 text_analysis.py:506] 554 batches submitted to accumulate stats from 35456 documents (2747714 virtual)\n",
      "I0228 01:30:10.194793 140370323834688 text_analysis.py:506] 555 batches submitted to accumulate stats from 35520 documents (2752648 virtual)\n",
      "I0228 01:30:10.198989 140370323834688 text_analysis.py:506] 556 batches submitted to accumulate stats from 35584 documents (2757541 virtual)\n",
      "I0228 01:30:10.212921 140370323834688 text_analysis.py:506] 557 batches submitted to accumulate stats from 35648 documents (2762571 virtual)\n",
      "I0228 01:30:10.217207 140370323834688 text_analysis.py:506] 558 batches submitted to accumulate stats from 35712 documents (2767772 virtual)\n",
      "I0228 01:30:10.221389 140370323834688 text_analysis.py:506] 559 batches submitted to accumulate stats from 35776 documents (2772971 virtual)\n",
      "I0228 01:30:10.253183 140370323834688 text_analysis.py:506] 560 batches submitted to accumulate stats from 35840 documents (2778449 virtual)\n",
      "I0228 01:30:10.258557 140370323834688 text_analysis.py:506] 561 batches submitted to accumulate stats from 35904 documents (2783407 virtual)\n",
      "I0228 01:30:10.276713 140370323834688 text_analysis.py:506] 562 batches submitted to accumulate stats from 35968 documents (2788224 virtual)\n",
      "I0228 01:30:10.282998 140370323834688 text_analysis.py:506] 563 batches submitted to accumulate stats from 36032 documents (2788314 virtual)\n",
      "I0228 01:30:10.344937 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:30:10.347596 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:30:10.348975 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:30:10.375227 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:30:10.380545 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:30:10.397770 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:30:10.350070 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:30:10.384338 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:30:10.383074 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:30:10.400219 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:30:10.776427 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:30:10.795031 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2788658 virtual documents\n",
      "I0228 01:30:10.883025 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:30:11.269984 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (4980 virtual)\n",
      "I0228 01:30:11.274933 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10307 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:11.277727 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16414 virtual)\n",
      "I0228 01:30:11.281486 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21215 virtual)\n",
      "I0228 01:30:11.285392 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27004 virtual)\n",
      "I0228 01:30:11.289526 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32207 virtual)\n",
      "I0228 01:30:11.292249 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37655 virtual)\n",
      "I0228 01:30:11.294708 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43431 virtual)\n",
      "I0228 01:30:11.296990 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48712 virtual)\n",
      "I0228 01:30:11.299371 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54323 virtual)\n",
      "I0228 01:30:11.345951 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60103 virtual)\n",
      "I0228 01:30:11.350656 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65442 virtual)\n",
      "I0228 01:30:11.357059 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70624 virtual)\n",
      "I0228 01:30:11.360637 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (75859 virtual)\n",
      "I0228 01:30:11.363517 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81509 virtual)\n",
      "I0228 01:30:11.411720 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87623 virtual)\n",
      "I0228 01:30:11.419121 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93416 virtual)\n",
      "I0228 01:30:11.424200 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (98773 virtual)\n",
      "I0228 01:30:11.427557 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104047 virtual)\n",
      "I0228 01:30:11.433679 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109514 virtual)\n",
      "I0228 01:30:11.477474 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (114907 virtual)\n",
      "I0228 01:30:11.484588 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120170 virtual)\n",
      "I0228 01:30:11.489085 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (125847 virtual)\n",
      "I0228 01:30:11.495413 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (131538 virtual)\n",
      "I0228 01:30:11.505612 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137175 virtual)\n",
      "I0228 01:30:11.548523 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (142576 virtual)\n",
      "I0228 01:30:11.553171 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148030 virtual)\n",
      "I0228 01:30:11.557677 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154156 virtual)\n",
      "I0228 01:30:11.561649 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160081 virtual)\n",
      "I0228 01:30:11.573175 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (165472 virtual)\n",
      "I0228 01:30:11.614356 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (170939 virtual)\n",
      "I0228 01:30:11.617490 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176352 virtual)\n",
      "I0228 01:30:11.625262 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (181651 virtual)\n",
      "I0228 01:30:11.631260 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (186789 virtual)\n",
      "I0228 01:30:11.644815 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (192564 virtual)\n",
      "I0228 01:30:11.681604 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (197886 virtual)\n",
      "I0228 01:30:11.686095 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (203407 virtual)\n",
      "I0228 01:30:11.701937 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (208963 virtual)\n",
      "I0228 01:30:11.706510 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (214435 virtual)\n",
      "I0228 01:30:11.711709 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220054 virtual)\n",
      "I0228 01:30:11.748759 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (225400 virtual)\n",
      "I0228 01:30:11.753521 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (232909 virtual)\n",
      "I0228 01:30:11.767030 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (238438 virtual)\n",
      "I0228 01:30:11.773550 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (244392 virtual)\n",
      "I0228 01:30:11.783857 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (249922 virtual)\n",
      "I0228 01:30:11.814039 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (255158 virtual)\n",
      "I0228 01:30:11.819349 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (260438 virtual)\n",
      "I0228 01:30:11.838426 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (265773 virtual)\n",
      "I0228 01:30:11.843356 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (271299 virtual)\n",
      "I0228 01:30:11.852555 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (276453 virtual)\n",
      "I0228 01:30:11.877935 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (281973 virtual)\n",
      "I0228 01:30:11.900111 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288037 virtual)\n",
      "I0228 01:30:11.912987 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (292873 virtual)\n",
      "I0228 01:30:11.917213 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (298597 virtual)\n",
      "I0228 01:30:11.921493 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304108 virtual)\n",
      "I0228 01:30:11.940502 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (309240 virtual)\n",
      "I0228 01:30:11.966656 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (314799 virtual)\n",
      "I0228 01:30:11.976165 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (320631 virtual)\n",
      "I0228 01:30:11.986308 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (325850 virtual)\n",
      "I0228 01:30:11.990486 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (332130 virtual)\n",
      "I0228 01:30:12.009083 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (337900 virtual)\n",
      "I0228 01:30:12.038467 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (343723 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:12.044023 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (349304 virtual)\n",
      "I0228 01:30:12.054699 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (355163 virtual)\n",
      "I0228 01:30:12.057478 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (361024 virtual)\n",
      "I0228 01:30:12.072274 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (366537 virtual)\n",
      "I0228 01:30:12.105950 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (372157 virtual)\n",
      "I0228 01:30:12.110270 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (378118 virtual)\n",
      "I0228 01:30:12.124477 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (383990 virtual)\n",
      "I0228 01:30:12.135055 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (389065 virtual)\n",
      "I0228 01:30:12.140634 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (393951 virtual)\n",
      "I0228 01:30:12.176884 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398202 virtual)\n",
      "I0228 01:30:12.180843 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (401664 virtual)\n",
      "I0228 01:30:12.192548 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (405568 virtual)\n",
      "I0228 01:30:12.207242 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (410195 virtual)\n",
      "I0228 01:30:12.211714 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (414564 virtual)\n",
      "I0228 01:30:12.244476 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (418954 virtual)\n",
      "I0228 01:30:12.254420 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (423122 virtual)\n",
      "I0228 01:30:12.261768 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (427529 virtual)\n",
      "I0228 01:30:12.266699 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (431881 virtual)\n",
      "I0228 01:30:12.270474 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (436865 virtual)\n",
      "I0228 01:30:12.297791 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (441715 virtual)\n",
      "I0228 01:30:12.300590 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (446269 virtual)\n",
      "I0228 01:30:12.317743 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (450229 virtual)\n",
      "I0228 01:30:12.320855 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (454348 virtual)\n",
      "I0228 01:30:12.330169 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (457795 virtual)\n",
      "I0228 01:30:12.347330 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (461452 virtual)\n",
      "I0228 01:30:12.351320 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (465384 virtual)\n",
      "I0228 01:30:12.369900 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (469488 virtual)\n",
      "I0228 01:30:12.374589 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (474359 virtual)\n",
      "I0228 01:30:12.389711 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (479001 virtual)\n",
      "I0228 01:30:12.402455 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (483540 virtual)\n",
      "I0228 01:30:12.406982 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (488190 virtual)\n",
      "I0228 01:30:12.420745 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (492919 virtual)\n",
      "I0228 01:30:12.424908 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (497624 virtual)\n",
      "I0228 01:30:12.432114 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (501951 virtual)\n",
      "I0228 01:30:12.445364 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (506220 virtual)\n",
      "I0228 01:30:12.452405 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (511590 virtual)\n",
      "I0228 01:30:12.468962 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (516569 virtual)\n",
      "I0228 01:30:12.483013 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (521086 virtual)\n",
      "I0228 01:30:12.491155 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (525301 virtual)\n",
      "I0228 01:30:12.497956 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (530053 virtual)\n",
      "I0228 01:30:12.511416 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (533956 virtual)\n",
      "I0228 01:30:12.523922 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (538376 virtual)\n",
      "I0228 01:30:12.540110 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (542205 virtual)\n",
      "I0228 01:30:12.547045 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (546723 virtual)\n",
      "I0228 01:30:12.552576 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (551110 virtual)\n",
      "I0228 01:30:12.577525 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (555327 virtual)\n",
      "I0228 01:30:12.583150 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (560014 virtual)\n",
      "I0228 01:30:12.598976 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (563839 virtual)\n",
      "I0228 01:30:12.603567 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (568567 virtual)\n",
      "I0228 01:30:12.608483 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (572512 virtual)\n",
      "I0228 01:30:12.624813 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (576636 virtual)\n",
      "I0228 01:30:12.634399 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (580519 virtual)\n",
      "I0228 01:30:12.644934 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (583468 virtual)\n",
      "I0228 01:30:12.655304 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (588107 virtual)\n",
      "I0228 01:30:12.661495 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (593543 virtual)\n",
      "I0228 01:30:12.674916 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (599025 virtual)\n",
      "I0228 01:30:12.690968 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (603868 virtual)\n",
      "I0228 01:30:12.694837 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (609077 virtual)\n",
      "I0228 01:30:12.711789 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (614860 virtual)\n",
      "I0228 01:30:12.715711 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (619866 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:12.720683 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (624419 virtual)\n",
      "I0228 01:30:12.731336 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (629283 virtual)\n",
      "I0228 01:30:12.738733 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (633200 virtual)\n",
      "I0228 01:30:12.768075 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (638223 virtual)\n",
      "I0228 01:30:12.781438 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (643508 virtual)\n",
      "I0228 01:30:12.786194 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (648945 virtual)\n",
      "I0228 01:30:12.790816 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (654536 virtual)\n",
      "I0228 01:30:12.799685 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (659566 virtual)\n",
      "I0228 01:30:12.837077 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (664092 virtual)\n",
      "I0228 01:30:12.842423 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (669308 virtual)\n",
      "I0228 01:30:12.846746 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (674653 virtual)\n",
      "I0228 01:30:12.849590 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (679797 virtual)\n",
      "I0228 01:30:12.852399 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (684773 virtual)\n",
      "I0228 01:30:12.897408 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (689748 virtual)\n",
      "I0228 01:30:12.902975 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (694886 virtual)\n",
      "I0228 01:30:12.908589 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (699866 virtual)\n",
      "I0228 01:30:12.912477 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (705230 virtual)\n",
      "I0228 01:30:12.915976 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (710399 virtual)\n",
      "I0228 01:30:12.945350 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (715531 virtual)\n",
      "I0228 01:30:12.966082 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (720922 virtual)\n",
      "I0228 01:30:12.972090 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (725769 virtual)\n",
      "I0228 01:30:12.977447 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (731410 virtual)\n",
      "I0228 01:30:12.980500 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (736835 virtual)\n",
      "I0228 01:30:13.003367 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (742492 virtual)\n",
      "I0228 01:30:13.027248 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (747339 virtual)\n",
      "I0228 01:30:13.033280 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (752481 virtual)\n",
      "I0228 01:30:13.039492 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (757532 virtual)\n",
      "I0228 01:30:13.042557 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (763262 virtual)\n",
      "I0228 01:30:13.060730 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (768150 virtual)\n",
      "I0228 01:30:13.088433 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (774090 virtual)\n",
      "I0228 01:30:13.093868 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (779912 virtual)\n",
      "I0228 01:30:13.103241 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (785627 virtual)\n",
      "I0228 01:30:13.106252 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (791566 virtual)\n",
      "I0228 01:30:13.131728 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (797575 virtual)\n",
      "I0228 01:30:13.140657 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (803232 virtual)\n",
      "I0228 01:30:13.149762 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (808620 virtual)\n",
      "I0228 01:30:13.159045 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (814228 virtual)\n",
      "I0228 01:30:13.183970 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (819858 virtual)\n",
      "I0228 01:30:13.189109 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (825916 virtual)\n",
      "I0228 01:30:13.216729 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (831451 virtual)\n",
      "I0228 01:30:13.220935 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (837013 virtual)\n",
      "I0228 01:30:13.230206 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (842318 virtual)\n",
      "I0228 01:30:13.251041 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (848419 virtual)\n",
      "I0228 01:30:13.259856 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (854272 virtual)\n",
      "I0228 01:30:13.282557 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (860177 virtual)\n",
      "I0228 01:30:13.287577 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (866041 virtual)\n",
      "I0228 01:30:13.297899 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (871592 virtual)\n",
      "I0228 01:30:13.316444 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (877878 virtual)\n",
      "I0228 01:30:13.334649 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (883501 virtual)\n",
      "I0228 01:30:13.348420 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (889640 virtual)\n",
      "I0228 01:30:13.353512 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (895493 virtual)\n",
      "I0228 01:30:13.361283 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (901050 virtual)\n",
      "I0228 01:30:13.384892 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (907019 virtual)\n",
      "I0228 01:30:13.403203 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (914186 virtual)\n",
      "I0228 01:30:13.418406 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (918418 virtual)\n",
      "I0228 01:30:13.424201 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (922372 virtual)\n",
      "I0228 01:30:13.427900 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (924815 virtual)\n",
      "I0228 01:30:13.454482 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (930910 virtual)\n",
      "I0228 01:30:13.469852 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (935495 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:13.486581 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (939328 virtual)\n",
      "I0228 01:30:13.491883 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (943039 virtual)\n",
      "I0228 01:30:13.495931 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (946820 virtual)\n",
      "I0228 01:30:13.519581 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (950303 virtual)\n",
      "I0228 01:30:13.523806 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (954050 virtual)\n",
      "I0228 01:30:13.534822 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (957734 virtual)\n",
      "I0228 01:30:13.538193 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (961475 virtual)\n",
      "I0228 01:30:13.557492 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (964579 virtual)\n",
      "I0228 01:30:13.573645 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (968770 virtual)\n",
      "I0228 01:30:13.579479 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (972684 virtual)\n",
      "I0228 01:30:13.583703 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (976322 virtual)\n",
      "I0228 01:30:13.595184 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (979694 virtual)\n",
      "I0228 01:30:13.602271 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (983407 virtual)\n",
      "I0228 01:30:13.612181 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (987111 virtual)\n",
      "I0228 01:30:13.623443 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (990513 virtual)\n",
      "I0228 01:30:13.628250 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (994647 virtual)\n",
      "I0228 01:30:13.640345 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (997974 virtual)\n",
      "I0228 01:30:13.643711 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1001742 virtual)\n",
      "I0228 01:30:13.660552 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1006059 virtual)\n",
      "I0228 01:30:13.664492 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1009660 virtual)\n",
      "I0228 01:30:13.669593 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1013233 virtual)\n",
      "I0228 01:30:13.679035 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1016819 virtual)\n",
      "I0228 01:30:13.687124 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1021428 virtual)\n",
      "I0228 01:30:13.702680 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1026582 virtual)\n",
      "I0228 01:30:13.707300 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1030400 virtual)\n",
      "I0228 01:30:13.716304 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1034108 virtual)\n",
      "I0228 01:30:13.719211 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1038037 virtual)\n",
      "I0228 01:30:13.731030 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1041817 virtual)\n",
      "I0228 01:30:13.747122 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1046782 virtual)\n",
      "I0228 01:30:13.752992 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1050371 virtual)\n",
      "I0228 01:30:13.756789 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1054766 virtual)\n",
      "I0228 01:30:13.760019 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1058528 virtual)\n",
      "I0228 01:30:13.784247 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1062524 virtual)\n",
      "I0228 01:30:13.792978 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1066329 virtual)\n",
      "I0228 01:30:13.801229 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1070200 virtual)\n",
      "I0228 01:30:13.805994 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1074154 virtual)\n",
      "I0228 01:30:13.811109 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1078842 virtual)\n",
      "I0228 01:30:13.829491 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1083059 virtual)\n",
      "I0228 01:30:13.842378 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1087171 virtual)\n",
      "I0228 01:30:13.847569 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1092002 virtual)\n",
      "I0228 01:30:13.852952 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1096025 virtual)\n",
      "I0228 01:30:13.856829 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1099826 virtual)\n",
      "I0228 01:30:13.874881 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1103227 virtual)\n",
      "I0228 01:30:13.884581 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1107670 virtual)\n",
      "I0228 01:30:13.889817 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1111416 virtual)\n",
      "I0228 01:30:13.893544 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1115816 virtual)\n",
      "I0228 01:30:13.903010 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1120030 virtual)\n",
      "I0228 01:30:13.923821 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1124740 virtual)\n",
      "I0228 01:30:13.933461 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1129268 virtual)\n",
      "I0228 01:30:13.936430 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1132879 virtual)\n",
      "I0228 01:30:13.940131 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1138140 virtual)\n",
      "I0228 01:30:13.949933 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1141798 virtual)\n",
      "I0228 01:30:13.959715 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1147554 virtual)\n",
      "I0228 01:30:13.975993 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1152145 virtual)\n",
      "I0228 01:30:13.980445 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1155772 virtual)\n",
      "I0228 01:30:13.989855 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1159498 virtual)\n",
      "I0228 01:30:13.999381 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1163568 virtual)\n",
      "I0228 01:30:14.015378 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1168053 virtual)\n",
      "I0228 01:30:14.020146 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1171898 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:14.029319 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1176768 virtual)\n",
      "I0228 01:30:14.041030 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1181005 virtual)\n",
      "I0228 01:30:14.047316 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1185014 virtual)\n",
      "I0228 01:30:14.072478 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1190004 virtual)\n",
      "I0228 01:30:14.076811 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1194714 virtual)\n",
      "I0228 01:30:14.081868 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1198450 virtual)\n",
      "I0228 01:30:14.085902 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1202746 virtual)\n",
      "I0228 01:30:14.096437 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1207405 virtual)\n",
      "I0228 01:30:14.120422 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1211530 virtual)\n",
      "I0228 01:30:14.124302 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1215556 virtual)\n",
      "I0228 01:30:14.134452 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1219955 virtual)\n",
      "I0228 01:30:14.138048 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1223600 virtual)\n",
      "I0228 01:30:14.140947 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1228226 virtual)\n",
      "I0228 01:30:14.176062 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1232487 virtual)\n",
      "I0228 01:30:14.179754 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1236285 virtual)\n",
      "I0228 01:30:14.182664 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1239199 virtual)\n",
      "I0228 01:30:14.185601 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1242562 virtual)\n",
      "I0228 01:30:14.188661 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1246521 virtual)\n",
      "I0228 01:30:14.211544 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1250522 virtual)\n",
      "I0228 01:30:14.217196 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1255291 virtual)\n",
      "I0228 01:30:14.222164 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1260300 virtual)\n",
      "I0228 01:30:14.225244 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1264756 virtual)\n",
      "I0228 01:30:14.235940 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1268585 virtual)\n",
      "I0228 01:30:14.248644 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1272719 virtual)\n",
      "I0228 01:30:14.256564 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1276828 virtual)\n",
      "I0228 01:30:14.260455 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1280568 virtual)\n",
      "I0228 01:30:14.263739 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1284568 virtual)\n",
      "I0228 01:30:14.279122 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1288591 virtual)\n",
      "I0228 01:30:14.285848 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1293156 virtual)\n",
      "I0228 01:30:14.311366 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1296849 virtual)\n",
      "I0228 01:30:14.315318 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1301436 virtual)\n",
      "I0228 01:30:14.318831 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1305653 virtual)\n",
      "I0228 01:30:14.321750 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1309042 virtual)\n",
      "I0228 01:30:14.328567 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1313179 virtual)\n",
      "I0228 01:30:14.354506 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1318137 virtual)\n",
      "I0228 01:30:14.359464 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1322571 virtual)\n",
      "I0228 01:30:14.363252 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1325857 virtual)\n",
      "I0228 01:30:14.366381 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1330507 virtual)\n",
      "I0228 01:30:14.380842 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1334355 virtual)\n",
      "I0228 01:30:14.390194 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1337780 virtual)\n",
      "I0228 01:30:14.394311 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1341636 virtual)\n",
      "I0228 01:30:14.408516 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1345864 virtual)\n",
      "I0228 01:30:14.411721 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1349929 virtual)\n",
      "I0228 01:30:14.420690 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1354621 virtual)\n",
      "I0228 01:30:14.439979 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1358886 virtual)\n",
      "I0228 01:30:14.444745 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1363625 virtual)\n",
      "I0228 01:30:14.448829 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1368179 virtual)\n",
      "I0228 01:30:14.460213 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1370237 virtual)\n",
      "I0228 01:30:14.464790 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1375561 virtual)\n",
      "I0228 01:30:14.477238 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1380617 virtual)\n",
      "I0228 01:30:14.484000 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1384786 virtual)\n",
      "I0228 01:30:14.497324 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1389491 virtual)\n",
      "I0228 01:30:14.507674 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1393425 virtual)\n",
      "I0228 01:30:14.516601 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1398683 virtual)\n",
      "I0228 01:30:14.520662 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1402389 virtual)\n",
      "I0228 01:30:14.532526 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1407208 virtual)\n",
      "I0228 01:30:14.536785 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1412035 virtual)\n",
      "I0228 01:30:14.554228 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1416438 virtual)\n",
      "I0228 01:30:14.577011 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1420604 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:14.581938 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1425342 virtual)\n",
      "I0228 01:30:14.586024 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1430021 virtual)\n",
      "I0228 01:30:14.590013 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1434038 virtual)\n",
      "I0228 01:30:14.598924 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1438896 virtual)\n",
      "I0228 01:30:14.620484 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1443280 virtual)\n",
      "I0228 01:30:14.632944 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1447579 virtual)\n",
      "I0228 01:30:14.637985 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1453029 virtual)\n",
      "I0228 01:30:14.644209 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1457823 virtual)\n",
      "I0228 01:30:14.650447 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1463930 virtual)\n",
      "I0228 01:30:14.669548 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1468510 virtual)\n",
      "I0228 01:30:14.697196 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1474437 virtual)\n",
      "I0228 01:30:14.700285 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1479157 virtual)\n",
      "I0228 01:30:14.709270 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1486848 virtual)\n",
      "I0228 01:30:14.714042 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1493816 virtual)\n",
      "I0228 01:30:14.719339 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1498724 virtual)\n",
      "I0228 01:30:14.735895 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1503362 virtual)\n",
      "I0228 01:30:14.739439 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1508709 virtual)\n",
      "I0228 01:30:14.747125 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1513981 virtual)\n",
      "I0228 01:30:14.769884 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1519194 virtual)\n",
      "I0228 01:30:14.786379 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1523981 virtual)\n",
      "I0228 01:30:14.791993 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1529084 virtual)\n",
      "I0228 01:30:14.799224 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1537444 virtual)\n",
      "I0228 01:30:14.827234 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1544579 virtual)\n",
      "I0228 01:30:14.841811 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1549789 virtual)\n",
      "I0228 01:30:14.846148 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1555616 virtual)\n",
      "I0228 01:30:14.850799 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1560486 virtual)\n",
      "I0228 01:30:14.859590 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1564770 virtual)\n",
      "I0228 01:30:14.887333 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1569307 virtual)\n",
      "I0228 01:30:14.896602 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1574055 virtual)\n",
      "I0228 01:30:14.900995 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1578775 virtual)\n",
      "I0228 01:30:14.905222 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1583548 virtual)\n",
      "I0228 01:30:14.953811 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1589083 virtual)\n",
      "I0228 01:30:14.958881 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1595363 virtual)\n",
      "I0228 01:30:14.963094 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1600937 virtual)\n",
      "I0228 01:30:14.967183 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1606536 virtual)\n",
      "I0228 01:30:14.970896 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1612004 virtual)\n",
      "I0228 01:30:15.002228 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1617992 virtual)\n",
      "I0228 01:30:15.009441 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1624217 virtual)\n",
      "I0228 01:30:15.013636 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1628811 virtual)\n",
      "I0228 01:30:15.018027 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1633772 virtual)\n",
      "I0228 01:30:15.022445 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1638925 virtual)\n",
      "I0228 01:30:15.063929 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1644194 virtual)\n",
      "I0228 01:30:15.076264 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1649422 virtual)\n",
      "I0228 01:30:15.080561 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1654806 virtual)\n",
      "I0228 01:30:15.085233 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1659925 virtual)\n",
      "I0228 01:30:15.090180 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1665648 virtual)\n",
      "I0228 01:30:15.129759 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1670544 virtual)\n",
      "I0228 01:30:15.144361 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1676162 virtual)\n",
      "I0228 01:30:15.148505 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1684269 virtual)\n",
      "I0228 01:30:15.152197 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1692703 virtual)\n",
      "I0228 01:30:15.155632 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1700869 virtual)\n",
      "I0228 01:30:15.187605 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1708967 virtual)\n",
      "I0228 01:30:15.192568 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1715723 virtual)\n",
      "I0228 01:30:15.196979 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1720656 virtual)\n",
      "I0228 01:30:15.202742 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1726863 virtual)\n",
      "I0228 01:30:15.206103 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1732507 virtual)\n",
      "I0228 01:30:15.244353 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1738057 virtual)\n",
      "I0228 01:30:15.260314 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1744271 virtual)\n",
      "I0228 01:30:15.294583 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1749867 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:15.299292 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1755693 virtual)\n",
      "I0228 01:30:15.304128 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1760692 virtual)\n",
      "I0228 01:30:15.340689 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1765856 virtual)\n",
      "I0228 01:30:15.354131 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1771270 virtual)\n",
      "I0228 01:30:15.359180 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1776128 virtual)\n",
      "I0228 01:30:15.365747 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1781039 virtual)\n",
      "I0228 01:30:15.372795 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1786505 virtual)\n",
      "I0228 01:30:15.401821 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1791795 virtual)\n",
      "I0228 01:30:15.423079 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1796772 virtual)\n",
      "I0228 01:30:15.427397 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1801627 virtual)\n",
      "I0228 01:30:15.431673 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1806734 virtual)\n",
      "I0228 01:30:15.434438 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1811797 virtual)\n",
      "I0228 01:30:15.464876 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1816555 virtual)\n",
      "I0228 01:30:15.483205 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1821994 virtual)\n",
      "I0228 01:30:15.488136 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1827190 virtual)\n",
      "I0228 01:30:15.493277 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1832469 virtual)\n",
      "I0228 01:30:15.497353 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1837457 virtual)\n",
      "I0228 01:30:15.530836 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1842314 virtual)\n",
      "I0228 01:30:15.540712 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1847204 virtual)\n",
      "I0228 01:30:15.545030 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1852579 virtual)\n",
      "I0228 01:30:15.558552 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1857559 virtual)\n",
      "I0228 01:30:15.562086 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1863045 virtual)\n",
      "I0228 01:30:15.583533 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1868321 virtual)\n",
      "I0228 01:30:15.604512 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1872927 virtual)\n",
      "I0228 01:30:15.611063 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1877906 virtual)\n",
      "I0228 01:30:15.619226 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1882681 virtual)\n",
      "I0228 01:30:15.623085 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1887818 virtual)\n",
      "I0228 01:30:15.643314 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1892586 virtual)\n",
      "I0228 01:30:15.662522 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1897610 virtual)\n",
      "I0228 01:30:15.668564 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1902845 virtual)\n",
      "I0228 01:30:15.678729 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1908285 virtual)\n",
      "I0228 01:30:15.692692 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1913688 virtual)\n",
      "I0228 01:30:15.707290 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1918437 virtual)\n",
      "I0228 01:30:15.718874 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1923642 virtual)\n",
      "I0228 01:30:15.727017 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1928971 virtual)\n",
      "I0228 01:30:15.732466 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1933903 virtual)\n",
      "I0228 01:30:15.757743 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1938436 virtual)\n",
      "I0228 01:30:15.763031 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1943251 virtual)\n",
      "I0228 01:30:15.780699 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1948389 virtual)\n",
      "I0228 01:30:15.789762 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1953254 virtual)\n",
      "I0228 01:30:15.797377 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1958406 virtual)\n",
      "I0228 01:30:15.818853 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1963559 virtual)\n",
      "I0228 01:30:15.825028 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1968280 virtual)\n",
      "I0228 01:30:15.839482 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (1973534 virtual)\n",
      "I0228 01:30:15.852144 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (1978800 virtual)\n",
      "I0228 01:30:15.856654 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (1983747 virtual)\n",
      "I0228 01:30:15.873839 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (1988637 virtual)\n",
      "I0228 01:30:15.882661 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (1993724 virtual)\n",
      "I0228 01:30:15.901607 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (1998995 virtual)\n",
      "I0228 01:30:15.910073 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2004303 virtual)\n",
      "I0228 01:30:15.915780 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2009536 virtual)\n",
      "I0228 01:30:15.936354 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2014712 virtual)\n",
      "I0228 01:30:15.940431 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2020085 virtual)\n",
      "I0228 01:30:15.965037 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2025427 virtual)\n",
      "I0228 01:30:15.972134 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2030569 virtual)\n",
      "I0228 01:30:15.976636 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2035744 virtual)\n",
      "I0228 01:30:15.998205 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2040875 virtual)\n",
      "I0228 01:30:16.002434 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2045683 virtual)\n",
      "I0228 01:30:16.032402 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2050555 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:16.036769 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2055627 virtual)\n",
      "I0228 01:30:16.041165 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2060247 virtual)\n",
      "I0228 01:30:16.062612 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2064859 virtual)\n",
      "I0228 01:30:16.067007 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2069475 virtual)\n",
      "I0228 01:30:16.097753 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2074430 virtual)\n",
      "I0228 01:30:16.101878 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2079785 virtual)\n",
      "I0228 01:30:16.106052 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2084623 virtual)\n",
      "I0228 01:30:16.125080 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2089856 virtual)\n",
      "I0228 01:30:16.129527 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2095133 virtual)\n",
      "I0228 01:30:16.157436 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2100307 virtual)\n",
      "I0228 01:30:16.162148 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2105641 virtual)\n",
      "I0228 01:30:16.166385 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2111039 virtual)\n",
      "I0228 01:30:16.180422 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2116410 virtual)\n",
      "I0228 01:30:16.183921 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2120897 virtual)\n",
      "I0228 01:30:16.220423 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2125815 virtual)\n",
      "I0228 01:30:16.224697 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2130769 virtual)\n",
      "I0228 01:30:16.255293 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2135680 virtual)\n",
      "I0228 01:30:16.258254 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2140145 virtual)\n",
      "I0228 01:30:16.261231 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2145216 virtual)\n",
      "I0228 01:30:16.281169 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2150562 virtual)\n",
      "I0228 01:30:16.284131 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2155739 virtual)\n",
      "I0228 01:30:16.287158 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2161290 virtual)\n",
      "I0228 01:30:16.302595 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2166385 virtual)\n",
      "I0228 01:30:16.307736 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2171481 virtual)\n",
      "I0228 01:30:16.340893 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2176371 virtual)\n",
      "I0228 01:30:16.345557 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2181309 virtual)\n",
      "I0228 01:30:16.349990 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2186614 virtual)\n",
      "I0228 01:30:16.359079 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2191634 virtual)\n",
      "I0228 01:30:16.369992 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2196579 virtual)\n",
      "I0228 01:30:16.406121 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2201169 virtual)\n",
      "I0228 01:30:16.411021 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2206506 virtual)\n",
      "I0228 01:30:16.415025 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2211419 virtual)\n",
      "I0228 01:30:16.419703 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2216703 virtual)\n",
      "I0228 01:30:16.433388 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2222306 virtual)\n",
      "I0228 01:30:16.462918 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2227103 virtual)\n",
      "I0228 01:30:16.468110 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2232290 virtual)\n",
      "I0228 01:30:16.478925 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2237384 virtual)\n",
      "I0228 01:30:16.482045 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2242121 virtual)\n",
      "I0228 01:30:16.494808 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2247178 virtual)\n",
      "I0228 01:30:16.516927 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2252538 virtual)\n",
      "I0228 01:30:16.538599 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2257475 virtual)\n",
      "I0228 01:30:16.545150 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2262741 virtual)\n",
      "I0228 01:30:16.552057 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2267514 virtual)\n",
      "I0228 01:30:16.564182 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2272843 virtual)\n",
      "I0228 01:30:16.575637 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2277657 virtual)\n",
      "I0228 01:30:16.600662 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2282653 virtual)\n",
      "I0228 01:30:16.604720 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2287597 virtual)\n",
      "I0228 01:30:16.608864 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2292651 virtual)\n",
      "I0228 01:30:16.625973 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2297600 virtual)\n",
      "I0228 01:30:16.639044 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2302472 virtual)\n",
      "I0228 01:30:16.658100 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2307325 virtual)\n",
      "I0228 01:30:16.666172 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2312655 virtual)\n",
      "I0228 01:30:16.670191 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2317820 virtual)\n",
      "I0228 01:30:16.687023 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2322840 virtual)\n",
      "I0228 01:30:16.698248 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2328155 virtual)\n",
      "I0228 01:30:16.716693 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2333370 virtual)\n",
      "I0228 01:30:16.724870 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2338484 virtual)\n",
      "I0228 01:30:16.729788 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2343548 virtual)\n",
      "I0228 01:30:16.749830 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2348536 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:16.760594 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2353911 virtual)\n",
      "I0228 01:30:16.775304 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2359314 virtual)\n",
      "I0228 01:30:16.789945 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2364358 virtual)\n",
      "I0228 01:30:16.795019 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2369383 virtual)\n",
      "I0228 01:30:16.810590 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2374083 virtual)\n",
      "I0228 01:30:16.825170 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2378907 virtual)\n",
      "I0228 01:30:16.836346 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2384427 virtual)\n",
      "I0228 01:30:16.849689 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2389188 virtual)\n",
      "I0228 01:30:16.854647 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2394207 virtual)\n",
      "I0228 01:30:16.870656 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2399875 virtual)\n",
      "I0228 01:30:16.889221 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2404786 virtual)\n",
      "I0228 01:30:16.902664 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2409661 virtual)\n",
      "I0228 01:30:16.910995 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2414750 virtual)\n",
      "I0228 01:30:16.915764 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2419653 virtual)\n",
      "I0228 01:30:16.934493 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2424578 virtual)\n",
      "I0228 01:30:16.947671 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2429478 virtual)\n",
      "I0228 01:30:16.970798 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2434777 virtual)\n",
      "I0228 01:30:16.975546 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2440217 virtual)\n",
      "I0228 01:30:16.980000 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2445370 virtual)\n",
      "I0228 01:30:16.999711 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2450212 virtual)\n",
      "I0228 01:30:17.007044 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2455236 virtual)\n",
      "I0228 01:30:17.034574 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2460555 virtual)\n",
      "I0228 01:30:17.039366 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2465436 virtual)\n",
      "I0228 01:30:17.044514 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2470329 virtual)\n",
      "I0228 01:30:17.057927 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2475343 virtual)\n",
      "I0228 01:30:17.065188 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2480177 virtual)\n",
      "I0228 01:30:17.098038 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2485600 virtual)\n",
      "I0228 01:30:17.102355 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2491229 virtual)\n",
      "I0228 01:30:17.106971 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2496244 virtual)\n",
      "I0228 01:30:17.116206 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2501583 virtual)\n",
      "I0228 01:30:17.125362 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2506130 virtual)\n",
      "I0228 01:30:17.158744 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2510757 virtual)\n",
      "I0228 01:30:17.162120 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2515570 virtual)\n",
      "I0228 01:30:17.165109 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2521102 virtual)\n",
      "I0228 01:30:17.178434 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2526019 virtual)\n",
      "I0228 01:30:17.183447 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2531474 virtual)\n",
      "I0228 01:30:17.222291 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2536259 virtual)\n",
      "I0228 01:30:17.226673 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2541394 virtual)\n",
      "I0228 01:30:17.231029 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2546702 virtual)\n",
      "I0228 01:30:17.235748 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2551639 virtual)\n",
      "I0228 01:30:17.243852 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2556324 virtual)\n",
      "I0228 01:30:17.276323 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2561317 virtual)\n",
      "I0228 01:30:17.282589 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2566393 virtual)\n",
      "I0228 01:30:17.294410 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2571530 virtual)\n",
      "I0228 01:30:17.297360 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2576675 virtual)\n",
      "I0228 01:30:17.315421 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2581593 virtual)\n",
      "I0228 01:30:17.332534 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2586883 virtual)\n",
      "I0228 01:30:17.344105 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2592005 virtual)\n",
      "I0228 01:30:17.357382 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2597074 virtual)\n",
      "I0228 01:30:17.359601 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2602277 virtual)\n",
      "I0228 01:30:17.364469 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2607541 virtual)\n",
      "I0228 01:30:17.391392 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2612990 virtual)\n",
      "I0228 01:30:17.404679 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2618385 virtual)\n",
      "I0228 01:30:17.422599 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2623500 virtual)\n",
      "I0228 01:30:17.428020 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2628900 virtual)\n",
      "I0228 01:30:17.433754 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2634082 virtual)\n",
      "I0228 01:30:17.456416 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2638937 virtual)\n",
      "I0228 01:30:17.465430 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2643807 virtual)\n",
      "I0228 01:30:17.486194 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2649057 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:30:17.490570 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2653857 virtual)\n",
      "I0228 01:30:17.495086 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2658910 virtual)\n",
      "I0228 01:30:17.522387 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2663926 virtual)\n",
      "I0228 01:30:17.527207 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2668586 virtual)\n",
      "I0228 01:30:17.547981 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2673721 virtual)\n",
      "I0228 01:30:17.554450 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2678864 virtual)\n",
      "I0228 01:30:17.558451 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2684491 virtual)\n",
      "I0228 01:30:17.578184 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2689738 virtual)\n",
      "I0228 01:30:17.588452 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2694861 virtual)\n",
      "I0228 01:30:17.608322 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2700216 virtual)\n",
      "I0228 01:30:17.615432 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2705338 virtual)\n",
      "I0228 01:30:17.622489 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2710819 virtual)\n",
      "I0228 01:30:17.637070 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2715711 virtual)\n",
      "I0228 01:30:17.648297 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2720896 virtual)\n",
      "I0228 01:30:17.670022 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2725763 virtual)\n",
      "I0228 01:30:17.677561 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2731136 virtual)\n",
      "I0228 01:30:17.693167 140370323834688 text_analysis.py:506] 552 batches submitted to accumulate stats from 35328 documents (2736564 virtual)\n",
      "I0228 01:30:17.700180 140370323834688 text_analysis.py:506] 553 batches submitted to accumulate stats from 35392 documents (2742203 virtual)\n",
      "I0228 01:30:17.716338 140370323834688 text_analysis.py:506] 554 batches submitted to accumulate stats from 35456 documents (2747714 virtual)\n",
      "I0228 01:30:17.734973 140370323834688 text_analysis.py:506] 555 batches submitted to accumulate stats from 35520 documents (2752648 virtual)\n",
      "I0228 01:30:17.739734 140370323834688 text_analysis.py:506] 556 batches submitted to accumulate stats from 35584 documents (2757541 virtual)\n",
      "I0228 01:30:17.759627 140370323834688 text_analysis.py:506] 557 batches submitted to accumulate stats from 35648 documents (2762571 virtual)\n",
      "I0228 01:30:17.763842 140370323834688 text_analysis.py:506] 558 batches submitted to accumulate stats from 35712 documents (2767772 virtual)\n",
      "I0228 01:30:17.789514 140370323834688 text_analysis.py:506] 559 batches submitted to accumulate stats from 35776 documents (2772971 virtual)\n",
      "I0228 01:30:17.799371 140370323834688 text_analysis.py:506] 560 batches submitted to accumulate stats from 35840 documents (2778449 virtual)\n",
      "I0228 01:30:17.803433 140370323834688 text_analysis.py:506] 561 batches submitted to accumulate stats from 35904 documents (2783407 virtual)\n",
      "I0228 01:30:17.827068 140370323834688 text_analysis.py:506] 562 batches submitted to accumulate stats from 35968 documents (2788224 virtual)\n",
      "I0228 01:30:17.845940 140370323834688 text_analysis.py:506] 563 batches submitted to accumulate stats from 36032 documents (2788314 virtual)\n",
      "I0228 01:30:17.911842 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:30:17.916569 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:30:17.921463 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:30:17.924901 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:30:17.951963 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:30:17.920285 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:30:17.915541 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:30:17.926452 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:30:17.955532 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:30:17.930702 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:30:18.348723 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:30:18.372527 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2788658 virtual documents\n",
      "/home/dhamzeia/Thesis/BiomedicalTopicModelling/contextualized_topic_models/models/ctm.py:511: Warning: This is an experimental feature that we has not been fully tested. Refer to the following issue:https://github.com/MilaNLProc/contextualized-topic-models/issues/38\n",
      "  Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores\n",
      "{'epoch': 49, 'cv': 0.6819299428265957, 'umass': -3.7720260326194, 'uci': -0.0032725985977276164, 'npmi': 0.07511567224891204, 'rbo': 1.0, 'td': 0.992, 'train_loss': 642.4195572661398, 'topics': [['c0117438', 'c0267454', 'salivary', 'c0605290', 'c0020933', 'ref', 'c0536858', 'c0026231', 'c0036323', 'postpartum', 'c1446219', 'cessation', 'dyad', 'c0851891', 'c0521990', 'discordant', 'c1260298', 'c0007789', 'c0949907', 'shareholder', 'c0042313', 'cow', 'appreciable', 'c0444584', 'thirty-three', 'c0032821'], ['c0012634', 'c0011900', 'child', 'c1457887', 'c0546788', 'clinical', 'common', 'c0015967', 'cause', 'c3714514', 'c0019993', 'c1446409', 'c0032285', 'c0809949', 'c0019994', 'c0035236', 'c0010076', 'confirm', 'c0221423', 'sars-cov-2', 'c0035648', 'c1306577', 'c0021400', 'c0003232', 'c0231221', 'c0010200'], ['c0025080', 'c0543467', 'undergo', 'postoperative', 'perform', 'c0005898', 'c0009566', 'c0031150', 'c0229962', 'c0728940', 'procedure', 'c1522577', 'c0038930', 'technique', 'operative', 'complication', 'c0087111', 'c0019080', 'c0582175', 'surgical', 'c0002940', 'c0187996', 'c0850292', 'c0162522', 'perioperative', 'feasible'], ['compare', 'difference', 'c0243095', 'significant', 'significantly', 'receive', 'c0199470', 'associate', 'assess', 'c0021708', 'primary', 'decrease', 'determine', 'versus', 'measure', 'respectively', 'association', 'c0034108', 'concentration', 'c0430022', 'measurement', 'c0008976', 'c0022885', 'c0918012', 'predict', 'incidence'], ['crisis', 'policy', 'economic', 'political', 'public', 'challenge', 'c1561598', 'disaster', 'threat', 'international', 'argue', 'chapter', 'market', 'face', 'national', 'c0018696', 'supply', 'food', 'management', 'governance', 'financial', 'sector', 'emergency', 'draw', 'way', 'bring'], ['c0042210', 'c1514562', 'c1254351', 'c1167622', 'c0042736', 'c0014442', 'c0030956', 'c0033684', 'c0029224', 'c0003320', 'c0003241', 'potential', 'c0003250', 'c1148560', 'novel', 'c0678594', 'interaction', 'c0035668', 'c0003316', 'c0002520', 'c0596901', 'c1510800', 'c0574031', 'c0017337', 'c0032136', 'c0020971'], ['c1171362', 'c0025929', 'c0007634', 'role', 'c0007613', 'c0079189', 'induce', 'mechanism', 'c0017262', 'c0021368', 'c0024432', 'activation', 'c0040300', 'c3539881', 'c0039194', 'c0024109', 'mouse', 'c0301872', 'c0037080', 'induction', 'pathway', 'c0596290', 'c0041904', 'c0021747', 'activate', 'c0023810'], ['propose', 'c3161035', 'c0002045', 'c0025663', 'performance', 'c0150098', 'base', 'prediction', 'accuracy', 'automate', 'machine', 'application', 'solution', 'c0037585', 'paper', 'c0679083', 'c0033213', 'learn', 'c1710191', 'c0037589', 'representation', 'solve', 'algorithm', 'apply', 'feature', 'input'], ['c0679646', 'c1257890', 'conduct', 'c2603343', 'search', 'train', 'c0027361', 'report', 'evidence', 'impact', 'c0242481', 'include', 'c0242356', 'c0282574', 'need', 'c0086388', 'c0038951', 'measure', 'c0018724', 'identify', 'c0184661', 'c1706852', 'recommendation', 'provide', 'c0025353', 'physical'], ['c0042776', 'sample', 'c1705920', 'c0017446', 'c0032098', 'c0003062', 'c0684063', 'c0442726', 'c0086418', 'pathogen', 'c0242781', 'c0012984', 'c0017337', 'c0017428', 'c0005595', 'host', 'different', 'c1764827', 'c0007452', 'c0039005', 'c0439663', 'population', 'isolate', 'c1511790', 'diversity', 'c0014406']]}\n",
      "Epoch: [51/250]\tSamples: [1873026/9181500]\tTrain Loss: 642.3688124982982\tTime: 0:00:04.444776\n",
      "Epoch: [52/250]\tSamples: [1909752/9181500]\tTrain Loss: 642.2237413548985\tTime: 0:00:04.480240\n",
      "Epoch: [53/250]\tSamples: [1946478/9181500]\tTrain Loss: 642.2210987915578\tTime: 0:00:04.552188\n",
      "Epoch: [54/250]\tSamples: [1983204/9181500]\tTrain Loss: 642.2935600162351\tTime: 0:00:04.463008\n",
      "Epoch: [55/250]\tSamples: [2019930/9181500]\tTrain Loss: 642.3516073847546\tTime: 0:00:04.753438\n",
      "Epoch: [56/250]\tSamples: [2056656/9181500]\tTrain Loss: 642.0804601772246\tTime: 0:00:04.753747\n",
      "Epoch: [57/250]\tSamples: [2093382/9181500]\tTrain Loss: 642.1890574584422\tTime: 0:00:04.731914\n",
      "Epoch: [58/250]\tSamples: [2130108/9181500]\tTrain Loss: 642.1110070181887\tTime: 0:00:04.688234\n",
      "Epoch: [59/250]\tSamples: [2166834/9181500]\tTrain Loss: 642.1647739552701\tTime: 0:00:04.759594\n",
      "Epoch: [60/250]\tSamples: [2203560/9181500]\tTrain Loss: 642.0140502045554\tTime: 0:00:04.749493\n",
      "Epoch: [61/250]\tSamples: [2240286/9181500]\tTrain Loss: 642.2643915201425\tTime: 0:00:04.730875\n",
      "Epoch: [62/250]\tSamples: [2277012/9181500]\tTrain Loss: 642.127737225501\tTime: 0:00:04.680431\n",
      "Epoch: [63/250]\tSamples: [2313738/9181500]\tTrain Loss: 642.274644814811\tTime: 0:00:04.729334\n",
      "Epoch: [64/250]\tSamples: [2350464/9181500]\tTrain Loss: 642.1477259383679\tTime: 0:00:04.724746\n",
      "Epoch: [65/250]\tSamples: [2387190/9181500]\tTrain Loss: 642.3566991453603\tTime: 0:00:04.753341\n",
      "Epoch: [66/250]\tSamples: [2423916/9181500]\tTrain Loss: 642.0801589601032\tTime: 0:00:04.719634\n",
      "Epoch: [67/250]\tSamples: [2460642/9181500]\tTrain Loss: 642.2303841922276\tTime: 0:00:04.759901\n",
      "Epoch: [68/250]\tSamples: [2497368/9181500]\tTrain Loss: 642.0817686954575\tTime: 0:00:04.772304\n",
      "Epoch: [69/250]\tSamples: [2534094/9181500]\tTrain Loss: 642.1404741787154\tTime: 0:00:04.776284\n",
      "Epoch: [70/250]\tSamples: [2570820/9181500]\tTrain Loss: 642.3416505209184\tTime: 0:00:04.783238\n",
      "Epoch: [71/250]\tSamples: [2607546/9181500]\tTrain Loss: 642.0486417256337\tTime: 0:00:04.804135\n",
      "Epoch: [72/250]\tSamples: [2644272/9181500]\tTrain Loss: 642.141077357492\tTime: 0:00:04.797884\n",
      "Epoch: [73/250]\tSamples: [2680998/9181500]\tTrain Loss: 642.2197402300142\tTime: 0:00:04.731382\n",
      "Epoch: [74/250]\tSamples: [2717724/9181500]\tTrain Loss: 642.0649416455644\tTime: 0:00:04.750891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:18.198294 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [75/250]\tSamples: [2754450/9181500]\tTrain Loss: 642.1155406974963\tTime: 0:00:04.730334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:19.028046 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:32:19.717820 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:32:20.537864 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:32:21.077032 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:32:21.083471 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:32:21.830096 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:32:22.496684 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:32:23.295732 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:32:23.815724 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:32:23.820609 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:32:24.569036 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:32:25.240839 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:32:26.035771 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:32:26.556934 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:32:26.563268 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:32:27.311503 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:32:27.978979 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:32:28.770633 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:32:29.288684 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:32:29.310527 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:32:29.855280 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (-35785 virtual)\n",
      "I0228 01:32:30.364952 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (-212789 virtual)\n",
      "I0228 01:32:30.752915 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (-509147 virtual)\n",
      "I0228 01:32:30.757465 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (-508702 virtual)\n",
      "I0228 01:32:30.795386 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (-514677 virtual)\n",
      "I0228 01:32:30.914847 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (-541509 virtual)\n",
      "I0228 01:32:30.921024 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (-539219 virtual)\n",
      "I0228 01:32:30.924982 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (-537402 virtual)\n",
      "I0228 01:32:30.928923 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (-535869 virtual)\n",
      "I0228 01:32:30.938130 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (-537589 virtual)\n",
      "I0228 01:32:31.797753 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:32:31.801380 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:32:31.802485 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:32:31.803639 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:32:31.807981 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:32:31.802348 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:32:31.815028 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:32:31.805797 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:32:31.804945 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:32:31.805871 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:32:32.250699 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:32:32.275902 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 229945 virtual documents\n",
      "I0228 01:32:34.150035 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 1000 documents\n",
      "I0228 01:32:34.166588 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 2000 documents\n",
      "I0228 01:32:34.183304 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 3000 documents\n",
      "I0228 01:32:34.200111 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 4000 documents\n",
      "I0228 01:32:34.215164 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 5000 documents\n",
      "I0228 01:32:34.228914 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 6000 documents\n",
      "I0228 01:32:34.243445 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 7000 documents\n",
      "I0228 01:32:34.257504 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 8000 documents\n",
      "I0228 01:32:34.272864 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 9000 documents\n",
      "I0228 01:32:34.289389 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 10000 documents\n",
      "I0228 01:32:34.306527 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 11000 documents\n",
      "I0228 01:32:34.321243 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 12000 documents\n",
      "I0228 01:32:34.333884 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 13000 documents\n",
      "I0228 01:32:34.347022 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 14000 documents\n",
      "I0228 01:32:34.360038 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 15000 documents\n",
      "I0228 01:32:34.373326 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 16000 documents\n",
      "I0228 01:32:34.385382 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 17000 documents\n",
      "I0228 01:32:34.398096 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 18000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:34.411105 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 19000 documents\n",
      "I0228 01:32:34.424779 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 20000 documents\n",
      "I0228 01:32:34.440173 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 21000 documents\n",
      "I0228 01:32:34.456107 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 22000 documents\n",
      "I0228 01:32:34.473236 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 23000 documents\n",
      "I0228 01:32:34.490183 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 24000 documents\n",
      "I0228 01:32:34.505811 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 25000 documents\n",
      "I0228 01:32:34.521028 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 26000 documents\n",
      "I0228 01:32:34.536715 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 27000 documents\n",
      "I0228 01:32:34.552244 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 28000 documents\n",
      "I0228 01:32:34.567679 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 29000 documents\n",
      "I0228 01:32:34.583374 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 30000 documents\n",
      "I0228 01:32:34.605715 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 31000 documents\n",
      "I0228 01:32:34.622299 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 32000 documents\n",
      "I0228 01:32:34.638938 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 33000 documents\n",
      "I0228 01:32:34.656162 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 34000 documents\n",
      "I0228 01:32:34.673972 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 35000 documents\n",
      "I0228 01:32:34.691617 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 36000 documents\n",
      "I0228 01:32:34.880075 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:32:35.244849 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (4980 virtual)\n",
      "I0228 01:32:35.249180 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10307 virtual)\n",
      "I0228 01:32:35.252092 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16414 virtual)\n",
      "I0228 01:32:35.254601 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21215 virtual)\n",
      "I0228 01:32:35.261902 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27004 virtual)\n",
      "I0228 01:32:35.264631 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32207 virtual)\n",
      "I0228 01:32:35.267502 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37655 virtual)\n",
      "I0228 01:32:35.270808 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43431 virtual)\n",
      "I0228 01:32:35.273830 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48712 virtual)\n",
      "I0228 01:32:35.279025 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54392 virtual)\n",
      "I0228 01:32:35.314697 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60169 virtual)\n",
      "I0228 01:32:35.319022 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65418 virtual)\n",
      "I0228 01:32:35.323393 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70600 virtual)\n",
      "I0228 01:32:35.333309 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (75752 virtual)\n",
      "I0228 01:32:35.340060 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81442 virtual)\n",
      "I0228 01:32:35.381696 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87556 virtual)\n",
      "I0228 01:32:35.386720 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93400 virtual)\n",
      "I0228 01:32:35.392086 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (98906 virtual)\n",
      "I0228 01:32:35.399491 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104125 virtual)\n",
      "I0228 01:32:35.412984 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109538 virtual)\n",
      "I0228 01:32:35.448507 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (114978 virtual)\n",
      "I0228 01:32:35.453578 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120311 virtual)\n",
      "I0228 01:32:35.458693 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (125916 virtual)\n",
      "I0228 01:32:35.466812 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (131677 virtual)\n",
      "I0228 01:32:35.485440 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137152 virtual)\n",
      "I0228 01:32:35.515802 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (142615 virtual)\n",
      "I0228 01:32:35.520337 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148047 virtual)\n",
      "I0228 01:32:35.526786 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154157 virtual)\n",
      "I0228 01:32:35.532505 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160129 virtual)\n",
      "I0228 01:32:35.550412 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (165433 virtual)\n",
      "I0228 01:32:35.583700 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (170931 virtual)\n",
      "I0228 01:32:35.587977 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176305 virtual)\n",
      "I0228 01:32:35.593086 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (181599 virtual)\n",
      "I0228 01:32:35.601221 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (186732 virtual)\n",
      "I0228 01:32:35.616747 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (192658 virtual)\n",
      "I0228 01:32:35.650145 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (197877 virtual)\n",
      "I0228 01:32:35.654608 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (203397 virtual)\n",
      "I0228 01:32:35.665542 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (209020 virtual)\n",
      "I0228 01:32:35.674587 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (214539 virtual)\n",
      "I0228 01:32:35.686558 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220074 virtual)\n",
      "I0228 01:32:35.716373 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (225459 virtual)\n",
      "I0228 01:32:35.721405 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (233015 virtual)\n",
      "I0228 01:32:35.728587 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (238442 virtual)\n",
      "I0228 01:32:35.737545 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (244340 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:35.757421 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (249921 virtual)\n",
      "I0228 01:32:35.780827 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (255218 virtual)\n",
      "I0228 01:32:35.785983 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (260411 virtual)\n",
      "I0228 01:32:35.798281 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (265819 virtual)\n",
      "I0228 01:32:35.805897 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (271275 virtual)\n",
      "I0228 01:32:35.821547 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (276497 virtual)\n",
      "I0228 01:32:35.845777 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (282001 virtual)\n",
      "I0228 01:32:35.865017 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288096 virtual)\n",
      "I0228 01:32:35.869424 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (292937 virtual)\n",
      "I0228 01:32:35.875477 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (298539 virtual)\n",
      "I0228 01:32:35.889399 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304080 virtual)\n",
      "I0228 01:32:35.908324 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (309169 virtual)\n",
      "I0228 01:32:35.929696 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (314757 virtual)\n",
      "I0228 01:32:35.934830 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (320573 virtual)\n",
      "I0228 01:32:35.942831 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (325789 virtual)\n",
      "I0228 01:32:35.953379 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (332108 virtual)\n",
      "I0228 01:32:35.976950 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (337791 virtual)\n",
      "I0228 01:32:35.991842 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (343559 virtual)\n",
      "I0228 01:32:36.002430 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (349131 virtual)\n",
      "I0228 01:32:36.009164 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (354991 virtual)\n",
      "I0228 01:32:36.019057 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (360893 virtual)\n",
      "I0228 01:32:36.038735 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (366440 virtual)\n",
      "I0228 01:32:36.060138 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (372033 virtual)\n",
      "I0228 01:32:36.070895 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (377969 virtual)\n",
      "I0228 01:32:36.076148 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (383863 virtual)\n",
      "I0228 01:32:36.095592 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (388936 virtual)\n",
      "I0228 01:32:36.111016 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (393764 virtual)\n",
      "I0228 01:32:36.129105 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398046 virtual)\n",
      "I0228 01:32:36.139412 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (401523 virtual)\n",
      "I0228 01:32:36.145916 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (405441 virtual)\n",
      "I0228 01:32:36.166239 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (410070 virtual)\n",
      "I0228 01:32:36.177859 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (414430 virtual)\n",
      "I0228 01:32:36.195907 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (418798 virtual)\n",
      "I0228 01:32:36.213756 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (423041 virtual)\n",
      "I0228 01:32:36.218667 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (427387 virtual)\n",
      "I0228 01:32:36.227301 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (431746 virtual)\n",
      "I0228 01:32:36.230932 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (436698 virtual)\n",
      "I0228 01:32:36.247390 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (441596 virtual)\n",
      "I0228 01:32:36.256952 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (446224 virtual)\n",
      "I0228 01:32:36.267927 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (450015 virtual)\n",
      "I0228 01:32:36.280640 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (454054 virtual)\n",
      "I0228 01:32:36.283804 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (457632 virtual)\n",
      "I0228 01:32:36.296127 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (461221 virtual)\n",
      "I0228 01:32:36.305625 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (465162 virtual)\n",
      "I0228 01:32:36.318878 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (469144 virtual)\n",
      "I0228 01:32:36.328668 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (474183 virtual)\n",
      "I0228 01:32:36.335838 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (478797 virtual)\n",
      "I0228 01:32:36.350830 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (483333 virtual)\n",
      "I0228 01:32:36.361778 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (488023 virtual)\n",
      "I0228 01:32:36.365862 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (492766 virtual)\n",
      "I0228 01:32:36.375297 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (497465 virtual)\n",
      "I0228 01:32:36.379069 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (501739 virtual)\n",
      "I0228 01:32:36.391734 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (505993 virtual)\n",
      "I0228 01:32:36.407761 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (511366 virtual)\n",
      "I0228 01:32:36.412981 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (516275 virtual)\n",
      "I0228 01:32:36.432173 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (520796 virtual)\n",
      "I0228 01:32:36.436096 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (525065 virtual)\n",
      "I0228 01:32:36.443798 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (529750 virtual)\n",
      "I0228 01:32:36.465717 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (533663 virtual)\n",
      "I0228 01:32:36.470155 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (538187 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:36.487833 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (542005 virtual)\n",
      "I0228 01:32:36.491816 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (546437 virtual)\n",
      "I0228 01:32:36.495815 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (550919 virtual)\n",
      "I0228 01:32:36.524885 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (555121 virtual)\n",
      "I0228 01:32:36.530659 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (559797 virtual)\n",
      "I0228 01:32:36.540097 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (563687 virtual)\n",
      "I0228 01:32:36.543001 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (568402 virtual)\n",
      "I0228 01:32:36.549136 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (572387 virtual)\n",
      "I0228 01:32:36.570526 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (576502 virtual)\n",
      "I0228 01:32:36.583602 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (580400 virtual)\n",
      "I0228 01:32:36.587494 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (583333 virtual)\n",
      "I0228 01:32:36.595438 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (587797 virtual)\n",
      "I0228 01:32:36.599373 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (593085 virtual)\n",
      "I0228 01:32:36.618126 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (598726 virtual)\n",
      "I0228 01:32:36.628871 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (603668 virtual)\n",
      "I0228 01:32:36.638892 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (608692 virtual)\n",
      "I0228 01:32:36.647503 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (614614 virtual)\n",
      "I0228 01:32:36.651122 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (619491 virtual)\n",
      "I0228 01:32:36.664752 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (624129 virtual)\n",
      "I0228 01:32:36.676160 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (628847 virtual)\n",
      "I0228 01:32:36.679252 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (632947 virtual)\n",
      "I0228 01:32:36.706755 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (637698 virtual)\n",
      "I0228 01:32:36.713154 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (642958 virtual)\n",
      "I0228 01:32:36.736104 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (648420 virtual)\n",
      "I0228 01:32:36.738249 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (653994 virtual)\n",
      "I0228 01:32:36.745352 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (659226 virtual)\n",
      "I0228 01:32:36.770313 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (663650 virtual)\n",
      "I0228 01:32:36.775186 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (668859 virtual)\n",
      "I0228 01:32:36.791747 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (674213 virtual)\n",
      "I0228 01:32:36.794815 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (679398 virtual)\n",
      "I0228 01:32:36.798874 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (684399 virtual)\n",
      "I0228 01:32:36.827534 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (689238 virtual)\n",
      "I0228 01:32:36.833491 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (694430 virtual)\n",
      "I0228 01:32:36.850586 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (699440 virtual)\n",
      "I0228 01:32:36.856379 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (704902 virtual)\n",
      "I0228 01:32:36.861645 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (709929 virtual)\n",
      "I0228 01:32:36.872722 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (715064 virtual)\n",
      "I0228 01:32:36.895100 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (720460 virtual)\n",
      "I0228 01:32:36.912126 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (725306 virtual)\n",
      "I0228 01:32:36.919278 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (730950 virtual)\n",
      "I0228 01:32:36.922906 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (736460 virtual)\n",
      "I0228 01:32:36.929357 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (742071 virtual)\n",
      "I0228 01:32:36.954785 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (746881 virtual)\n",
      "I0228 01:32:36.975268 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (751885 virtual)\n",
      "I0228 01:32:36.982575 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (757114 virtual)\n",
      "I0228 01:32:36.988561 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (762454 virtual)\n",
      "I0228 01:32:36.991640 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (767657 virtual)\n",
      "I0228 01:32:37.015460 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (773413 virtual)\n",
      "I0228 01:32:37.029560 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (779202 virtual)\n",
      "I0228 01:32:37.046862 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (785047 virtual)\n",
      "I0228 01:32:37.051694 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (790926 virtual)\n",
      "I0228 01:32:37.057053 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (797015 virtual)\n",
      "I0228 01:32:37.066263 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (802533 virtual)\n",
      "I0228 01:32:37.085369 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (808136 virtual)\n",
      "I0228 01:32:37.108340 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (813651 virtual)\n",
      "I0228 01:32:37.114158 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (819307 virtual)\n",
      "I0228 01:32:37.119806 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (825175 virtual)\n",
      "I0228 01:32:37.135964 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (830915 virtual)\n",
      "I0228 01:32:37.154680 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (836393 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:37.178484 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (841732 virtual)\n",
      "I0228 01:32:37.182796 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (847513 virtual)\n",
      "I0228 01:32:37.186705 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (853533 virtual)\n",
      "I0228 01:32:37.199165 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (859602 virtual)\n",
      "I0228 01:32:37.220360 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (865413 virtual)\n",
      "I0228 01:32:37.242256 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (870999 virtual)\n",
      "I0228 01:32:37.250751 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (876958 virtual)\n",
      "I0228 01:32:37.255894 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (882948 virtual)\n",
      "I0228 01:32:37.266409 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (888953 virtual)\n",
      "I0228 01:32:37.293741 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (895025 virtual)\n",
      "I0228 01:32:37.306929 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (900391 virtual)\n",
      "I0228 01:32:37.316055 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (906071 virtual)\n",
      "I0228 01:32:37.322084 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (913611 virtual)\n",
      "I0228 01:32:37.337228 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (918133 virtual)\n",
      "I0228 01:32:37.361183 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (921988 virtual)\n",
      "I0228 01:32:37.368780 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (924318 virtual)\n",
      "I0228 01:32:37.383423 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (929722 virtual)\n",
      "I0228 01:32:37.391175 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (934869 virtual)\n",
      "I0228 01:32:37.403373 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (938830 virtual)\n",
      "I0228 01:32:37.429380 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (942665 virtual)\n",
      "I0228 01:32:37.433400 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (946332 virtual)\n",
      "I0228 01:32:37.447414 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (949822 virtual)\n",
      "I0228 01:32:37.455667 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (953642 virtual)\n",
      "I0228 01:32:37.461308 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (957363 virtual)\n",
      "I0228 01:32:37.474760 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (960935 virtual)\n",
      "I0228 01:32:37.478778 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (964170 virtual)\n",
      "I0228 01:32:37.506737 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (968506 virtual)\n",
      "I0228 01:32:37.511623 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (972465 virtual)\n",
      "I0228 01:32:37.515390 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (976135 virtual)\n",
      "I0228 01:32:37.519535 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (979505 virtual)\n",
      "I0228 01:32:37.522204 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (983324 virtual)\n",
      "I0228 01:32:37.544316 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (987050 virtual)\n",
      "I0228 01:32:37.555384 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (990553 virtual)\n",
      "I0228 01:32:37.559710 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (994574 virtual)\n",
      "I0228 01:32:37.564316 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (997954 virtual)\n",
      "I0228 01:32:37.569098 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1001694 virtual)\n",
      "I0228 01:32:37.594592 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1005989 virtual)\n",
      "I0228 01:32:37.598671 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1009587 virtual)\n",
      "I0228 01:32:37.602870 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1013206 virtual)\n",
      "I0228 01:32:37.606580 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1016727 virtual)\n",
      "I0228 01:32:37.609315 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1021277 virtual)\n",
      "I0228 01:32:37.636126 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1026536 virtual)\n",
      "I0228 01:32:37.640609 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1030210 virtual)\n",
      "I0228 01:32:37.645678 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1033942 virtual)\n",
      "I0228 01:32:37.649905 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1037938 virtual)\n",
      "I0228 01:32:37.652606 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1041774 virtual)\n",
      "I0228 01:32:37.676815 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1046694 virtual)\n",
      "I0228 01:32:37.680371 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1050161 virtual)\n",
      "I0228 01:32:37.683521 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1054653 virtual)\n",
      "I0228 01:32:37.686449 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1058469 virtual)\n",
      "I0228 01:32:37.701064 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1062426 virtual)\n",
      "I0228 01:32:37.719351 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1066317 virtual)\n",
      "I0228 01:32:37.723718 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1070071 virtual)\n",
      "I0228 01:32:37.728119 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1074030 virtual)\n",
      "I0228 01:32:37.731971 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1078804 virtual)\n",
      "I0228 01:32:37.745310 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1082847 virtual)\n",
      "I0228 01:32:37.761782 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1087096 virtual)\n",
      "I0228 01:32:37.768622 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1091882 virtual)\n",
      "I0228 01:32:37.773944 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1096093 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:37.778151 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1099741 virtual)\n",
      "I0228 01:32:37.790576 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1103237 virtual)\n",
      "I0228 01:32:37.804766 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1107623 virtual)\n",
      "I0228 01:32:37.809735 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1111400 virtual)\n",
      "I0228 01:32:37.816642 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1115760 virtual)\n",
      "I0228 01:32:37.830014 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1119926 virtual)\n",
      "I0228 01:32:37.836033 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1124571 virtual)\n",
      "I0228 01:32:37.854264 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1129254 virtual)\n",
      "I0228 01:32:37.857755 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1132624 virtual)\n",
      "I0228 01:32:37.861847 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1137924 virtual)\n",
      "I0228 01:32:37.871822 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1141868 virtual)\n",
      "I0228 01:32:37.874781 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1147479 virtual)\n",
      "I0228 01:32:37.896651 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1152173 virtual)\n",
      "I0228 01:32:37.900979 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1155823 virtual)\n",
      "I0228 01:32:37.909905 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1159488 virtual)\n",
      "I0228 01:32:37.919978 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1163662 virtual)\n",
      "I0228 01:32:37.927682 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1168000 virtual)\n",
      "I0228 01:32:37.936217 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1171926 virtual)\n",
      "I0228 01:32:37.950429 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1176835 virtual)\n",
      "I0228 01:32:37.963366 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1181118 virtual)\n",
      "I0228 01:32:37.969118 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1185041 virtual)\n",
      "I0228 01:32:37.991120 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1190147 virtual)\n",
      "I0228 01:32:37.995388 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1194817 virtual)\n",
      "I0228 01:32:37.998850 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1198457 virtual)\n",
      "I0228 01:32:38.002671 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1202756 virtual)\n",
      "I0228 01:32:38.015790 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1207350 virtual)\n",
      "I0228 01:32:38.037351 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1211582 virtual)\n",
      "I0228 01:32:38.041603 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1215611 virtual)\n",
      "I0228 01:32:38.046677 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1220052 virtual)\n",
      "I0228 01:32:38.050427 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1223694 virtual)\n",
      "I0228 01:32:38.055644 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1228347 virtual)\n",
      "I0228 01:32:38.083842 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1232405 virtual)\n",
      "I0228 01:32:38.088963 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1236223 virtual)\n",
      "I0228 01:32:38.093398 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1239192 virtual)\n",
      "I0228 01:32:38.096899 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1242680 virtual)\n",
      "I0228 01:32:38.102118 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1246618 virtual)\n",
      "I0228 01:32:38.122576 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1250479 virtual)\n",
      "I0228 01:32:38.130344 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1255344 virtual)\n",
      "I0228 01:32:38.145296 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1260353 virtual)\n",
      "I0228 01:32:38.148277 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1264902 virtual)\n",
      "I0228 01:32:38.150876 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1268731 virtual)\n",
      "I0228 01:32:38.161349 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1272792 virtual)\n",
      "I0228 01:32:38.167279 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1276946 virtual)\n",
      "I0228 01:32:38.171029 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1280696 virtual)\n",
      "I0228 01:32:38.178611 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1284654 virtual)\n",
      "I0228 01:32:38.191177 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1288753 virtual)\n",
      "I0228 01:32:38.199868 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1293277 virtual)\n",
      "I0228 01:32:38.222704 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1296936 virtual)\n",
      "I0228 01:32:38.226827 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1301479 virtual)\n",
      "I0228 01:32:38.230916 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1305737 virtual)\n",
      "I0228 01:32:38.244825 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1309131 virtual)\n",
      "I0228 01:32:38.251682 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1313230 virtual)\n",
      "I0228 01:32:38.267196 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1318147 virtual)\n",
      "I0228 01:32:38.271966 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1322700 virtual)\n",
      "I0228 01:32:38.275261 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1326024 virtual)\n",
      "I0228 01:32:38.282698 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1330692 virtual)\n",
      "I0228 01:32:38.288185 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1334410 virtual)\n",
      "I0228 01:32:38.304980 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1337852 virtual)\n",
      "I0228 01:32:38.312336 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1341756 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:38.317343 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1345864 virtual)\n",
      "I0228 01:32:38.321974 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1350014 virtual)\n",
      "I0228 01:32:38.326606 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1354691 virtual)\n",
      "I0228 01:32:38.353484 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1358766 virtual)\n",
      "I0228 01:32:38.357293 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1363405 virtual)\n",
      "I0228 01:32:38.362279 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1368510 virtual)\n",
      "I0228 01:32:38.366064 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1369912 virtual)\n",
      "I0228 01:32:38.371793 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1375226 virtual)\n",
      "I0228 01:32:38.388292 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1380411 virtual)\n",
      "I0228 01:32:38.394924 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1384672 virtual)\n",
      "I0228 01:32:38.404103 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1389497 virtual)\n",
      "I0228 01:32:38.411191 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1393488 virtual)\n",
      "I0228 01:32:38.421809 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1398638 virtual)\n",
      "I0228 01:32:38.428157 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1402578 virtual)\n",
      "I0228 01:32:38.432609 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1407397 virtual)\n",
      "I0228 01:32:38.442575 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1412224 virtual)\n",
      "I0228 01:32:38.464012 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1416671 virtual)\n",
      "I0228 01:32:38.476755 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1420842 virtual)\n",
      "I0228 01:32:38.484683 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1425594 virtual)\n",
      "I0228 01:32:38.488453 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1430224 virtual)\n",
      "I0228 01:32:38.494950 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1434524 virtual)\n",
      "I0228 01:32:38.507022 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1439161 virtual)\n",
      "I0228 01:32:38.531407 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1443448 virtual)\n",
      "I0228 01:32:38.535595 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1448075 virtual)\n",
      "I0228 01:32:38.540049 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1453726 virtual)\n",
      "I0228 01:32:38.554868 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1458374 virtual)\n",
      "I0228 01:32:38.576766 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1464296 virtual)\n",
      "I0228 01:32:38.579617 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1469002 virtual)\n",
      "I0228 01:32:38.585606 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1474792 virtual)\n",
      "I0228 01:32:38.589543 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1480146 virtual)\n",
      "I0228 01:32:38.595429 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1487653 virtual)\n",
      "I0228 01:32:38.627123 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1494498 virtual)\n",
      "I0228 01:32:38.630913 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1499341 virtual)\n",
      "I0228 01:32:38.637183 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1503954 virtual)\n",
      "I0228 01:32:38.642026 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1509439 virtual)\n",
      "I0228 01:32:38.651829 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1514523 virtual)\n",
      "I0228 01:32:38.682040 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1519563 virtual)\n",
      "I0228 01:32:38.693557 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1524780 virtual)\n",
      "I0228 01:32:38.699041 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1529584 virtual)\n",
      "I0228 01:32:38.705508 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1539723 virtual)\n",
      "I0228 01:32:38.731226 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1545299 virtual)\n",
      "I0228 01:32:38.747852 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1550352 virtual)\n",
      "I0228 01:32:38.753785 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1556186 virtual)\n",
      "I0228 01:32:38.757667 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1561268 virtual)\n",
      "I0228 01:32:38.760682 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1565384 virtual)\n",
      "I0228 01:32:38.787812 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1569771 virtual)\n",
      "I0228 01:32:38.804782 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1574726 virtual)\n",
      "I0228 01:32:38.809285 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1579479 virtual)\n",
      "I0228 01:32:38.813573 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1584363 virtual)\n",
      "I0228 01:32:38.851318 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1589810 virtual)\n",
      "I0228 01:32:38.861060 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1595996 virtual)\n",
      "I0228 01:32:38.865837 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1601785 virtual)\n",
      "I0228 01:32:38.869702 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1607377 virtual)\n",
      "I0228 01:32:38.873440 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1612788 virtual)\n",
      "I0228 01:32:38.898213 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1618753 virtual)\n",
      "I0228 01:32:38.910239 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1624784 virtual)\n",
      "I0228 01:32:38.917765 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1629271 virtual)\n",
      "I0228 01:32:38.922167 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1634668 virtual)\n",
      "I0228 01:32:38.925308 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1639698 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:38.957091 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1645077 virtual)\n",
      "I0228 01:32:38.979782 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1650241 virtual)\n",
      "I0228 01:32:38.985536 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1655827 virtual)\n",
      "I0228 01:32:38.991781 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1660852 virtual)\n",
      "I0228 01:32:38.994765 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1666499 virtual)\n",
      "I0228 01:32:39.021370 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1671463 virtual)\n",
      "I0228 01:32:39.031388 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1677663 virtual)\n",
      "I0228 01:32:39.043442 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1685691 virtual)\n",
      "I0228 01:32:39.048989 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1694381 virtual)\n",
      "I0228 01:32:39.054379 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1702598 virtual)\n",
      "I0228 01:32:39.078574 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1710531 virtual)\n",
      "I0228 01:32:39.087622 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1716610 virtual)\n",
      "I0228 01:32:39.102468 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1721383 virtual)\n",
      "I0228 01:32:39.107155 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1728011 virtual)\n",
      "I0228 01:32:39.111726 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1733502 virtual)\n",
      "I0228 01:32:39.133151 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1739299 virtual)\n",
      "I0228 01:32:39.162269 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1745504 virtual)\n",
      "I0228 01:32:39.198509 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1751068 virtual)\n",
      "I0228 01:32:39.203402 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1756308 virtual)\n",
      "I0228 01:32:39.208609 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1761770 virtual)\n",
      "I0228 01:32:39.226109 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1766829 virtual)\n",
      "I0228 01:32:39.238722 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1772209 virtual)\n",
      "I0228 01:32:39.253855 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1777070 virtual)\n",
      "I0228 01:32:39.268829 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1782137 virtual)\n",
      "I0228 01:32:39.276757 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1787472 virtual)\n",
      "I0228 01:32:39.290037 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1793075 virtual)\n",
      "I0228 01:32:39.307821 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1797569 virtual)\n",
      "I0228 01:32:39.320025 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1802588 virtual)\n",
      "I0228 01:32:39.326837 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1807487 virtual)\n",
      "I0228 01:32:39.340369 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1812754 virtual)\n",
      "I0228 01:32:39.348819 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1817734 virtual)\n",
      "I0228 01:32:39.372690 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1822990 virtual)\n",
      "I0228 01:32:39.378373 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1828116 virtual)\n",
      "I0228 01:32:39.386430 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1833352 virtual)\n",
      "I0228 01:32:39.401995 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1838568 virtual)\n",
      "I0228 01:32:39.417420 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1843347 virtual)\n",
      "I0228 01:32:39.425725 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1848313 virtual)\n",
      "I0228 01:32:39.437732 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1853643 virtual)\n",
      "I0228 01:32:39.447382 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1858495 virtual)\n",
      "I0228 01:32:39.463320 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1864290 virtual)\n",
      "I0228 01:32:39.472247 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1869128 virtual)\n",
      "I0228 01:32:39.487100 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1873947 virtual)\n",
      "I0228 01:32:39.498275 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1878661 virtual)\n",
      "I0228 01:32:39.507824 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1883813 virtual)\n",
      "I0228 01:32:39.525259 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1888744 virtual)\n",
      "I0228 01:32:39.530422 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1893718 virtual)\n",
      "I0228 01:32:39.545473 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1898575 virtual)\n",
      "I0228 01:32:39.560774 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1903943 virtual)\n",
      "I0228 01:32:39.565703 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1909402 virtual)\n",
      "I0228 01:32:39.587128 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1914581 virtual)\n",
      "I0228 01:32:39.593186 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1919334 virtual)\n",
      "I0228 01:32:39.604632 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1924612 virtual)\n",
      "I0228 01:32:39.614805 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1929792 virtual)\n",
      "I0228 01:32:39.625546 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1934685 virtual)\n",
      "I0228 01:32:39.647710 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1939318 virtual)\n",
      "I0228 01:32:39.652447 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1944092 virtual)\n",
      "I0228 01:32:39.663501 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1949227 virtual)\n",
      "I0228 01:32:39.682669 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1954330 virtual)\n",
      "I0228 01:32:39.689732 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1959229 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:39.706398 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1964353 virtual)\n",
      "I0228 01:32:39.710973 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1969013 virtual)\n",
      "I0228 01:32:39.723305 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (1974526 virtual)\n",
      "I0228 01:32:39.739066 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (1979570 virtual)\n",
      "I0228 01:32:39.761643 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (1984708 virtual)\n",
      "I0228 01:32:39.764382 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (1989462 virtual)\n",
      "I0228 01:32:39.767964 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (1994624 virtual)\n",
      "I0228 01:32:39.786590 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (1999979 virtual)\n",
      "I0228 01:32:39.799017 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2005016 virtual)\n",
      "I0228 01:32:39.820406 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2010297 virtual)\n",
      "I0228 01:32:39.823270 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2015742 virtual)\n",
      "I0228 01:32:39.830356 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2020971 virtual)\n",
      "I0228 01:32:39.853571 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2026058 virtual)\n",
      "I0228 01:32:39.857840 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2031390 virtual)\n",
      "I0228 01:32:39.881383 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2036603 virtual)\n",
      "I0228 01:32:39.885582 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2041735 virtual)\n",
      "I0228 01:32:39.889378 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2046667 virtual)\n",
      "I0228 01:32:39.916505 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2051467 virtual)\n",
      "I0228 01:32:39.920650 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2056236 virtual)\n",
      "I0228 01:32:39.944101 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2060913 virtual)\n",
      "I0228 01:32:39.949087 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2065603 virtual)\n",
      "I0228 01:32:39.952969 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2070349 virtual)\n",
      "I0228 01:32:39.979312 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2075206 virtual)\n",
      "I0228 01:32:39.983395 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2080656 virtual)\n",
      "I0228 01:32:40.008022 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2085587 virtual)\n",
      "I0228 01:32:40.013843 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2090826 virtual)\n",
      "I0228 01:32:40.019548 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2096058 virtual)\n",
      "I0228 01:32:40.038220 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2101253 virtual)\n",
      "I0228 01:32:40.042406 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2106470 virtual)\n",
      "I0228 01:32:40.064960 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2111899 virtual)\n",
      "I0228 01:32:40.070341 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2117097 virtual)\n",
      "I0228 01:32:40.075369 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2121773 virtual)\n",
      "I0228 01:32:40.097074 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2126911 virtual)\n",
      "I0228 01:32:40.105715 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2131622 virtual)\n",
      "I0228 01:32:40.121532 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2136490 virtual)\n",
      "I0228 01:32:40.126565 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2141045 virtual)\n",
      "I0228 01:32:40.132438 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2146153 virtual)\n",
      "I0228 01:32:40.158602 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2151530 virtual)\n",
      "I0228 01:32:40.168047 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2156992 virtual)\n",
      "I0228 01:32:40.185120 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2162028 virtual)\n",
      "I0228 01:32:40.188050 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2167212 virtual)\n",
      "I0228 01:32:40.192643 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2172420 virtual)\n",
      "I0228 01:32:40.218617 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2177152 virtual)\n",
      "I0228 01:32:40.223792 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2182174 virtual)\n",
      "I0228 01:32:40.243941 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2187390 virtual)\n",
      "I0228 01:32:40.249383 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2192175 virtual)\n",
      "I0228 01:32:40.254721 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2197252 virtual)\n",
      "I0228 01:32:40.282803 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2201978 virtual)\n",
      "I0228 01:32:40.290135 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2207158 virtual)\n",
      "I0228 01:32:40.303432 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2212233 virtual)\n",
      "I0228 01:32:40.308024 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2217450 virtual)\n",
      "I0228 01:32:40.318588 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2223036 virtual)\n",
      "I0228 01:32:40.338409 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2227784 virtual)\n",
      "I0228 01:32:40.350733 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2233205 virtual)\n",
      "I0228 01:32:40.361934 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2238147 virtual)\n",
      "I0228 01:32:40.366844 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2242965 virtual)\n",
      "I0228 01:32:40.381215 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2248138 virtual)\n",
      "I0228 01:32:40.399777 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2253249 virtual)\n",
      "I0228 01:32:40.413006 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2258371 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:40.424822 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2263684 virtual)\n",
      "I0228 01:32:40.429785 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2268495 virtual)\n",
      "I0228 01:32:40.449980 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2273783 virtual)\n",
      "I0228 01:32:40.463791 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2278605 virtual)\n",
      "I0228 01:32:40.478744 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2283494 virtual)\n",
      "I0228 01:32:40.483705 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2288625 virtual)\n",
      "I0228 01:32:40.488689 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2293675 virtual)\n",
      "I0228 01:32:40.509976 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2298739 virtual)\n",
      "I0228 01:32:40.529005 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2303330 virtual)\n",
      "I0228 01:32:40.538463 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2308508 virtual)\n",
      "I0228 01:32:40.545503 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2313681 virtual)\n",
      "I0228 01:32:40.549657 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2318679 virtual)\n",
      "I0228 01:32:40.571335 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2323605 virtual)\n",
      "I0228 01:32:40.587603 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2328946 virtual)\n",
      "I0228 01:32:40.594150 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2334151 virtual)\n",
      "I0228 01:32:40.606158 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2339459 virtual)\n",
      "I0228 01:32:40.610166 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2344359 virtual)\n",
      "I0228 01:32:40.634619 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2349287 virtual)\n",
      "I0228 01:32:40.645990 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2354500 virtual)\n",
      "I0228 01:32:40.656286 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2360121 virtual)\n",
      "I0228 01:32:40.666008 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2365115 virtual)\n",
      "I0228 01:32:40.670279 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2370166 virtual)\n",
      "I0228 01:32:40.692611 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2374738 virtual)\n",
      "I0228 01:32:40.709928 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2379632 virtual)\n",
      "I0228 01:32:40.718290 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2385157 virtual)\n",
      "I0228 01:32:40.724677 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2390022 virtual)\n",
      "I0228 01:32:40.729919 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2394944 virtual)\n",
      "I0228 01:32:40.750526 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2400619 virtual)\n",
      "I0228 01:32:40.770649 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2405541 virtual)\n",
      "I0228 01:32:40.783530 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2410507 virtual)\n",
      "I0228 01:32:40.789062 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2415488 virtual)\n",
      "I0228 01:32:40.796065 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2420448 virtual)\n",
      "I0228 01:32:40.806005 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2425391 virtual)\n",
      "I0228 01:32:40.829845 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2430564 virtual)\n",
      "I0228 01:32:40.848392 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2435753 virtual)\n",
      "I0228 01:32:40.854565 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2441136 virtual)\n",
      "I0228 01:32:40.859435 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2446235 virtual)\n",
      "I0228 01:32:40.874951 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2451118 virtual)\n",
      "I0228 01:32:40.887955 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2456391 virtual)\n",
      "I0228 01:32:40.906835 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2461532 virtual)\n",
      "I0228 01:32:40.911862 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2466198 virtual)\n",
      "I0228 01:32:40.917123 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2471151 virtual)\n",
      "I0228 01:32:40.934803 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2476428 virtual)\n",
      "I0228 01:32:40.949359 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2481464 virtual)\n",
      "I0228 01:32:40.970801 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2486805 virtual)\n",
      "I0228 01:32:40.975632 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2492180 virtual)\n",
      "I0228 01:32:40.981080 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2497036 virtual)\n",
      "I0228 01:32:40.993369 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2502545 virtual)\n",
      "I0228 01:32:41.011863 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2507067 virtual)\n",
      "I0228 01:32:41.028483 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2511830 virtual)\n",
      "I0228 01:32:41.033512 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2516729 virtual)\n",
      "I0228 01:32:41.039226 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2522178 virtual)\n",
      "I0228 01:32:41.059041 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2527397 virtual)\n",
      "I0228 01:32:41.068482 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2532478 virtual)\n",
      "I0228 01:32:41.089299 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2537364 virtual)\n",
      "I0228 01:32:41.093726 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2542559 virtual)\n",
      "I0228 01:32:41.098151 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2547621 virtual)\n",
      "I0228 01:32:41.120682 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2552492 virtual)\n",
      "I0228 01:32:41.125147 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2557287 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:41.145556 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2562244 virtual)\n",
      "I0228 01:32:41.149413 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2567279 virtual)\n",
      "I0228 01:32:41.163898 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2572674 virtual)\n",
      "I0228 01:32:41.182561 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2577596 virtual)\n",
      "I0228 01:32:41.186389 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2582573 virtual)\n",
      "I0228 01:32:41.203142 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2588109 virtual)\n",
      "I0228 01:32:41.212477 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2592969 virtual)\n",
      "I0228 01:32:41.223674 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2598051 virtual)\n",
      "I0228 01:32:41.239179 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2603291 virtual)\n",
      "I0228 01:32:41.244423 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2608626 virtual)\n",
      "I0228 01:32:41.260533 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2614014 virtual)\n",
      "I0228 01:32:41.270904 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2619328 virtual)\n",
      "I0228 01:32:41.289038 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2624365 virtual)\n",
      "I0228 01:32:41.299640 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2629918 virtual)\n",
      "I0228 01:32:41.312731 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2635170 virtual)\n",
      "I0228 01:32:41.328860 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2639980 virtual)\n",
      "I0228 01:32:41.332876 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2644955 virtual)\n",
      "I0228 01:32:41.349417 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2649945 virtual)\n",
      "I0228 01:32:41.365092 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2654977 virtual)\n",
      "I0228 01:32:41.378837 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2659926 virtual)\n",
      "I0228 01:32:41.391517 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2664939 virtual)\n",
      "I0228 01:32:41.396317 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2669728 virtual)\n",
      "I0228 01:32:41.409797 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2674876 virtual)\n",
      "I0228 01:32:41.430953 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2680041 virtual)\n",
      "I0228 01:32:41.443121 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2685469 virtual)\n",
      "I0228 01:32:41.448230 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2690866 virtual)\n",
      "I0228 01:32:41.455383 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2696139 virtual)\n",
      "I0228 01:32:41.468334 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2701440 virtual)\n",
      "I0228 01:32:41.494306 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2706638 virtual)\n",
      "I0228 01:32:41.503765 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2712057 virtual)\n",
      "I0228 01:32:41.508365 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2716941 virtual)\n",
      "I0228 01:32:41.512486 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2722198 virtual)\n",
      "I0228 01:32:41.526978 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2727083 virtual)\n",
      "I0228 01:32:41.556885 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2732523 virtual)\n",
      "I0228 01:32:41.567168 140370323834688 text_analysis.py:506] 552 batches submitted to accumulate stats from 35328 documents (2737809 virtual)\n",
      "I0228 01:32:41.571826 140370323834688 text_analysis.py:506] 553 batches submitted to accumulate stats from 35392 documents (2743536 virtual)\n",
      "I0228 01:32:41.575907 140370323834688 text_analysis.py:506] 554 batches submitted to accumulate stats from 35456 documents (2748922 virtual)\n",
      "I0228 01:32:41.588803 140370323834688 text_analysis.py:506] 555 batches submitted to accumulate stats from 35520 documents (2753984 virtual)\n",
      "I0228 01:32:41.617857 140370323834688 text_analysis.py:506] 556 batches submitted to accumulate stats from 35584 documents (2759009 virtual)\n",
      "I0228 01:32:41.632573 140370323834688 text_analysis.py:506] 557 batches submitted to accumulate stats from 35648 documents (2763935 virtual)\n",
      "I0228 01:32:41.637609 140370323834688 text_analysis.py:506] 558 batches submitted to accumulate stats from 35712 documents (2769003 virtual)\n",
      "I0228 01:32:41.642848 140370323834688 text_analysis.py:506] 559 batches submitted to accumulate stats from 35776 documents (2774542 virtual)\n",
      "I0228 01:32:41.647756 140370323834688 text_analysis.py:506] 560 batches submitted to accumulate stats from 35840 documents (2779722 virtual)\n",
      "I0228 01:32:41.686915 140370323834688 text_analysis.py:506] 561 batches submitted to accumulate stats from 35904 documents (2784854 virtual)\n",
      "I0228 01:32:41.695990 140370323834688 text_analysis.py:506] 562 batches submitted to accumulate stats from 35968 documents (2788239 virtual)\n",
      "I0228 01:32:41.758154 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:32:41.766283 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:32:41.769168 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:32:41.770624 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:32:41.798683 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:32:41.803379 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:32:41.763462 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:32:41.774083 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:32:41.810094 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:32:41.804680 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:32:42.197552 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:32:42.215788 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2788588 virtual documents\n",
      "I0228 01:32:42.306004 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:32:42.726217 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (4980 virtual)\n",
      "I0228 01:32:42.730255 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10307 virtual)\n",
      "I0228 01:32:42.734999 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16414 virtual)\n",
      "I0228 01:32:42.737684 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21215 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:42.744645 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27004 virtual)\n",
      "I0228 01:32:42.748736 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32207 virtual)\n",
      "I0228 01:32:42.751196 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37655 virtual)\n",
      "I0228 01:32:42.753917 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43431 virtual)\n",
      "I0228 01:32:42.759974 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48712 virtual)\n",
      "I0228 01:32:42.761861 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54392 virtual)\n",
      "I0228 01:32:42.793033 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60169 virtual)\n",
      "I0228 01:32:42.799196 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65418 virtual)\n",
      "I0228 01:32:42.802560 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70600 virtual)\n",
      "I0228 01:32:42.819371 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (75752 virtual)\n",
      "I0228 01:32:42.827366 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81442 virtual)\n",
      "I0228 01:32:42.859150 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87556 virtual)\n",
      "I0228 01:32:42.865230 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93400 virtual)\n",
      "I0228 01:32:42.878474 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (98906 virtual)\n",
      "I0228 01:32:42.882730 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104125 virtual)\n",
      "I0228 01:32:42.900530 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109538 virtual)\n",
      "I0228 01:32:42.925345 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (114978 virtual)\n",
      "I0228 01:32:42.934355 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120311 virtual)\n",
      "I0228 01:32:42.942805 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (125916 virtual)\n",
      "I0228 01:32:42.951652 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (131677 virtual)\n",
      "I0228 01:32:42.971407 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137152 virtual)\n",
      "I0228 01:32:42.996769 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (142615 virtual)\n",
      "I0228 01:32:43.004094 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148047 virtual)\n",
      "I0228 01:32:43.008316 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154157 virtual)\n",
      "I0228 01:32:43.015910 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160129 virtual)\n",
      "I0228 01:32:43.035763 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (165433 virtual)\n",
      "I0228 01:32:43.064507 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (170931 virtual)\n",
      "I0228 01:32:43.070861 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176305 virtual)\n",
      "I0228 01:32:43.077162 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (181599 virtual)\n",
      "I0228 01:32:43.084244 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (186732 virtual)\n",
      "I0228 01:32:43.102103 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (192658 virtual)\n",
      "I0228 01:32:43.130871 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (197877 virtual)\n",
      "I0228 01:32:43.135295 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (203397 virtual)\n",
      "I0228 01:32:43.149544 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (209020 virtual)\n",
      "I0228 01:32:43.155004 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (214539 virtual)\n",
      "I0228 01:32:43.175244 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220074 virtual)\n",
      "I0228 01:32:43.196549 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (225459 virtual)\n",
      "I0228 01:32:43.201504 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (233015 virtual)\n",
      "I0228 01:32:43.212068 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (238442 virtual)\n",
      "I0228 01:32:43.219548 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (244340 virtual)\n",
      "I0228 01:32:43.236598 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (249921 virtual)\n",
      "I0228 01:32:43.260769 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (255218 virtual)\n",
      "I0228 01:32:43.269721 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (260411 virtual)\n",
      "I0228 01:32:43.280849 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (265819 virtual)\n",
      "I0228 01:32:43.287359 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (271275 virtual)\n",
      "I0228 01:32:43.301682 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (276497 virtual)\n",
      "I0228 01:32:43.325061 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (282001 virtual)\n",
      "I0228 01:32:43.346576 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288096 virtual)\n",
      "I0228 01:32:43.351059 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (292937 virtual)\n",
      "I0228 01:32:43.356074 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (298539 virtual)\n",
      "I0228 01:32:43.369598 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304080 virtual)\n",
      "I0228 01:32:43.387278 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (309169 virtual)\n",
      "I0228 01:32:43.410873 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (314757 virtual)\n",
      "I0228 01:32:43.417001 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (320573 virtual)\n",
      "I0228 01:32:43.423631 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (325789 virtual)\n",
      "I0228 01:32:43.433655 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (332108 virtual)\n",
      "I0228 01:32:43.455057 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (337791 virtual)\n",
      "I0228 01:32:43.474456 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (343559 virtual)\n",
      "I0228 01:32:43.481967 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (349131 virtual)\n",
      "I0228 01:32:43.490003 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (354991 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:43.499249 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (360893 virtual)\n",
      "I0228 01:32:43.516319 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (366440 virtual)\n",
      "I0228 01:32:43.541652 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (372033 virtual)\n",
      "I0228 01:32:43.549865 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (377969 virtual)\n",
      "I0228 01:32:43.554883 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (383863 virtual)\n",
      "I0228 01:32:43.587372 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (388936 virtual)\n",
      "I0228 01:32:43.589299 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (393764 virtual)\n",
      "I0228 01:32:43.610317 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398046 virtual)\n",
      "I0228 01:32:43.618588 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (401523 virtual)\n",
      "I0228 01:32:43.626872 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (405441 virtual)\n",
      "I0228 01:32:43.647828 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (410070 virtual)\n",
      "I0228 01:32:43.652145 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (414430 virtual)\n",
      "I0228 01:32:43.677904 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (418798 virtual)\n",
      "I0228 01:32:43.698312 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (423041 virtual)\n",
      "I0228 01:32:43.702518 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (427387 virtual)\n",
      "I0228 01:32:43.707063 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (431746 virtual)\n",
      "I0228 01:32:43.710114 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (436698 virtual)\n",
      "I0228 01:32:43.729740 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (441596 virtual)\n",
      "I0228 01:32:43.735737 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (446224 virtual)\n",
      "I0228 01:32:43.749238 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (450015 virtual)\n",
      "I0228 01:32:43.756616 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (454054 virtual)\n",
      "I0228 01:32:43.760291 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (457632 virtual)\n",
      "I0228 01:32:43.778510 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (461221 virtual)\n",
      "I0228 01:32:43.784453 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (465162 virtual)\n",
      "I0228 01:32:43.800258 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (469144 virtual)\n",
      "I0228 01:32:43.807102 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (474183 virtual)\n",
      "I0228 01:32:43.812031 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (478797 virtual)\n",
      "I0228 01:32:43.833265 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (483333 virtual)\n",
      "I0228 01:32:43.840320 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (488023 virtual)\n",
      "I0228 01:32:43.847540 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (492766 virtual)\n",
      "I0228 01:32:43.851863 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (497465 virtual)\n",
      "I0228 01:32:43.855949 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (501739 virtual)\n",
      "I0228 01:32:43.874090 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (505993 virtual)\n",
      "I0228 01:32:43.885946 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (511366 virtual)\n",
      "I0228 01:32:43.894499 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (516275 virtual)\n",
      "I0228 01:32:43.911094 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (520796 virtual)\n",
      "I0228 01:32:43.915590 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (525065 virtual)\n",
      "I0228 01:32:43.925886 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (529750 virtual)\n",
      "I0228 01:32:43.944016 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (533663 virtual)\n",
      "I0228 01:32:43.950621 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (538187 virtual)\n",
      "I0228 01:32:43.965188 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (542005 virtual)\n",
      "I0228 01:32:43.969132 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (546437 virtual)\n",
      "I0228 01:32:43.974805 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (550919 virtual)\n",
      "I0228 01:32:44.006666 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (555121 virtual)\n",
      "I0228 01:32:44.012001 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (559797 virtual)\n",
      "I0228 01:32:44.018520 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (563687 virtual)\n",
      "I0228 01:32:44.021302 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (568402 virtual)\n",
      "I0228 01:32:44.030762 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (572387 virtual)\n",
      "I0228 01:32:44.052511 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (576502 virtual)\n",
      "I0228 01:32:44.061863 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (580400 virtual)\n",
      "I0228 01:32:44.065530 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (583333 virtual)\n",
      "I0228 01:32:44.073873 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (587797 virtual)\n",
      "I0228 01:32:44.080154 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (593085 virtual)\n",
      "I0228 01:32:44.100203 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (598726 virtual)\n",
      "I0228 01:32:44.107068 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (603668 virtual)\n",
      "I0228 01:32:44.116628 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (608692 virtual)\n",
      "I0228 01:32:44.127979 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (614614 virtual)\n",
      "I0228 01:32:44.132350 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (619491 virtual)\n",
      "I0228 01:32:44.146519 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (624129 virtual)\n",
      "I0228 01:32:44.154103 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (628847 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:44.157833 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (632947 virtual)\n",
      "I0228 01:32:44.185964 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (637698 virtual)\n",
      "I0228 01:32:44.192187 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (642958 virtual)\n",
      "I0228 01:32:44.211811 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (648420 virtual)\n",
      "I0228 01:32:44.216515 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (653994 virtual)\n",
      "I0228 01:32:44.221130 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (659226 virtual)\n",
      "I0228 01:32:44.250793 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (663650 virtual)\n",
      "I0228 01:32:44.255712 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (668859 virtual)\n",
      "I0228 01:32:44.266119 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (674213 virtual)\n",
      "I0228 01:32:44.270063 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (679398 virtual)\n",
      "I0228 01:32:44.273335 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (684399 virtual)\n",
      "I0228 01:32:44.307042 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (689238 virtual)\n",
      "I0228 01:32:44.312071 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (694430 virtual)\n",
      "I0228 01:32:44.324801 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (699440 virtual)\n",
      "I0228 01:32:44.329469 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (704902 virtual)\n",
      "I0228 01:32:44.334269 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (709929 virtual)\n",
      "I0228 01:32:44.360712 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (715064 virtual)\n",
      "I0228 01:32:44.375730 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (720460 virtual)\n",
      "I0228 01:32:44.386725 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (725306 virtual)\n",
      "I0228 01:32:44.389832 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (730950 virtual)\n",
      "I0228 01:32:44.392320 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (736460 virtual)\n",
      "I0228 01:32:44.408881 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (742071 virtual)\n",
      "I0228 01:32:44.436371 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (746881 virtual)\n",
      "I0228 01:32:44.450176 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (751885 virtual)\n",
      "I0228 01:32:44.454884 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (757114 virtual)\n",
      "I0228 01:32:44.458908 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (762454 virtual)\n",
      "I0228 01:32:44.466141 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (767657 virtual)\n",
      "I0228 01:32:44.497678 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (773413 virtual)\n",
      "I0228 01:32:44.504324 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (779202 virtual)\n",
      "I0228 01:32:44.513978 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (785047 virtual)\n",
      "I0228 01:32:44.518714 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (790926 virtual)\n",
      "I0228 01:32:44.530656 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (797015 virtual)\n",
      "I0228 01:32:44.548518 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (802533 virtual)\n",
      "I0228 01:32:44.560164 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (808136 virtual)\n",
      "I0228 01:32:44.573575 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (813651 virtual)\n",
      "I0228 01:32:44.579754 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (819307 virtual)\n",
      "I0228 01:32:44.591729 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (825175 virtual)\n",
      "I0228 01:32:44.618781 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (830915 virtual)\n",
      "I0228 01:32:44.629879 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (836393 virtual)\n",
      "I0228 01:32:44.645381 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (841732 virtual)\n",
      "I0228 01:32:44.650614 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (847513 virtual)\n",
      "I0228 01:32:44.660092 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (853533 virtual)\n",
      "I0228 01:32:44.682549 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (859602 virtual)\n",
      "I0228 01:32:44.702898 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (865413 virtual)\n",
      "I0228 01:32:44.710404 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (870999 virtual)\n",
      "I0228 01:32:44.713637 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (876958 virtual)\n",
      "I0228 01:32:44.727415 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (882948 virtual)\n",
      "I0228 01:32:44.750635 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (888953 virtual)\n",
      "I0228 01:32:44.762620 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (895025 virtual)\n",
      "I0228 01:32:44.775425 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (900391 virtual)\n",
      "I0228 01:32:44.781038 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (906071 virtual)\n",
      "I0228 01:32:44.794037 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (913611 virtual)\n",
      "I0228 01:32:44.820540 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (918133 virtual)\n",
      "I0228 01:32:44.830041 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (921988 virtual)\n",
      "I0228 01:32:44.837540 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (924318 virtual)\n",
      "I0228 01:32:44.845089 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (929722 virtual)\n",
      "I0228 01:32:44.860005 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (934869 virtual)\n",
      "I0228 01:32:44.887120 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (938830 virtual)\n",
      "I0228 01:32:44.898080 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (942665 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:44.902750 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (946332 virtual)\n",
      "I0228 01:32:44.908488 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (949822 virtual)\n",
      "I0228 01:32:44.929445 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (953642 virtual)\n",
      "I0228 01:32:44.938580 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (957363 virtual)\n",
      "I0228 01:32:44.943565 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (960935 virtual)\n",
      "I0228 01:32:44.955047 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (964170 virtual)\n",
      "I0228 01:32:44.972979 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (968506 virtual)\n",
      "I0228 01:32:44.984000 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (972465 virtual)\n",
      "I0228 01:32:44.988210 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (976135 virtual)\n",
      "I0228 01:32:44.993009 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (979505 virtual)\n",
      "I0228 01:32:44.996592 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (983324 virtual)\n",
      "I0228 01:32:45.010832 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (987050 virtual)\n",
      "I0228 01:32:45.028232 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (990553 virtual)\n",
      "I0228 01:32:45.031991 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (994574 virtual)\n",
      "I0228 01:32:45.035695 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (997954 virtual)\n",
      "I0228 01:32:45.039455 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1001694 virtual)\n",
      "I0228 01:32:45.061081 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1005989 virtual)\n",
      "I0228 01:32:45.069825 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1009587 virtual)\n",
      "I0228 01:32:45.074077 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1013206 virtual)\n",
      "I0228 01:32:45.077868 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1016727 virtual)\n",
      "I0228 01:32:45.081188 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1021277 virtual)\n",
      "I0228 01:32:45.102601 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1026536 virtual)\n",
      "I0228 01:32:45.110900 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1030210 virtual)\n",
      "I0228 01:32:45.115118 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1033942 virtual)\n",
      "I0228 01:32:45.119532 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1037938 virtual)\n",
      "I0228 01:32:45.123112 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1041774 virtual)\n",
      "I0228 01:32:45.148660 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1046694 virtual)\n",
      "I0228 01:32:45.152847 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1050161 virtual)\n",
      "I0228 01:32:45.156799 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1054653 virtual)\n",
      "I0228 01:32:45.159929 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1058469 virtual)\n",
      "I0228 01:32:45.169833 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1062426 virtual)\n",
      "I0228 01:32:45.192143 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1066317 virtual)\n",
      "I0228 01:32:45.195557 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1070071 virtual)\n",
      "I0228 01:32:45.198778 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1074030 virtual)\n",
      "I0228 01:32:45.201941 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1078804 virtual)\n",
      "I0228 01:32:45.214340 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1082847 virtual)\n",
      "I0228 01:32:45.234447 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1087096 virtual)\n",
      "I0228 01:32:45.239441 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1091882 virtual)\n",
      "I0228 01:32:45.247132 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1096093 virtual)\n",
      "I0228 01:32:45.250983 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1099741 virtual)\n",
      "I0228 01:32:45.258566 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1103237 virtual)\n",
      "I0228 01:32:45.281112 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1107623 virtual)\n",
      "I0228 01:32:45.285231 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1111400 virtual)\n",
      "I0228 01:32:45.290365 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1115760 virtual)\n",
      "I0228 01:32:45.299982 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1119926 virtual)\n",
      "I0228 01:32:45.304613 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1124571 virtual)\n",
      "I0228 01:32:45.326350 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1129254 virtual)\n",
      "I0228 01:32:45.330753 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1132624 virtual)\n",
      "I0228 01:32:45.336087 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1137924 virtual)\n",
      "I0228 01:32:45.340036 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1141868 virtual)\n",
      "I0228 01:32:45.343484 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1147479 virtual)\n",
      "I0228 01:32:45.366794 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1152173 virtual)\n",
      "I0228 01:32:45.372596 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1155823 virtual)\n",
      "I0228 01:32:45.384952 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1159488 virtual)\n",
      "I0228 01:32:45.389939 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1163662 virtual)\n",
      "I0228 01:32:45.397255 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1168000 virtual)\n",
      "I0228 01:32:45.407596 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1171926 virtual)\n",
      "I0228 01:32:45.420184 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1176835 virtual)\n",
      "I0228 01:32:45.431317 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1181118 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:45.442167 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1185041 virtual)\n",
      "I0228 01:32:45.462465 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1190147 virtual)\n",
      "I0228 01:32:45.467440 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1194817 virtual)\n",
      "I0228 01:32:45.471891 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1198457 virtual)\n",
      "I0228 01:32:45.476742 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1202756 virtual)\n",
      "I0228 01:32:45.491272 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1207350 virtual)\n",
      "I0228 01:32:45.506111 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1211582 virtual)\n",
      "I0228 01:32:45.510628 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1215611 virtual)\n",
      "I0228 01:32:45.514658 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1220052 virtual)\n",
      "I0228 01:32:45.519449 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1223694 virtual)\n",
      "I0228 01:32:45.531836 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1228347 virtual)\n",
      "I0228 01:32:45.551476 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1232405 virtual)\n",
      "I0228 01:32:45.559401 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1236223 virtual)\n",
      "I0228 01:32:45.563619 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1239192 virtual)\n",
      "I0228 01:32:45.567380 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1242680 virtual)\n",
      "I0228 01:32:45.578132 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1246618 virtual)\n",
      "I0228 01:32:45.589846 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1250479 virtual)\n",
      "I0228 01:32:45.600286 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1255344 virtual)\n",
      "I0228 01:32:45.604290 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1260353 virtual)\n",
      "I0228 01:32:45.610870 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1264902 virtual)\n",
      "I0228 01:32:45.625829 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1268731 virtual)\n",
      "I0228 01:32:45.630451 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1272792 virtual)\n",
      "I0228 01:32:45.634977 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1276946 virtual)\n",
      "I0228 01:32:45.639225 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1280696 virtual)\n",
      "I0228 01:32:45.646407 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1284654 virtual)\n",
      "I0228 01:32:45.665468 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1288753 virtual)\n",
      "I0228 01:32:45.669760 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1293277 virtual)\n",
      "I0228 01:32:45.689506 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1296936 virtual)\n",
      "I0228 01:32:45.693308 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1301479 virtual)\n",
      "I0228 01:32:45.697482 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1305737 virtual)\n",
      "I0228 01:32:45.707690 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1309131 virtual)\n",
      "I0228 01:32:45.710991 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1313230 virtual)\n",
      "I0228 01:32:45.732686 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1318147 virtual)\n",
      "I0228 01:32:45.737185 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1322700 virtual)\n",
      "I0228 01:32:45.741884 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1326024 virtual)\n",
      "I0228 01:32:45.747386 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1330692 virtual)\n",
      "I0228 01:32:45.760667 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1334410 virtual)\n",
      "I0228 01:32:45.770138 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1337852 virtual)\n",
      "I0228 01:32:45.777611 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1341756 virtual)\n",
      "I0228 01:32:45.781836 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1345864 virtual)\n",
      "I0228 01:32:45.784876 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1350014 virtual)\n",
      "I0228 01:32:45.799759 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1354691 virtual)\n",
      "I0228 01:32:45.815253 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1358766 virtual)\n",
      "I0228 01:32:45.821581 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1363405 virtual)\n",
      "I0228 01:32:45.828999 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1368510 virtual)\n",
      "I0228 01:32:45.834128 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1369912 virtual)\n",
      "I0228 01:32:45.837620 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1375226 virtual)\n",
      "I0228 01:32:45.849903 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1380411 virtual)\n",
      "I0228 01:32:45.860140 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1384672 virtual)\n",
      "I0228 01:32:45.877717 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1389497 virtual)\n",
      "I0228 01:32:45.882570 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1393488 virtual)\n",
      "I0228 01:32:45.905736 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1398638 virtual)\n",
      "I0228 01:32:45.912988 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1402578 virtual)\n",
      "I0228 01:32:45.915814 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1407397 virtual)\n",
      "I0228 01:32:45.921982 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1412224 virtual)\n",
      "I0228 01:32:45.937125 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1416671 virtual)\n",
      "I0228 01:32:45.946810 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1420842 virtual)\n",
      "I0228 01:32:45.950491 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1425594 virtual)\n",
      "I0228 01:32:45.954391 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1430224 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:45.960781 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1434524 virtual)\n",
      "I0228 01:32:45.980121 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1439161 virtual)\n",
      "I0228 01:32:45.990771 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1443448 virtual)\n",
      "I0228 01:32:46.003556 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1448075 virtual)\n",
      "I0228 01:32:46.009209 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1453726 virtual)\n",
      "I0228 01:32:46.018009 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1458374 virtual)\n",
      "I0228 01:32:46.028182 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1464296 virtual)\n",
      "I0228 01:32:46.038675 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1469002 virtual)\n",
      "I0228 01:32:46.054192 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1474792 virtual)\n",
      "I0228 01:32:46.059121 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1480146 virtual)\n",
      "I0228 01:32:46.063633 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1487653 virtual)\n",
      "I0228 01:32:46.079740 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1494498 virtual)\n",
      "I0228 01:32:46.085797 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1499341 virtual)\n",
      "I0228 01:32:46.105705 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1503954 virtual)\n",
      "I0228 01:32:46.110279 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1509439 virtual)\n",
      "I0228 01:32:46.121394 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1514523 virtual)\n",
      "I0228 01:32:46.137923 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1519563 virtual)\n",
      "I0228 01:32:46.143791 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1524780 virtual)\n",
      "I0228 01:32:46.167007 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1529584 virtual)\n",
      "I0228 01:32:46.176786 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1539723 virtual)\n",
      "I0228 01:32:46.197031 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1545299 virtual)\n",
      "I0228 01:32:46.201844 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1550352 virtual)\n",
      "I0228 01:32:46.210526 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1556186 virtual)\n",
      "I0228 01:32:46.216366 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1561268 virtual)\n",
      "I0228 01:32:46.229671 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1565384 virtual)\n",
      "I0228 01:32:46.254099 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1569771 virtual)\n",
      "I0228 01:32:46.259256 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1574726 virtual)\n",
      "I0228 01:32:46.266573 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1579479 virtual)\n",
      "I0228 01:32:46.269647 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1584363 virtual)\n",
      "I0228 01:32:46.315623 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1589810 virtual)\n",
      "I0228 01:32:46.320588 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1595996 virtual)\n",
      "I0228 01:32:46.325551 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1601785 virtual)\n",
      "I0228 01:32:46.328515 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1607377 virtual)\n",
      "I0228 01:32:46.337054 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1612788 virtual)\n",
      "I0228 01:32:46.362463 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1618753 virtual)\n",
      "I0228 01:32:46.366931 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1624784 virtual)\n",
      "I0228 01:32:46.381739 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1629271 virtual)\n",
      "I0228 01:32:46.385253 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1634668 virtual)\n",
      "I0228 01:32:46.388845 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1639698 virtual)\n",
      "I0228 01:32:46.422079 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1645077 virtual)\n",
      "I0228 01:32:46.437049 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1650241 virtual)\n",
      "I0228 01:32:46.444028 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1655827 virtual)\n",
      "I0228 01:32:46.447745 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1660852 virtual)\n",
      "I0228 01:32:46.451246 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1666499 virtual)\n",
      "I0228 01:32:46.486418 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1671463 virtual)\n",
      "I0228 01:32:46.494037 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1677663 virtual)\n",
      "I0228 01:32:46.500131 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1685691 virtual)\n",
      "I0228 01:32:46.506544 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1694381 virtual)\n",
      "I0228 01:32:46.510544 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1702598 virtual)\n",
      "I0228 01:32:46.544178 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1710531 virtual)\n",
      "I0228 01:32:46.549779 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1716610 virtual)\n",
      "I0228 01:32:46.558555 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1721383 virtual)\n",
      "I0228 01:32:46.564873 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1728011 virtual)\n",
      "I0228 01:32:46.569191 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1733502 virtual)\n",
      "I0228 01:32:46.599390 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1739299 virtual)\n",
      "I0228 01:32:46.623209 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1745504 virtual)\n",
      "I0228 01:32:46.655258 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1751068 virtual)\n",
      "I0228 01:32:46.663055 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1756308 virtual)\n",
      "I0228 01:32:46.667576 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1761770 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:46.691409 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1766829 virtual)\n",
      "I0228 01:32:46.698737 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1772209 virtual)\n",
      "I0228 01:32:46.712168 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1777070 virtual)\n",
      "I0228 01:32:46.724003 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1782137 virtual)\n",
      "I0228 01:32:46.739883 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1787472 virtual)\n",
      "I0228 01:32:46.755760 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1793075 virtual)\n",
      "I0228 01:32:46.767074 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1797569 virtual)\n",
      "I0228 01:32:46.776064 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1802588 virtual)\n",
      "I0228 01:32:46.780857 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1807487 virtual)\n",
      "I0228 01:32:46.804887 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1812754 virtual)\n",
      "I0228 01:32:46.815049 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1817734 virtual)\n",
      "I0228 01:32:46.830510 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1822990 virtual)\n",
      "I0228 01:32:46.835640 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1828116 virtual)\n",
      "I0228 01:32:46.840043 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1833352 virtual)\n",
      "I0228 01:32:46.867817 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1838568 virtual)\n",
      "I0228 01:32:46.882835 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1843347 virtual)\n",
      "I0228 01:32:46.887284 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1848313 virtual)\n",
      "I0228 01:32:46.893725 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1853643 virtual)\n",
      "I0228 01:32:46.900077 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1858495 virtual)\n",
      "I0228 01:32:46.929966 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1864290 virtual)\n",
      "I0228 01:32:46.938018 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1869128 virtual)\n",
      "I0228 01:32:46.944481 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1873947 virtual)\n",
      "I0228 01:32:46.953338 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1878661 virtual)\n",
      "I0228 01:32:46.960227 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1883813 virtual)\n",
      "I0228 01:32:46.992646 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1888744 virtual)\n",
      "I0228 01:32:46.997141 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1893718 virtual)\n",
      "I0228 01:32:47.003161 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1898575 virtual)\n",
      "I0228 01:32:47.015915 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1903943 virtual)\n",
      "I0228 01:32:47.019940 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1909402 virtual)\n",
      "I0228 01:32:47.051694 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1914581 virtual)\n",
      "I0228 01:32:47.061645 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1919334 virtual)\n",
      "I0228 01:32:47.066112 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1924612 virtual)\n",
      "I0228 01:32:47.070776 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1929792 virtual)\n",
      "I0228 01:32:47.076818 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1934685 virtual)\n",
      "I0228 01:32:47.111382 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1939318 virtual)\n",
      "I0228 01:32:47.119465 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1944092 virtual)\n",
      "I0228 01:32:47.124232 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1949227 virtual)\n",
      "I0228 01:32:47.131872 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1954330 virtual)\n",
      "I0228 01:32:47.140221 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1959229 virtual)\n",
      "I0228 01:32:47.176950 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1964353 virtual)\n",
      "I0228 01:32:47.182268 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1969013 virtual)\n",
      "I0228 01:32:47.185674 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (1974526 virtual)\n",
      "I0228 01:32:47.199263 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (1979570 virtual)\n",
      "I0228 01:32:47.202787 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (1984708 virtual)\n",
      "I0228 01:32:47.227383 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (1989462 virtual)\n",
      "I0228 01:32:47.232788 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (1994624 virtual)\n",
      "I0228 01:32:47.245353 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (1999979 virtual)\n",
      "I0228 01:32:47.252599 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2005016 virtual)\n",
      "I0228 01:32:47.256736 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2010297 virtual)\n",
      "I0228 01:32:47.287313 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2015742 virtual)\n",
      "I0228 01:32:47.292026 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2020971 virtual)\n",
      "I0228 01:32:47.310285 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2026058 virtual)\n",
      "I0228 01:32:47.314107 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2031390 virtual)\n",
      "I0228 01:32:47.318386 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2036603 virtual)\n",
      "I0228 01:32:47.345213 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2041735 virtual)\n",
      "I0228 01:32:47.350967 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2046667 virtual)\n",
      "I0228 01:32:47.372229 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2051467 virtual)\n",
      "I0228 01:32:47.376576 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2056236 virtual)\n",
      "I0228 01:32:47.381767 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2060913 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:47.410530 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2065603 virtual)\n",
      "I0228 01:32:47.414829 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2070349 virtual)\n",
      "I0228 01:32:47.435997 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2075206 virtual)\n",
      "I0228 01:32:47.461930 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2080656 virtual)\n",
      "I0228 01:32:47.464964 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2085587 virtual)\n",
      "I0228 01:32:47.472417 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2090826 virtual)\n",
      "I0228 01:32:47.475900 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2096058 virtual)\n",
      "I0228 01:32:47.495139 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2101253 virtual)\n",
      "I0228 01:32:47.499778 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2106470 virtual)\n",
      "I0228 01:32:47.502694 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2111899 virtual)\n",
      "I0228 01:32:47.527683 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2117097 virtual)\n",
      "I0228 01:32:47.535002 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2121773 virtual)\n",
      "I0228 01:32:47.554450 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2126911 virtual)\n",
      "I0228 01:32:47.559366 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2131622 virtual)\n",
      "I0228 01:32:47.563590 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2136490 virtual)\n",
      "I0228 01:32:47.587932 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2141045 virtual)\n",
      "I0228 01:32:47.604634 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2146153 virtual)\n",
      "I0228 01:32:47.615587 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2151530 virtual)\n",
      "I0228 01:32:47.624078 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2156992 virtual)\n",
      "I0228 01:32:47.628010 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2162028 virtual)\n",
      "I0228 01:32:47.648908 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2167212 virtual)\n",
      "I0228 01:32:47.666941 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2172420 virtual)\n",
      "I0228 01:32:47.677712 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2177152 virtual)\n",
      "I0228 01:32:47.683941 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2182174 virtual)\n",
      "I0228 01:32:47.686995 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2187390 virtual)\n",
      "I0228 01:32:47.705839 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2192175 virtual)\n",
      "I0228 01:32:47.729084 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2197252 virtual)\n",
      "I0228 01:32:47.739913 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2201978 virtual)\n",
      "I0228 01:32:47.746941 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2207158 virtual)\n",
      "I0228 01:32:47.755872 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2212233 virtual)\n",
      "I0228 01:32:47.766666 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2217450 virtual)\n",
      "I0228 01:32:47.790077 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2223036 virtual)\n",
      "I0228 01:32:47.795176 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2227784 virtual)\n",
      "I0228 01:32:47.807502 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2233205 virtual)\n",
      "I0228 01:32:47.822799 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2238147 virtual)\n",
      "I0228 01:32:47.828213 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2242965 virtual)\n",
      "I0228 01:32:47.851519 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2248138 virtual)\n",
      "I0228 01:32:47.856702 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2253249 virtual)\n",
      "I0228 01:32:47.870394 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2258371 virtual)\n",
      "I0228 01:32:47.885612 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2263684 virtual)\n",
      "I0228 01:32:47.890202 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2268495 virtual)\n",
      "I0228 01:32:47.909371 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2273783 virtual)\n",
      "I0228 01:32:47.918624 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2278605 virtual)\n",
      "I0228 01:32:47.935839 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2283494 virtual)\n",
      "I0228 01:32:47.944195 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2288625 virtual)\n",
      "I0228 01:32:47.956080 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2293675 virtual)\n",
      "I0228 01:32:47.969017 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2298739 virtual)\n",
      "I0228 01:32:47.980124 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2303330 virtual)\n",
      "I0228 01:32:47.996179 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2308508 virtual)\n",
      "I0228 01:32:48.007029 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2313681 virtual)\n",
      "I0228 01:32:48.019110 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2318679 virtual)\n",
      "I0228 01:32:48.030126 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2323605 virtual)\n",
      "I0228 01:32:48.038170 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2328946 virtual)\n",
      "I0228 01:32:48.051830 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2334151 virtual)\n",
      "I0228 01:32:48.067885 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2339459 virtual)\n",
      "I0228 01:32:48.082844 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2344359 virtual)\n",
      "I0228 01:32:48.092725 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2349287 virtual)\n",
      "I0228 01:32:48.097666 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2354500 virtual)\n",
      "I0228 01:32:48.114125 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2360121 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:48.128520 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2365115 virtual)\n",
      "I0228 01:32:48.146900 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2370166 virtual)\n",
      "I0228 01:32:48.151143 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2374738 virtual)\n",
      "I0228 01:32:48.159066 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2379632 virtual)\n",
      "I0228 01:32:48.176113 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2385157 virtual)\n",
      "I0228 01:32:48.193027 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2390022 virtual)\n",
      "I0228 01:32:48.208518 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2394944 virtual)\n",
      "I0228 01:32:48.211804 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2400619 virtual)\n",
      "I0228 01:32:48.219416 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2405541 virtual)\n",
      "I0228 01:32:48.245807 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2410507 virtual)\n",
      "I0228 01:32:48.252022 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2415488 virtual)\n",
      "I0228 01:32:48.267270 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2420448 virtual)\n",
      "I0228 01:32:48.272708 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2425391 virtual)\n",
      "I0228 01:32:48.280459 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2430564 virtual)\n",
      "I0228 01:32:48.311002 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2435753 virtual)\n",
      "I0228 01:32:48.315703 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2441136 virtual)\n",
      "I0228 01:32:48.325365 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2446235 virtual)\n",
      "I0228 01:32:48.336507 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2451118 virtual)\n",
      "I0228 01:32:48.340488 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2456391 virtual)\n",
      "I0228 01:32:48.368983 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2461532 virtual)\n",
      "I0228 01:32:48.376327 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2466198 virtual)\n",
      "I0228 01:32:48.384360 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2471151 virtual)\n",
      "I0228 01:32:48.395778 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2476428 virtual)\n",
      "I0228 01:32:48.399534 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2481464 virtual)\n",
      "I0228 01:32:48.433325 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2486805 virtual)\n",
      "I0228 01:32:48.439908 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2492180 virtual)\n",
      "I0228 01:32:48.445971 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2497036 virtual)\n",
      "I0228 01:32:48.453644 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2502545 virtual)\n",
      "I0228 01:32:48.460316 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2507067 virtual)\n",
      "I0228 01:32:48.494399 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2511830 virtual)\n",
      "I0228 01:32:48.498472 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2516729 virtual)\n",
      "I0228 01:32:48.503494 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2522178 virtual)\n",
      "I0228 01:32:48.517080 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2527397 virtual)\n",
      "I0228 01:32:48.519973 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2532478 virtual)\n",
      "I0228 01:32:48.555479 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2537364 virtual)\n",
      "I0228 01:32:48.561366 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2542559 virtual)\n",
      "I0228 01:32:48.565610 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2547621 virtual)\n",
      "I0228 01:32:48.569827 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2552492 virtual)\n",
      "I0228 01:32:48.580411 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2557287 virtual)\n",
      "I0228 01:32:48.623371 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2562244 virtual)\n",
      "I0228 01:32:48.628731 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2567279 virtual)\n",
      "I0228 01:32:48.634045 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2572674 virtual)\n",
      "I0228 01:32:48.636147 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2577596 virtual)\n",
      "I0228 01:32:48.639420 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2582573 virtual)\n",
      "I0228 01:32:48.679351 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2588109 virtual)\n",
      "I0228 01:32:48.683362 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2592969 virtual)\n",
      "I0228 01:32:48.687404 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2598051 virtual)\n",
      "I0228 01:32:48.690740 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2603291 virtual)\n",
      "I0228 01:32:48.695272 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2608626 virtual)\n",
      "I0228 01:32:48.741847 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2614014 virtual)\n",
      "I0228 01:32:48.749139 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2619328 virtual)\n",
      "I0228 01:32:48.755957 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2624365 virtual)\n",
      "I0228 01:32:48.758930 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2629918 virtual)\n",
      "I0228 01:32:48.761839 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2635170 virtual)\n",
      "I0228 01:32:48.801050 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2639980 virtual)\n",
      "I0228 01:32:48.810053 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2644955 virtual)\n",
      "I0228 01:32:48.814200 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2649945 virtual)\n",
      "I0228 01:32:48.818377 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2654977 virtual)\n",
      "I0228 01:32:48.822530 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2659926 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:32:48.867961 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2664939 virtual)\n",
      "I0228 01:32:48.872718 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2669728 virtual)\n",
      "I0228 01:32:48.878309 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2674876 virtual)\n",
      "I0228 01:32:48.882298 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2680041 virtual)\n",
      "I0228 01:32:48.885241 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2685469 virtual)\n",
      "I0228 01:32:48.923765 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2690866 virtual)\n",
      "I0228 01:32:48.929190 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2696139 virtual)\n",
      "I0228 01:32:48.934551 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2701440 virtual)\n",
      "I0228 01:32:48.943519 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2706638 virtual)\n",
      "I0228 01:32:48.946537 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2712057 virtual)\n",
      "I0228 01:32:48.982625 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2716941 virtual)\n",
      "I0228 01:32:48.987314 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2722198 virtual)\n",
      "I0228 01:32:48.992023 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2727083 virtual)\n",
      "I0228 01:32:49.019557 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2732523 virtual)\n",
      "I0228 01:32:49.026083 140370323834688 text_analysis.py:506] 552 batches submitted to accumulate stats from 35328 documents (2737809 virtual)\n",
      "I0228 01:32:49.046317 140370323834688 text_analysis.py:506] 553 batches submitted to accumulate stats from 35392 documents (2743536 virtual)\n",
      "I0228 01:32:49.049590 140370323834688 text_analysis.py:506] 554 batches submitted to accumulate stats from 35456 documents (2748922 virtual)\n",
      "I0228 01:32:49.052589 140370323834688 text_analysis.py:506] 555 batches submitted to accumulate stats from 35520 documents (2753984 virtual)\n",
      "I0228 01:32:49.066361 140370323834688 text_analysis.py:506] 556 batches submitted to accumulate stats from 35584 documents (2759009 virtual)\n",
      "I0228 01:32:49.071168 140370323834688 text_analysis.py:506] 557 batches submitted to accumulate stats from 35648 documents (2763935 virtual)\n",
      "I0228 01:32:49.107368 140370323834688 text_analysis.py:506] 558 batches submitted to accumulate stats from 35712 documents (2769003 virtual)\n",
      "I0228 01:32:49.111654 140370323834688 text_analysis.py:506] 559 batches submitted to accumulate stats from 35776 documents (2774542 virtual)\n",
      "I0228 01:32:49.115874 140370323834688 text_analysis.py:506] 560 batches submitted to accumulate stats from 35840 documents (2779722 virtual)\n",
      "I0228 01:32:49.128301 140370323834688 text_analysis.py:506] 561 batches submitted to accumulate stats from 35904 documents (2784854 virtual)\n",
      "I0228 01:32:49.133260 140370323834688 text_analysis.py:506] 562 batches submitted to accumulate stats from 35968 documents (2788239 virtual)\n",
      "I0228 01:32:49.232604 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:32:49.237592 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:32:49.235160 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:32:49.239238 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:32:49.247957 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:32:49.241305 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:32:49.238955 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:32:49.245154 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:32:49.252639 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:32:49.244231 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:32:49.675013 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:32:49.693577 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2788588 virtual documents\n",
      "/home/dhamzeia/Thesis/BiomedicalTopicModelling/contextualized_topic_models/models/ctm.py:511: Warning: This is an experimental feature that we has not been fully tested. Refer to the following issue:https://github.com/MilaNLProc/contextualized-topic-models/issues/38\n",
      "  Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores\n",
      "{'epoch': 74, 'cv': 0.6882101963397178, 'umass': -3.7817298788627562, 'uci': 0.02498273431171612, 'npmi': 0.07723182077800385, 'rbo': 1.0, 'td': 0.992, 'train_loss': 642.1155406974963, 'topics': [['c0267454', 'salivary', 'c0020933', 'ref', 'heparin', 'c0536858', 'c0117438', 'c0605290', 'cow', 'c1446219', 'c0851891', 'c0521990', 'c0444584', 'c0026231', 'c0042313', 'horse', 'excretion', 'c0036323', 'c0245109', 'shareholder', 'goat', 'thirty-three', 'candida', 'c0042610', 'c0014346', 'c1819716'], ['c0011900', 'c0012634', 'child', 'common', 'c0546788', 'clinical', 'c0032285', 'c1457887', 'c0015967', 'cause', 'c0019993', 'c3714514', 'sars-cov-2', 'c0809949', 'confirm', 'c0010076', 'manifestation', 'c0035236', 'c0035648', 'c0019994', 'presentation', 'c1446409', 'hospitalize', 'c0035235', 'c0021708', 'c0003232'], ['c0025080', 'c0543467', 'undergo', 'postoperative', 'c0005898', 'c0031150', 'perform', 'c0229962', 'c0009566', 'procedure', 'c0002940', 'c0728940', 'operative', 'c0038930', 'c1522577', 'technique', 'c0019080', 'complication', 'c0582175', 'c0162522', 'recurrence', 'preoperative', 'perioperative', 'aneurysm', 'c0850292', 'surgical'], ['compare', 'c0243095', 'difference', 'significant', 'significantly', 'receive', 'associate', 'c0199470', 'c0008976', 'c0021708', 'primary', 'concentration', 'c0032042', 'determine', 'assess', 'measure', 'c0005516', 'trial', 'c0034108', 'predict', 'c0430022', 'association', 'decrease', 'infant', 'measurement', 'c0087111'], ['crisis', 'policy', 'economic', 'political', 'international', 'public', 'challenge', 'threat', 'emergency', 'national', 'c1561598', 'market', 'disaster', 'face', 'argue', 'food', 'economy', 'financial', 'c0018696', 'sector', 'governance', 'way', 'draw', 'issue', 'supply', 'opportunity'], ['c0042210', 'c1167622', 'c1254351', 'c1514562', 'c0030956', 'c0029224', 'c0014442', 'c0003320', 'c0042736', 'interaction', 'c0033684', 'c0596901', 'c0003316', 'potential', 'bind', 'c1706082', 'c0035668', 'c0003250', 'c0678594', 'novel', 'c1148560', 'c0020971', 'potent', 'c0574031', 'c0243077', 'molecule'], ['c1171362', 'c0025929', 'c0007634', 'c0007613', 'role', 'mechanism', 'induce', 'activation', 'c0017262', 'c0079189', 'c0021368', 'c0024432', 'mouse', 'c3539881', 'c0301872', 'activate', 'c0039194', 'c0037080', 'induction', 'c0024109', 'c0021747', 'c1327622', 'c0162638', 'c0040300', 'c0041904', 'release'], ['propose', 'c3161035', 'c0002045', 'c0025663', 'accuracy', 'c0150098', 'machine', 'performance', 'prediction', 'automate', 'c0679083', 'solution', 'image', 'learn', 'base', 'representation', 'algorithm', 'c0037585', 'compute', 'application', 'sensor', 'c1710191', 'paper', 'solve', 'feature', 'c0037589'], ['c0679646', 'c2603343', 'search', 'conduct', 'train', 'c0027361', 'c1257890', 'report', 'include', 'c0242356', 'c0025353', 'impact', 'c0242481', 'c0086388', 'c0018724', 'need', 'c0282574', 'identify', 'evidence', 'measure', 'c0184661', 'c0038951', 'c0030971', 'c1706852', 'c0376554', 'psychological'], ['c1705920', 'c0042776', 'c0032098', 'sample', 'c0003062', 'c0017446', 'c0684063', 'c0017428', 'c0086418', 'c0017337', 'pathogen', 'c0012984', 'c0442726', 'c0242781', 'c1764827', 'genetic', 'c0005595', 'host', 'population', 'c0029347', 'c0007452', 'different', 'c0039005', 'c0004611', 'c0439663', 'c1511790']]}\n",
      "Epoch: [76/250]\tSamples: [2791176/9181500]\tTrain Loss: 642.1783701005419\tTime: 0:00:04.390126\n",
      "Epoch: [77/250]\tSamples: [2827902/9181500]\tTrain Loss: 642.25636916796\tTime: 0:00:04.502965\n",
      "Epoch: [78/250]\tSamples: [2864628/9181500]\tTrain Loss: 642.0468611729428\tTime: 0:00:04.561472\n",
      "Epoch: [79/250]\tSamples: [2901354/9181500]\tTrain Loss: 642.1233581965092\tTime: 0:00:04.512901\n",
      "Epoch: [80/250]\tSamples: [2938080/9181500]\tTrain Loss: 642.0095515183385\tTime: 0:00:04.839821\n",
      "Epoch: [81/250]\tSamples: [2974806/9181500]\tTrain Loss: 641.9988539496883\tTime: 0:00:04.767967\n",
      "Epoch: [82/250]\tSamples: [3011532/9181500]\tTrain Loss: 642.2039035696781\tTime: 0:00:04.773910\n",
      "Epoch: [83/250]\tSamples: [3048258/9181500]\tTrain Loss: 642.1483092274547\tTime: 0:00:04.774539\n",
      "Epoch: [84/250]\tSamples: [3084984/9181500]\tTrain Loss: 642.2480595320753\tTime: 0:00:04.779003\n",
      "Epoch: [85/250]\tSamples: [3121710/9181500]\tTrain Loss: 642.003030890922\tTime: 0:00:04.762488\n",
      "Epoch: [86/250]\tSamples: [3158436/9181500]\tTrain Loss: 642.1385215855253\tTime: 0:00:04.758326\n",
      "Epoch: [87/250]\tSamples: [3195162/9181500]\tTrain Loss: 642.2776314591501\tTime: 0:00:04.799126\n",
      "Epoch: [88/250]\tSamples: [3231888/9181500]\tTrain Loss: 642.2215890139139\tTime: 0:00:04.778174\n",
      "Epoch: [89/250]\tSamples: [3268614/9181500]\tTrain Loss: 641.8081432006617\tTime: 0:00:04.771314\n",
      "Epoch: [90/250]\tSamples: [3305340/9181500]\tTrain Loss: 642.1327266658839\tTime: 0:00:04.752653\n",
      "Epoch: [91/250]\tSamples: [3342066/9181500]\tTrain Loss: 641.9010718734684\tTime: 0:00:04.737672\n",
      "Epoch: [92/250]\tSamples: [3378792/9181500]\tTrain Loss: 641.9064392179587\tTime: 0:00:04.732700\n",
      "Epoch: [93/250]\tSamples: [3415518/9181500]\tTrain Loss: 642.0417387800877\tTime: 0:00:04.808604\n",
      "Epoch: [94/250]\tSamples: [3452244/9181500]\tTrain Loss: 642.1304764718795\tTime: 0:00:04.768644\n",
      "Epoch: [95/250]\tSamples: [3488970/9181500]\tTrain Loss: 642.0314849536092\tTime: 0:00:04.764574\n",
      "Epoch: [96/250]\tSamples: [3525696/9181500]\tTrain Loss: 642.1762122287344\tTime: 0:00:04.753839\n",
      "Epoch: [97/250]\tSamples: [3562422/9181500]\tTrain Loss: 641.9692735136551\tTime: 0:00:04.735347\n",
      "Epoch: [98/250]\tSamples: [3599148/9181500]\tTrain Loss: 641.8504443165265\tTime: 0:00:04.752388\n",
      "Epoch: [99/250]\tSamples: [3635874/9181500]\tTrain Loss: 642.2161348780496\tTime: 0:00:04.770366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:34:50.025277 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [100/250]\tSamples: [3672600/9181500]\tTrain Loss: 641.9431243150289\tTime: 0:00:04.733976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:34:50.858185 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:34:51.548513 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:34:52.365399 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:34:52.896368 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:34:52.902707 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:34:53.656712 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:34:54.328966 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:34:55.129667 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:34:55.648465 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:34:55.653390 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:34:56.410406 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:34:57.078454 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:34:57.877124 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:34:58.397721 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:34:58.404187 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:34:59.160316 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:34:59.835519 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:35:00.641407 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:35:01.159965 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:35:01.181855 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:35:01.726279 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (-35602 virtual)\n",
      "I0228 01:35:02.228755 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (-211845 virtual)\n",
      "I0228 01:35:02.584540 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (-503716 virtual)\n",
      "I0228 01:35:02.586901 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (-503693 virtual)\n",
      "I0228 01:35:02.627283 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (-509696 virtual)\n",
      "I0228 01:35:02.743789 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (-536384 virtual)\n",
      "I0228 01:35:02.747169 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (-534161 virtual)\n",
      "I0228 01:35:02.753369 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (-532395 virtual)\n",
      "I0228 01:35:02.756991 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (-530806 virtual)\n",
      "I0228 01:35:02.763763 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (-532670 virtual)\n",
      "I0228 01:35:03.570680 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:35:03.571489 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:35:03.576011 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:35:03.576411 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:35:03.576557 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:35:03.583576 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:35:03.574928 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:35:03.589843 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:35:03.578581 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:35:03.574072 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:35:04.000165 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:35:04.019381 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 229853 virtual documents\n",
      "I0228 01:35:05.898953 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 1000 documents\n",
      "I0228 01:35:05.915149 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 2000 documents\n",
      "I0228 01:35:05.931533 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 3000 documents\n",
      "I0228 01:35:05.947996 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 4000 documents\n",
      "I0228 01:35:05.962856 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 5000 documents\n",
      "I0228 01:35:05.976302 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 6000 documents\n",
      "I0228 01:35:05.990061 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 7000 documents\n",
      "I0228 01:35:06.003978 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 8000 documents\n",
      "I0228 01:35:06.020028 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 9000 documents\n",
      "I0228 01:35:06.035961 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 10000 documents\n",
      "I0228 01:35:06.052934 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 11000 documents\n",
      "I0228 01:35:06.067023 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 12000 documents\n",
      "I0228 01:35:06.079339 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 13000 documents\n",
      "I0228 01:35:06.091934 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 14000 documents\n",
      "I0228 01:35:06.104684 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 15000 documents\n",
      "I0228 01:35:06.117659 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 16000 documents\n",
      "I0228 01:35:06.130127 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 17000 documents\n",
      "I0228 01:35:06.142378 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 18000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:06.154812 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 19000 documents\n",
      "I0228 01:35:06.168990 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 20000 documents\n",
      "I0228 01:35:06.185153 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 21000 documents\n",
      "I0228 01:35:06.201119 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 22000 documents\n",
      "I0228 01:35:06.218025 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 23000 documents\n",
      "I0228 01:35:06.235174 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 24000 documents\n",
      "I0228 01:35:06.250635 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 25000 documents\n",
      "I0228 01:35:06.265751 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 26000 documents\n",
      "I0228 01:35:06.281530 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 27000 documents\n",
      "I0228 01:35:06.297308 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 28000 documents\n",
      "I0228 01:35:06.312339 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 29000 documents\n",
      "I0228 01:35:06.328064 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 30000 documents\n",
      "I0228 01:35:06.343368 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 31000 documents\n",
      "I0228 01:35:06.358627 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 32000 documents\n",
      "I0228 01:35:06.374082 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 33000 documents\n",
      "I0228 01:35:06.389322 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 34000 documents\n",
      "I0228 01:35:06.405364 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 35000 documents\n",
      "I0228 01:35:06.421881 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 36000 documents\n",
      "I0228 01:35:06.585383 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:35:06.996372 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (4980 virtual)\n",
      "I0228 01:35:07.000676 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10307 virtual)\n",
      "I0228 01:35:07.003404 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16414 virtual)\n",
      "I0228 01:35:07.006536 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21149 virtual)\n",
      "I0228 01:35:07.014779 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27032 virtual)\n",
      "I0228 01:35:07.018726 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32205 virtual)\n",
      "I0228 01:35:07.021310 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37609 virtual)\n",
      "I0228 01:35:07.023687 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43423 virtual)\n",
      "I0228 01:35:07.025919 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48656 virtual)\n",
      "I0228 01:35:07.028754 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54346 virtual)\n",
      "I0228 01:35:07.063079 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60210 virtual)\n",
      "I0228 01:35:07.066193 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65483 virtual)\n",
      "I0228 01:35:07.069681 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70552 virtual)\n",
      "I0228 01:35:07.082806 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (75767 virtual)\n",
      "I0228 01:35:07.093710 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81407 virtual)\n",
      "I0228 01:35:07.125505 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87510 virtual)\n",
      "I0228 01:35:07.130266 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93328 virtual)\n",
      "I0228 01:35:07.138257 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (98857 virtual)\n",
      "I0228 01:35:07.144826 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104131 virtual)\n",
      "I0228 01:35:07.162542 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109598 virtual)\n",
      "I0228 01:35:07.190421 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (114991 virtual)\n",
      "I0228 01:35:07.195059 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120254 virtual)\n",
      "I0228 01:35:07.202990 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (125931 virtual)\n",
      "I0228 01:35:07.208425 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (131754 virtual)\n",
      "I0228 01:35:07.229358 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137291 virtual)\n",
      "I0228 01:35:07.258706 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (142797 virtual)\n",
      "I0228 01:35:07.262793 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148247 virtual)\n",
      "I0228 01:35:07.267391 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154364 virtual)\n",
      "I0228 01:35:07.272619 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160342 virtual)\n",
      "I0228 01:35:07.292688 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (165623 virtual)\n",
      "I0228 01:35:07.319377 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (171132 virtual)\n",
      "I0228 01:35:07.324270 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176531 virtual)\n",
      "I0228 01:35:07.335407 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (181818 virtual)\n",
      "I0228 01:35:07.339711 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (186909 virtual)\n",
      "I0228 01:35:07.356187 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (192856 virtual)\n",
      "I0228 01:35:07.385065 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (198133 virtual)\n",
      "I0228 01:35:07.389219 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (203591 virtual)\n",
      "I0228 01:35:07.407745 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (209248 virtual)\n",
      "I0228 01:35:07.412083 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (214706 virtual)\n",
      "I0228 01:35:07.416487 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220240 virtual)\n",
      "I0228 01:35:07.449720 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (225695 virtual)\n",
      "I0228 01:35:07.453491 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (233198 virtual)\n",
      "I0228 01:35:07.470011 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (238688 virtual)\n",
      "I0228 01:35:07.473559 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (244509 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:07.483613 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (250113 virtual)\n",
      "I0228 01:35:07.512657 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (255391 virtual)\n",
      "I0228 01:35:07.517347 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (260604 virtual)\n",
      "I0228 01:35:07.533846 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (266002 virtual)\n",
      "I0228 01:35:07.538402 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (271447 virtual)\n",
      "I0228 01:35:07.547541 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (276707 virtual)\n",
      "I0228 01:35:07.576648 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (282221 virtual)\n",
      "I0228 01:35:07.588730 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288259 virtual)\n",
      "I0228 01:35:07.599761 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (293154 virtual)\n",
      "I0228 01:35:07.605629 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (298752 virtual)\n",
      "I0228 01:35:07.613034 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304277 virtual)\n",
      "I0228 01:35:07.636144 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (309398 virtual)\n",
      "I0228 01:35:07.652362 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (315027 virtual)\n",
      "I0228 01:35:07.663735 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (320938 virtual)\n",
      "I0228 01:35:07.675507 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (326088 virtual)\n",
      "I0228 01:35:07.679250 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (332462 virtual)\n",
      "I0228 01:35:07.700514 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (338107 virtual)\n",
      "I0228 01:35:07.721158 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (343966 virtual)\n",
      "I0228 01:35:07.725867 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (349516 virtual)\n",
      "I0228 01:35:07.738064 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (355280 virtual)\n",
      "I0228 01:35:07.741055 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (361296 virtual)\n",
      "I0228 01:35:07.761206 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (366803 virtual)\n",
      "I0228 01:35:07.788251 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (372383 virtual)\n",
      "I0228 01:35:07.794366 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (378274 virtual)\n",
      "I0228 01:35:07.801621 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (384175 virtual)\n",
      "I0228 01:35:07.816225 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (389207 virtual)\n",
      "I0228 01:35:07.823086 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (394147 virtual)\n",
      "I0228 01:35:07.854161 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398334 virtual)\n",
      "I0228 01:35:07.858876 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (401829 virtual)\n",
      "I0228 01:35:07.868447 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (405702 virtual)\n",
      "I0228 01:35:07.886152 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (410288 virtual)\n",
      "I0228 01:35:07.890218 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (414695 virtual)\n",
      "I0228 01:35:07.919448 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (419136 virtual)\n",
      "I0228 01:35:07.928566 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (423265 virtual)\n",
      "I0228 01:35:07.937097 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (427670 virtual)\n",
      "I0228 01:35:07.941780 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (432055 virtual)\n",
      "I0228 01:35:07.946132 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (436983 virtual)\n",
      "I0228 01:35:07.969030 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (441863 virtual)\n",
      "I0228 01:35:07.973280 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (446434 virtual)\n",
      "I0228 01:35:07.982665 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (450448 virtual)\n",
      "I0228 01:35:07.988345 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (454503 virtual)\n",
      "I0228 01:35:07.991231 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (458114 virtual)\n",
      "I0228 01:35:08.016418 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (461562 virtual)\n",
      "I0228 01:35:08.020416 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (465633 virtual)\n",
      "I0228 01:35:08.032330 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (470068 virtual)\n",
      "I0228 01:35:08.038078 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (474795 virtual)\n",
      "I0228 01:35:08.061468 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (479455 virtual)\n",
      "I0228 01:35:08.069317 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (483942 virtual)\n",
      "I0228 01:35:08.076364 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (488586 virtual)\n",
      "I0228 01:35:08.080166 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (493326 virtual)\n",
      "I0228 01:35:08.082967 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (498035 virtual)\n",
      "I0228 01:35:08.087078 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (502366 virtual)\n",
      "I0228 01:35:08.108150 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (506686 virtual)\n",
      "I0228 01:35:08.122500 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (512069 virtual)\n",
      "I0228 01:35:08.129670 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (517237 virtual)\n",
      "I0228 01:35:08.137197 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (521412 virtual)\n",
      "I0228 01:35:08.140772 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (525781 virtual)\n",
      "I0228 01:35:08.159595 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (530439 virtual)\n",
      "I0228 01:35:08.180758 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (534341 virtual)\n",
      "I0228 01:35:08.183762 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (538681 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:08.190191 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (542721 virtual)\n",
      "I0228 01:35:08.195978 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (547173 virtual)\n",
      "I0228 01:35:08.212650 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (551474 virtual)\n",
      "I0228 01:35:08.239163 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (555643 virtual)\n",
      "I0228 01:35:08.244274 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (560391 virtual)\n",
      "I0228 01:35:08.249227 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (564295 virtual)\n",
      "I0228 01:35:08.254397 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (569007 virtual)\n",
      "I0228 01:35:08.271886 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (572993 virtual)\n",
      "I0228 01:35:08.283668 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (576881 virtual)\n",
      "I0228 01:35:08.290365 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (580748 virtual)\n",
      "I0228 01:35:08.293866 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (583784 virtual)\n",
      "I0228 01:35:08.298902 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (588507 virtual)\n",
      "I0228 01:35:08.319008 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (594028 virtual)\n",
      "I0228 01:35:08.328805 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (599430 virtual)\n",
      "I0228 01:35:08.333878 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (604091 virtual)\n",
      "I0228 01:35:08.345608 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (609522 virtual)\n",
      "I0228 01:35:08.353738 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (615401 virtual)\n",
      "I0228 01:35:08.363998 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (620548 virtual)\n",
      "I0228 01:35:08.370804 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (624738 virtual)\n",
      "I0228 01:35:08.379977 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (629613 virtual)\n",
      "I0228 01:35:08.384277 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (633477 virtual)\n",
      "I0228 01:35:08.408666 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (638708 virtual)\n",
      "I0228 01:35:08.430973 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (643955 virtual)\n",
      "I0228 01:35:08.435644 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (649300 virtual)\n",
      "I0228 01:35:08.438991 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (654966 virtual)\n",
      "I0228 01:35:08.445788 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (659939 virtual)\n",
      "I0228 01:35:08.477833 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (664420 virtual)\n",
      "I0228 01:35:08.481914 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (669693 virtual)\n",
      "I0228 01:35:08.491931 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (674924 virtual)\n",
      "I0228 01:35:08.494888 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (680178 virtual)\n",
      "I0228 01:35:08.497999 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (685132 virtual)\n",
      "I0228 01:35:08.537661 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (690053 virtual)\n",
      "I0228 01:35:08.541871 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (695186 virtual)\n",
      "I0228 01:35:08.548064 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (700311 virtual)\n",
      "I0228 01:35:08.552836 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (705599 virtual)\n",
      "I0228 01:35:08.558215 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (710662 virtual)\n",
      "I0228 01:35:08.585835 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (715870 virtual)\n",
      "I0228 01:35:08.599106 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (721370 virtual)\n",
      "I0228 01:35:08.609144 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (726297 virtual)\n",
      "I0228 01:35:08.613768 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (731744 virtual)\n",
      "I0228 01:35:08.618534 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (737135 virtual)\n",
      "I0228 01:35:08.642469 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (742801 virtual)\n",
      "I0228 01:35:08.657753 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (747628 virtual)\n",
      "I0228 01:35:08.669521 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (752937 virtual)\n",
      "I0228 01:35:08.679832 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (757926 virtual)\n",
      "I0228 01:35:08.682944 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (763551 virtual)\n",
      "I0228 01:35:08.699527 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (768518 virtual)\n",
      "I0228 01:35:08.716466 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (774505 virtual)\n",
      "I0228 01:35:08.724587 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (780299 virtual)\n",
      "I0228 01:35:08.730722 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (786017 virtual)\n",
      "I0228 01:35:08.743438 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (791944 virtual)\n",
      "I0228 01:35:08.763299 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (798037 virtual)\n",
      "I0228 01:35:08.767818 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (803519 virtual)\n",
      "I0228 01:35:08.781934 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (808916 virtual)\n",
      "I0228 01:35:08.784949 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (814753 virtual)\n",
      "I0228 01:35:08.809012 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (820152 virtual)\n",
      "I0228 01:35:08.819170 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (826333 virtual)\n",
      "I0228 01:35:08.837771 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (831938 virtual)\n",
      "I0228 01:35:08.852021 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (837330 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:08.856851 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (842736 virtual)\n",
      "I0228 01:35:08.876407 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (848818 virtual)\n",
      "I0228 01:35:08.886327 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (854765 virtual)\n",
      "I0228 01:35:08.899874 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (860640 virtual)\n",
      "I0228 01:35:08.913700 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (866484 virtual)\n",
      "I0228 01:35:08.921266 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (872030 virtual)\n",
      "I0228 01:35:08.937875 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (878282 virtual)\n",
      "I0228 01:35:08.957443 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (884006 virtual)\n",
      "I0228 01:35:08.963358 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (890014 virtual)\n",
      "I0228 01:35:08.984244 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (895914 virtual)\n",
      "I0228 01:35:08.988610 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (901583 virtual)\n",
      "I0228 01:35:09.005260 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (907878 virtual)\n",
      "I0228 01:35:09.021173 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (914555 virtual)\n",
      "I0228 01:35:09.030163 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (918598 virtual)\n",
      "I0228 01:35:09.049283 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (922686 virtual)\n",
      "I0228 01:35:09.053517 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (925092 virtual)\n",
      "I0228 01:35:09.072721 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (931297 virtual)\n",
      "I0228 01:35:09.084643 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (935621 virtual)\n",
      "I0228 01:35:09.094856 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (939579 virtual)\n",
      "I0228 01:35:09.115828 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (943330 virtual)\n",
      "I0228 01:35:09.123210 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (947135 virtual)\n",
      "I0228 01:35:09.140072 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (950776 virtual)\n",
      "I0228 01:35:09.144575 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (954357 virtual)\n",
      "I0228 01:35:09.154967 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (958073 virtual)\n",
      "I0228 01:35:09.160404 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (961981 virtual)\n",
      "I0228 01:35:09.164217 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (965286 virtual)\n",
      "I0228 01:35:09.189918 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (969567 virtual)\n",
      "I0228 01:35:09.199390 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (973194 virtual)\n",
      "I0228 01:35:09.202372 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (976932 virtual)\n",
      "I0228 01:35:09.205464 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (980543 virtual)\n",
      "I0228 01:35:09.212235 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (984370 virtual)\n",
      "I0228 01:35:09.226042 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (987873 virtual)\n",
      "I0228 01:35:09.240475 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (991755 virtual)\n",
      "I0228 01:35:09.244807 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (995510 virtual)\n",
      "I0228 01:35:09.247449 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (999077 virtual)\n",
      "I0228 01:35:09.250336 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1002977 virtual)\n",
      "I0228 01:35:09.271856 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1006968 virtual)\n",
      "I0228 01:35:09.281008 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1010397 virtual)\n",
      "I0228 01:35:09.285153 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1014171 virtual)\n",
      "I0228 01:35:09.288682 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1017994 virtual)\n",
      "I0228 01:35:09.292072 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1022466 virtual)\n",
      "I0228 01:35:09.309551 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1027600 virtual)\n",
      "I0228 01:35:09.321088 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1031495 virtual)\n",
      "I0228 01:35:09.325769 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1035483 virtual)\n",
      "I0228 01:35:09.330378 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1039238 virtual)\n",
      "I0228 01:35:09.333460 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1043830 virtual)\n",
      "I0228 01:35:09.351950 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1047820 virtual)\n",
      "I0228 01:35:09.356343 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1051431 virtual)\n",
      "I0228 01:35:09.362823 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1055844 virtual)\n",
      "I0228 01:35:09.365420 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1059634 virtual)\n",
      "I0228 01:35:09.381118 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1063642 virtual)\n",
      "I0228 01:35:09.399637 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1067439 virtual)\n",
      "I0228 01:35:09.405241 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1071201 virtual)\n",
      "I0228 01:35:09.411252 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1075914 virtual)\n",
      "I0228 01:35:09.414771 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1079862 virtual)\n",
      "I0228 01:35:09.436083 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1084535 virtual)\n",
      "I0228 01:35:09.440809 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1088904 virtual)\n",
      "I0228 01:35:09.443526 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1093465 virtual)\n",
      "I0228 01:35:09.448545 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1097297 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:09.451222 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1100768 virtual)\n",
      "I0228 01:35:09.473351 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1105180 virtual)\n",
      "I0228 01:35:09.481164 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1109286 virtual)\n",
      "I0228 01:35:09.484916 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1112952 virtual)\n",
      "I0228 01:35:09.492073 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1117305 virtual)\n",
      "I0228 01:35:09.497313 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1121435 virtual)\n",
      "I0228 01:35:09.525977 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1126568 virtual)\n",
      "I0228 01:35:09.529848 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1130252 virtual)\n",
      "I0228 01:35:09.534701 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1135070 virtual)\n",
      "I0228 01:35:09.539341 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1139797 virtual)\n",
      "I0228 01:35:09.542717 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1144338 virtual)\n",
      "I0228 01:35:09.568071 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1149782 virtual)\n",
      "I0228 01:35:09.570733 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1153656 virtual)\n",
      "I0228 01:35:09.574359 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1157219 virtual)\n",
      "I0228 01:35:09.580641 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1161143 virtual)\n",
      "I0228 01:35:09.583597 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1165711 virtual)\n",
      "I0228 01:35:09.613642 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1169383 virtual)\n",
      "I0228 01:35:09.623646 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1174327 virtual)\n",
      "I0228 01:35:09.626561 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1178912 virtual)\n",
      "I0228 01:35:09.632511 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1182686 virtual)\n",
      "I0228 01:35:09.637480 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1187406 virtual)\n",
      "I0228 01:35:09.664278 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1192274 virtual)\n",
      "I0228 01:35:09.667953 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1196088 virtual)\n",
      "I0228 01:35:09.676996 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1200724 virtual)\n",
      "I0228 01:35:09.681379 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1205063 virtual)\n",
      "I0228 01:35:09.687762 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1209463 virtual)\n",
      "I0228 01:35:09.704505 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1213500 virtual)\n",
      "I0228 01:35:09.710145 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1217729 virtual)\n",
      "I0228 01:35:09.717483 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1221551 virtual)\n",
      "I0228 01:35:09.721330 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1226240 virtual)\n",
      "I0228 01:35:09.735540 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1230409 virtual)\n",
      "I0228 01:35:09.748672 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1234174 virtual)\n",
      "I0228 01:35:09.755423 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1237430 virtual)\n",
      "I0228 01:35:09.763623 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1240720 virtual)\n",
      "I0228 01:35:09.766813 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1244890 virtual)\n",
      "I0228 01:35:09.776858 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1248764 virtual)\n",
      "I0228 01:35:09.787418 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1253497 virtual)\n",
      "I0228 01:35:09.798879 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1258598 virtual)\n",
      "I0228 01:35:09.807054 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1262910 virtual)\n",
      "I0228 01:35:09.811656 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1267018 virtual)\n",
      "I0228 01:35:09.827411 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1271125 virtual)\n",
      "I0228 01:35:09.837212 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1275184 virtual)\n",
      "I0228 01:35:09.853965 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1279023 virtual)\n",
      "I0228 01:35:09.858301 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1282933 virtual)\n",
      "I0228 01:35:09.860941 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1287008 virtual)\n",
      "I0228 01:35:09.864217 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1291670 virtual)\n",
      "I0228 01:35:09.878450 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1295316 virtual)\n",
      "I0228 01:35:09.881627 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1299893 virtual)\n",
      "I0228 01:35:09.885405 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1304084 virtual)\n",
      "I0228 01:35:09.894507 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1307494 virtual)\n",
      "I0228 01:35:09.901649 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1311597 virtual)\n",
      "I0228 01:35:09.919553 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1316604 virtual)\n",
      "I0228 01:35:09.925038 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1321038 virtual)\n",
      "I0228 01:35:09.928564 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1324382 virtual)\n",
      "I0228 01:35:09.933192 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1329062 virtual)\n",
      "I0228 01:35:09.952889 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1332966 virtual)\n",
      "I0228 01:35:09.956752 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1336270 virtual)\n",
      "I0228 01:35:09.960935 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1340267 virtual)\n",
      "I0228 01:35:09.970072 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1344528 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:09.973269 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1348573 virtual)\n",
      "I0228 01:35:09.992733 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1353287 virtual)\n",
      "I0228 01:35:10.004225 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1357538 virtual)\n",
      "I0228 01:35:10.008621 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1362380 virtual)\n",
      "I0228 01:35:10.013285 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1366531 virtual)\n",
      "I0228 01:35:10.019593 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1368981 virtual)\n",
      "I0228 01:35:10.030880 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1374219 virtual)\n",
      "I0228 01:35:10.039259 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1379484 virtual)\n",
      "I0228 01:35:10.043585 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1383672 virtual)\n",
      "I0228 01:35:10.054759 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1388281 virtual)\n",
      "I0228 01:35:10.064869 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1392268 virtual)\n",
      "I0228 01:35:10.076988 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1397448 virtual)\n",
      "I0228 01:35:10.081635 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1401088 virtual)\n",
      "I0228 01:35:10.092941 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1406064 virtual)\n",
      "I0228 01:35:10.096277 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1411123 virtual)\n",
      "I0228 01:35:10.102492 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1415207 virtual)\n",
      "I0228 01:35:10.135733 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1419554 virtual)\n",
      "I0228 01:35:10.140449 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1424306 virtual)\n",
      "I0228 01:35:10.144902 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1428659 virtual)\n",
      "I0228 01:35:10.148651 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1433166 virtual)\n",
      "I0228 01:35:10.151386 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1437657 virtual)\n",
      "I0228 01:35:10.177827 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1442108 virtual)\n",
      "I0228 01:35:10.196174 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1446769 virtual)\n",
      "I0228 01:35:10.200023 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1452479 virtual)\n",
      "I0228 01:35:10.203179 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1457229 virtual)\n",
      "I0228 01:35:10.206371 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1462982 virtual)\n",
      "I0228 01:35:10.224414 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1467890 virtual)\n",
      "I0228 01:35:10.240965 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1473200 virtual)\n",
      "I0228 01:35:10.246457 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1479129 virtual)\n",
      "I0228 01:35:10.252751 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1486684 virtual)\n",
      "I0228 01:35:10.256061 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1493107 virtual)\n",
      "I0228 01:35:10.272273 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1498057 virtual)\n",
      "I0228 01:35:10.288496 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1502926 virtual)\n",
      "I0228 01:35:10.293530 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1508197 virtual)\n",
      "I0228 01:35:10.306673 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1513434 virtual)\n",
      "I0228 01:35:10.310790 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1518250 virtual)\n",
      "I0228 01:35:10.331689 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1523252 virtual)\n",
      "I0228 01:35:10.344050 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1528334 virtual)\n",
      "I0228 01:35:10.352070 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1538304 virtual)\n",
      "I0228 01:35:10.374697 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1543945 virtual)\n",
      "I0228 01:35:10.379988 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1549114 virtual)\n",
      "I0228 01:35:10.386771 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1554947 virtual)\n",
      "I0228 01:35:10.394382 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1559918 virtual)\n",
      "I0228 01:35:10.412063 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1563741 virtual)\n",
      "I0228 01:35:10.430745 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1568489 virtual)\n",
      "I0228 01:35:10.443483 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1573484 virtual)\n",
      "I0228 01:35:10.446901 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1578115 virtual)\n",
      "I0228 01:35:10.449726 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1583091 virtual)\n",
      "I0228 01:35:10.488961 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1588541 virtual)\n",
      "I0228 01:35:10.493689 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1594651 virtual)\n",
      "I0228 01:35:10.501148 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1600501 virtual)\n",
      "I0228 01:35:10.504194 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1606021 virtual)\n",
      "I0228 01:35:10.513760 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1611536 virtual)\n",
      "I0228 01:35:10.531930 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1617560 virtual)\n",
      "I0228 01:35:10.544522 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1623313 virtual)\n",
      "I0228 01:35:10.555557 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1627670 virtual)\n",
      "I0228 01:35:10.560850 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1633314 virtual)\n",
      "I0228 01:35:10.565346 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1638169 virtual)\n",
      "I0228 01:35:10.590047 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1643540 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:10.613532 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1648719 virtual)\n",
      "I0228 01:35:10.618687 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1654273 virtual)\n",
      "I0228 01:35:10.623647 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1659341 virtual)\n",
      "I0228 01:35:10.627688 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1665105 virtual)\n",
      "I0228 01:35:10.663274 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1669953 virtual)\n",
      "I0228 01:35:10.668217 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1676306 virtual)\n",
      "I0228 01:35:10.675338 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1684416 virtual)\n",
      "I0228 01:35:10.681381 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1693039 virtual)\n",
      "I0228 01:35:10.684604 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1701205 virtual)\n",
      "I0228 01:35:10.712524 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1709194 virtual)\n",
      "I0228 01:35:10.723261 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1715132 virtual)\n",
      "I0228 01:35:10.734089 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1719916 virtual)\n",
      "I0228 01:35:10.738865 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1726530 virtual)\n",
      "I0228 01:35:10.746797 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1732045 virtual)\n",
      "I0228 01:35:10.766837 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1737964 virtual)\n",
      "I0228 01:35:10.797422 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1744048 virtual)\n",
      "I0228 01:35:10.826694 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1749666 virtual)\n",
      "I0228 01:35:10.835370 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1754763 virtual)\n",
      "I0228 01:35:10.841148 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1760421 virtual)\n",
      "I0228 01:35:10.860736 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1765513 virtual)\n",
      "I0228 01:35:10.869734 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1770604 virtual)\n",
      "I0228 01:35:10.880279 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1775586 virtual)\n",
      "I0228 01:35:10.898946 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1780666 virtual)\n",
      "I0228 01:35:10.907943 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1786068 virtual)\n",
      "I0228 01:35:10.924647 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1791515 virtual)\n",
      "I0228 01:35:10.936920 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1796181 virtual)\n",
      "I0228 01:35:10.944005 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1801024 virtual)\n",
      "I0228 01:35:10.953842 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1805906 virtual)\n",
      "I0228 01:35:10.974244 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1811201 virtual)\n",
      "I0228 01:35:10.984354 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1816185 virtual)\n",
      "I0228 01:35:11.000381 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1821407 virtual)\n",
      "I0228 01:35:11.005506 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1826439 virtual)\n",
      "I0228 01:35:11.014944 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1831657 virtual)\n",
      "I0228 01:35:11.036692 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1836872 virtual)\n",
      "I0228 01:35:11.049299 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1841774 virtual)\n",
      "I0228 01:35:11.057836 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1846634 virtual)\n",
      "I0228 01:35:11.062377 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1851967 virtual)\n",
      "I0228 01:35:11.073820 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1857040 virtual)\n",
      "I0228 01:35:11.097477 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1862619 virtual)\n",
      "I0228 01:35:11.102771 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1867466 virtual)\n",
      "I0228 01:35:11.114696 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1872318 virtual)\n",
      "I0228 01:35:11.117689 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1876922 virtual)\n",
      "I0228 01:35:11.131781 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1882268 virtual)\n",
      "I0228 01:35:11.168726 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1887279 virtual)\n",
      "I0228 01:35:11.174932 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1892244 virtual)\n",
      "I0228 01:35:11.178644 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1897046 virtual)\n",
      "I0228 01:35:11.192496 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1902313 virtual)\n",
      "I0228 01:35:11.205674 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1907884 virtual)\n",
      "I0228 01:35:11.219138 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1913017 virtual)\n",
      "I0228 01:35:11.222393 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1917759 virtual)\n",
      "I0228 01:35:11.229274 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1923199 virtual)\n",
      "I0228 01:35:11.232317 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1928433 virtual)\n",
      "I0228 01:35:11.252230 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1933233 virtual)\n",
      "I0228 01:35:11.280659 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1937712 virtual)\n",
      "I0228 01:35:11.285009 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1942602 virtual)\n",
      "I0228 01:35:11.288174 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1947772 virtual)\n",
      "I0228 01:35:11.293864 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1952922 virtual)\n",
      "I0228 01:35:11.316663 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1958010 virtual)\n",
      "I0228 01:35:11.332837 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1962868 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:11.341032 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1967665 virtual)\n",
      "I0228 01:35:11.346500 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1973140 virtual)\n",
      "I0228 01:35:11.353190 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (1978281 virtual)\n",
      "I0228 01:35:11.374883 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (1983431 virtual)\n",
      "I0228 01:35:11.383983 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (1988126 virtual)\n",
      "I0228 01:35:11.397233 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (1993234 virtual)\n",
      "I0228 01:35:11.406818 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (1998822 virtual)\n",
      "I0228 01:35:11.412109 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (2003769 virtual)\n",
      "I0228 01:35:11.434233 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2009323 virtual)\n",
      "I0228 01:35:11.446311 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2014496 virtual)\n",
      "I0228 01:35:11.452314 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2019894 virtual)\n",
      "I0228 01:35:11.471606 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2024841 virtual)\n",
      "I0228 01:35:11.476042 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2030082 virtual)\n",
      "I0228 01:35:11.496328 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2035259 virtual)\n",
      "I0228 01:35:11.501210 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2040350 virtual)\n",
      "I0228 01:35:11.513464 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2045120 virtual)\n",
      "I0228 01:35:11.527300 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2049995 virtual)\n",
      "I0228 01:35:11.537389 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2054841 virtual)\n",
      "I0228 01:35:11.562153 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2059537 virtual)\n",
      "I0228 01:35:11.567041 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2064463 virtual)\n",
      "I0228 01:35:11.574139 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2069148 virtual)\n",
      "I0228 01:35:11.588426 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2073976 virtual)\n",
      "I0228 01:35:11.598286 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2079340 virtual)\n",
      "I0228 01:35:11.622155 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2084278 virtual)\n",
      "I0228 01:35:11.628413 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2089762 virtual)\n",
      "I0228 01:35:11.633279 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2094816 virtual)\n",
      "I0228 01:35:11.648013 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2100033 virtual)\n",
      "I0228 01:35:11.654953 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2105335 virtual)\n",
      "I0228 01:35:11.680667 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2110633 virtual)\n",
      "I0228 01:35:11.686315 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2115939 virtual)\n",
      "I0228 01:35:11.691458 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2120516 virtual)\n",
      "I0228 01:35:11.704731 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2125601 virtual)\n",
      "I0228 01:35:11.715169 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2130382 virtual)\n",
      "I0228 01:35:11.737987 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2135127 virtual)\n",
      "I0228 01:35:11.746616 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2140162 virtual)\n",
      "I0228 01:35:11.758997 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2145103 virtual)\n",
      "I0228 01:35:11.765112 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2150302 virtual)\n",
      "I0228 01:35:11.774012 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2155584 virtual)\n",
      "I0228 01:35:11.799252 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2160822 virtual)\n",
      "I0228 01:35:11.807063 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2166008 virtual)\n",
      "I0228 01:35:11.818592 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2171235 virtual)\n",
      "I0228 01:35:11.822592 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2176085 virtual)\n",
      "I0228 01:35:11.828841 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2181259 virtual)\n",
      "I0228 01:35:11.856640 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2186102 virtual)\n",
      "I0228 01:35:11.866545 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2190964 virtual)\n",
      "I0228 01:35:11.883318 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2196119 virtual)\n",
      "I0228 01:35:11.888430 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2200822 virtual)\n",
      "I0228 01:35:11.892845 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2206100 virtual)\n",
      "I0228 01:35:11.915504 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2211507 virtual)\n",
      "I0228 01:35:11.926095 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2216582 virtual)\n",
      "I0228 01:35:11.944033 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2221868 virtual)\n",
      "I0228 01:35:11.950129 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2226804 virtual)\n",
      "I0228 01:35:11.953950 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2232309 virtual)\n",
      "I0228 01:35:11.972651 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2237148 virtual)\n",
      "I0228 01:35:11.981479 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2241841 virtual)\n",
      "I0228 01:35:12.003309 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2247228 virtual)\n",
      "I0228 01:35:12.007250 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2252251 virtual)\n",
      "I0228 01:35:12.016864 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2257305 virtual)\n",
      "I0228 01:35:12.034983 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2262428 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:12.039947 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2267455 virtual)\n",
      "I0228 01:35:12.066318 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2272541 virtual)\n",
      "I0228 01:35:12.073626 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2277475 virtual)\n",
      "I0228 01:35:12.079702 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2282439 virtual)\n",
      "I0228 01:35:12.092618 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2287400 virtual)\n",
      "I0228 01:35:12.096749 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2292484 virtual)\n",
      "I0228 01:35:12.126712 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2297564 virtual)\n",
      "I0228 01:35:12.137109 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2302324 virtual)\n",
      "I0228 01:35:12.141920 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2307620 virtual)\n",
      "I0228 01:35:12.150587 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2312443 virtual)\n",
      "I0228 01:35:12.154234 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2317599 virtual)\n",
      "I0228 01:35:12.184488 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2323002 virtual)\n",
      "I0228 01:35:12.196186 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2328277 virtual)\n",
      "I0228 01:35:12.212096 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2333170 virtual)\n",
      "I0228 01:35:12.215558 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2338480 virtual)\n",
      "I0228 01:35:12.218503 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2343463 virtual)\n",
      "I0228 01:35:12.246604 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2348537 virtual)\n",
      "I0228 01:35:12.254388 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2353992 virtual)\n",
      "I0228 01:35:12.263776 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2359377 virtual)\n",
      "I0228 01:35:12.266717 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2364419 virtual)\n",
      "I0228 01:35:12.269832 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2369235 virtual)\n",
      "I0228 01:35:12.309642 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2373954 virtual)\n",
      "I0228 01:35:12.314809 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2379004 virtual)\n",
      "I0228 01:35:12.319603 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2384242 virtual)\n",
      "I0228 01:35:12.327734 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2389122 virtual)\n",
      "I0228 01:35:12.331346 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2394295 virtual)\n",
      "I0228 01:35:12.365715 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2399746 virtual)\n",
      "I0228 01:35:12.379040 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2404645 virtual)\n",
      "I0228 01:35:12.383666 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2409735 virtual)\n",
      "I0228 01:35:12.388153 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2414838 virtual)\n",
      "I0228 01:35:12.391110 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2419774 virtual)\n",
      "I0228 01:35:12.420574 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2424517 virtual)\n",
      "I0228 01:35:12.437640 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2429800 virtual)\n",
      "I0228 01:35:12.451764 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2435096 virtual)\n",
      "I0228 01:35:12.455275 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2440379 virtual)\n",
      "I0228 01:35:12.458179 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2445447 virtual)\n",
      "I0228 01:35:12.485077 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2450471 virtual)\n",
      "I0228 01:35:12.491369 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2455468 virtual)\n",
      "I0228 01:35:12.503929 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2460638 virtual)\n",
      "I0228 01:35:12.508967 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2465474 virtual)\n",
      "I0228 01:35:12.513044 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2470420 virtual)\n",
      "I0228 01:35:12.540753 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2475676 virtual)\n",
      "I0228 01:35:12.553129 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2480976 virtual)\n",
      "I0228 01:35:12.565234 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2486374 virtual)\n",
      "I0228 01:35:12.570642 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2491505 virtual)\n",
      "I0228 01:35:12.575001 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2496444 virtual)\n",
      "I0228 01:35:12.599795 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2501585 virtual)\n",
      "I0228 01:35:12.610175 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2506108 virtual)\n",
      "I0228 01:35:12.624634 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2510963 virtual)\n",
      "I0228 01:35:12.630138 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2515934 virtual)\n",
      "I0228 01:35:12.635289 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2521607 virtual)\n",
      "I0228 01:35:12.661808 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2526716 virtual)\n",
      "I0228 01:35:12.667078 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2531915 virtual)\n",
      "I0228 01:35:12.685596 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2536629 virtual)\n",
      "I0228 01:35:12.689784 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2542218 virtual)\n",
      "I0228 01:35:12.693983 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2546961 virtual)\n",
      "I0228 01:35:12.716557 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2551589 virtual)\n",
      "I0228 01:35:12.740566 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2556712 virtual)\n",
      "I0228 01:35:12.745721 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2561875 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:12.755226 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2566839 virtual)\n",
      "I0228 01:35:12.758135 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2572067 virtual)\n",
      "I0228 01:35:12.774997 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2576930 virtual)\n",
      "I0228 01:35:12.779965 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2582320 virtual)\n",
      "I0228 01:35:12.799716 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2587676 virtual)\n",
      "I0228 01:35:12.813606 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2592438 virtual)\n",
      "I0228 01:35:12.818413 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2597486 virtual)\n",
      "I0228 01:35:12.825861 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2603047 virtual)\n",
      "I0228 01:35:12.840440 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2608365 virtual)\n",
      "I0228 01:35:12.858292 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2613586 virtual)\n",
      "I0228 01:35:12.870955 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2618994 virtual)\n",
      "I0228 01:35:12.879005 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2624145 virtual)\n",
      "I0228 01:35:12.883859 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2629734 virtual)\n",
      "I0228 01:35:12.903404 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2634657 virtual)\n",
      "I0228 01:35:12.922357 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2639564 virtual)\n",
      "I0228 01:35:12.927089 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2644511 virtual)\n",
      "I0228 01:35:12.940925 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2649701 virtual)\n",
      "I0228 01:35:12.947843 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2654382 virtual)\n",
      "I0228 01:35:12.965220 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2659410 virtual)\n",
      "I0228 01:35:12.983306 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2664308 virtual)\n",
      "I0228 01:35:12.988389 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2669300 virtual)\n",
      "I0228 01:35:13.003283 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2674614 virtual)\n",
      "I0228 01:35:13.012124 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2679748 virtual)\n",
      "I0228 01:35:13.022476 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2685317 virtual)\n",
      "I0228 01:35:13.040069 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2690438 virtual)\n",
      "I0228 01:35:13.045117 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2696001 virtual)\n",
      "I0228 01:35:13.065703 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2700857 virtual)\n",
      "I0228 01:35:13.069527 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2706306 virtual)\n",
      "I0228 01:35:13.079884 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2711545 virtual)\n",
      "I0228 01:35:13.096549 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2716401 virtual)\n",
      "I0228 01:35:13.104435 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2721522 virtual)\n",
      "I0228 01:35:13.125359 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2726487 virtual)\n",
      "I0228 01:35:13.129635 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2732016 virtual)\n",
      "I0228 01:35:13.145176 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2737544 virtual)\n",
      "I0228 01:35:13.156336 140370323834688 text_analysis.py:506] 552 batches submitted to accumulate stats from 35328 documents (2743133 virtual)\n",
      "I0228 01:35:13.168149 140370323834688 text_analysis.py:506] 553 batches submitted to accumulate stats from 35392 documents (2748305 virtual)\n",
      "I0228 01:35:13.182567 140370323834688 text_analysis.py:506] 554 batches submitted to accumulate stats from 35456 documents (2753362 virtual)\n",
      "I0228 01:35:13.187210 140370323834688 text_analysis.py:506] 555 batches submitted to accumulate stats from 35520 documents (2758452 virtual)\n",
      "I0228 01:35:13.217803 140370323834688 text_analysis.py:506] 556 batches submitted to accumulate stats from 35584 documents (2763442 virtual)\n",
      "I0228 01:35:13.222356 140370323834688 text_analysis.py:506] 557 batches submitted to accumulate stats from 35648 documents (2768440 virtual)\n",
      "I0228 01:35:13.233844 140370323834688 text_analysis.py:506] 558 batches submitted to accumulate stats from 35712 documents (2774112 virtual)\n",
      "I0228 01:35:13.239684 140370323834688 text_analysis.py:506] 559 batches submitted to accumulate stats from 35776 documents (2779090 virtual)\n",
      "I0228 01:35:13.249305 140370323834688 text_analysis.py:506] 560 batches submitted to accumulate stats from 35840 documents (2784034 virtual)\n",
      "I0228 01:35:13.270776 140370323834688 text_analysis.py:506] 561 batches submitted to accumulate stats from 35904 documents (2785993 virtual)\n",
      "I0228 01:35:13.337791 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:35:13.351953 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:35:13.356121 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:35:13.363271 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:35:13.365291 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:35:13.367519 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:35:13.369172 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:35:13.343463 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:35:13.355066 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:35:13.372442 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:35:13.790468 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:35:13.808834 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2786331 virtual documents\n",
      "I0228 01:35:13.896583 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:35:14.269696 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (4980 virtual)\n",
      "I0228 01:35:14.274123 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10307 virtual)\n",
      "I0228 01:35:14.277935 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16414 virtual)\n",
      "I0228 01:35:14.281218 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21149 virtual)\n",
      "I0228 01:35:14.284584 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27032 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:14.287493 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32205 virtual)\n",
      "I0228 01:35:14.290079 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37609 virtual)\n",
      "I0228 01:35:14.292083 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43423 virtual)\n",
      "I0228 01:35:14.297228 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48656 virtual)\n",
      "I0228 01:35:14.300293 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54346 virtual)\n",
      "I0228 01:35:14.336194 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60210 virtual)\n",
      "I0228 01:35:14.344290 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65483 virtual)\n",
      "I0228 01:35:14.356343 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70552 virtual)\n",
      "I0228 01:35:14.359931 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (75767 virtual)\n",
      "I0228 01:35:14.363598 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81407 virtual)\n",
      "I0228 01:35:14.398002 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87510 virtual)\n",
      "I0228 01:35:14.411425 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93328 virtual)\n",
      "I0228 01:35:14.420922 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (98857 virtual)\n",
      "I0228 01:35:14.425662 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104131 virtual)\n",
      "I0228 01:35:14.431081 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109598 virtual)\n",
      "I0228 01:35:14.462959 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (114991 virtual)\n",
      "I0228 01:35:14.471986 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120254 virtual)\n",
      "I0228 01:35:14.481639 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (125931 virtual)\n",
      "I0228 01:35:14.488496 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (131754 virtual)\n",
      "I0228 01:35:14.498718 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137291 virtual)\n",
      "I0228 01:35:14.532472 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (142797 virtual)\n",
      "I0228 01:35:14.538765 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148247 virtual)\n",
      "I0228 01:35:14.545449 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154364 virtual)\n",
      "I0228 01:35:14.552634 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160342 virtual)\n",
      "I0228 01:35:14.562089 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (165623 virtual)\n",
      "I0228 01:35:14.596914 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (171132 virtual)\n",
      "I0228 01:35:14.601949 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176531 virtual)\n",
      "I0228 01:35:14.611164 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (181818 virtual)\n",
      "I0228 01:35:14.620728 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (186909 virtual)\n",
      "I0228 01:35:14.626038 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (192856 virtual)\n",
      "I0228 01:35:14.661396 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (198133 virtual)\n",
      "I0228 01:35:14.666228 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (203591 virtual)\n",
      "I0228 01:35:14.682051 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (209248 virtual)\n",
      "I0228 01:35:14.685804 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (214706 virtual)\n",
      "I0228 01:35:14.689884 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220240 virtual)\n",
      "I0228 01:35:14.725514 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (225695 virtual)\n",
      "I0228 01:35:14.730377 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (233198 virtual)\n",
      "I0228 01:35:14.748585 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (238688 virtual)\n",
      "I0228 01:35:14.752928 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (244509 virtual)\n",
      "I0228 01:35:14.757036 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (250113 virtual)\n",
      "I0228 01:35:14.788411 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (255391 virtual)\n",
      "I0228 01:35:14.792323 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (260604 virtual)\n",
      "I0228 01:35:14.812146 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (266002 virtual)\n",
      "I0228 01:35:14.817774 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (271447 virtual)\n",
      "I0228 01:35:14.823682 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (276707 virtual)\n",
      "I0228 01:35:14.853716 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (282221 virtual)\n",
      "I0228 01:35:14.877321 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288259 virtual)\n",
      "I0228 01:35:14.885366 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (293154 virtual)\n",
      "I0228 01:35:14.887414 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (298752 virtual)\n",
      "I0228 01:35:14.889476 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304277 virtual)\n",
      "I0228 01:35:14.913995 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (309398 virtual)\n",
      "I0228 01:35:14.935115 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (315027 virtual)\n",
      "I0228 01:35:14.944951 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (320938 virtual)\n",
      "I0228 01:35:14.948197 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (326088 virtual)\n",
      "I0228 01:35:14.951688 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (332462 virtual)\n",
      "I0228 01:35:14.979617 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (338107 virtual)\n",
      "I0228 01:35:15.003245 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (343966 virtual)\n",
      "I0228 01:35:15.007651 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (349516 virtual)\n",
      "I0228 01:35:15.011939 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (355280 virtual)\n",
      "I0228 01:35:15.015940 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (361296 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:15.041411 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (366803 virtual)\n",
      "I0228 01:35:15.069698 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (372383 virtual)\n",
      "I0228 01:35:15.074759 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (378274 virtual)\n",
      "I0228 01:35:15.079825 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (384175 virtual)\n",
      "I0228 01:35:15.088910 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (389207 virtual)\n",
      "I0228 01:35:15.104452 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (394147 virtual)\n",
      "I0228 01:35:15.136377 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398334 virtual)\n",
      "I0228 01:35:15.141298 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (401829 virtual)\n",
      "I0228 01:35:15.146960 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (405702 virtual)\n",
      "I0228 01:35:15.159757 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (410288 virtual)\n",
      "I0228 01:35:15.168521 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (414695 virtual)\n",
      "I0228 01:35:15.201541 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (419136 virtual)\n",
      "I0228 01:35:15.208969 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (423265 virtual)\n",
      "I0228 01:35:15.213573 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (427670 virtual)\n",
      "I0228 01:35:15.217067 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (432055 virtual)\n",
      "I0228 01:35:15.220026 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (436983 virtual)\n",
      "I0228 01:35:15.251148 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (441863 virtual)\n",
      "I0228 01:35:15.255778 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (446434 virtual)\n",
      "I0228 01:35:15.259850 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (450448 virtual)\n",
      "I0228 01:35:15.267640 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (454503 virtual)\n",
      "I0228 01:35:15.270143 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (458114 virtual)\n",
      "I0228 01:35:15.298664 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (461562 virtual)\n",
      "I0228 01:35:15.302574 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (465633 virtual)\n",
      "I0228 01:35:15.308937 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (470068 virtual)\n",
      "I0228 01:35:15.321121 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (474795 virtual)\n",
      "I0228 01:35:15.325175 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (479455 virtual)\n",
      "I0228 01:35:15.351966 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (483942 virtual)\n",
      "I0228 01:35:15.356328 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (488586 virtual)\n",
      "I0228 01:35:15.361167 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (493326 virtual)\n",
      "I0228 01:35:15.365982 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (498035 virtual)\n",
      "I0228 01:35:15.368772 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (502366 virtual)\n",
      "I0228 01:35:15.389910 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (506686 virtual)\n",
      "I0228 01:35:15.402473 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (512069 virtual)\n",
      "I0228 01:35:15.408187 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (517237 virtual)\n",
      "I0228 01:35:15.415706 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (521412 virtual)\n",
      "I0228 01:35:15.422533 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (525781 virtual)\n",
      "I0228 01:35:15.441086 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (530439 virtual)\n",
      "I0228 01:35:15.459091 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (534341 virtual)\n",
      "I0228 01:35:15.463451 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (538681 virtual)\n",
      "I0228 01:35:15.470189 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (542721 virtual)\n",
      "I0228 01:35:15.473123 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (547173 virtual)\n",
      "I0228 01:35:15.489019 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (551474 virtual)\n",
      "I0228 01:35:15.520086 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (555643 virtual)\n",
      "I0228 01:35:15.524363 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (560391 virtual)\n",
      "I0228 01:35:15.528776 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (564295 virtual)\n",
      "I0228 01:35:15.532584 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (569007 virtual)\n",
      "I0228 01:35:15.541745 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (572993 virtual)\n",
      "I0228 01:35:15.565530 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (576881 virtual)\n",
      "I0228 01:35:15.570866 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (580748 virtual)\n",
      "I0228 01:35:15.574578 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (583784 virtual)\n",
      "I0228 01:35:15.584181 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (588507 virtual)\n",
      "I0228 01:35:15.587973 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (594028 virtual)\n",
      "I0228 01:35:15.611563 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (599430 virtual)\n",
      "I0228 01:35:15.615949 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (604091 virtual)\n",
      "I0228 01:35:15.626447 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (609522 virtual)\n",
      "I0228 01:35:15.632118 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (615401 virtual)\n",
      "I0228 01:35:15.639960 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (620548 virtual)\n",
      "I0228 01:35:15.654882 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (624738 virtual)\n",
      "I0228 01:35:15.661511 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (629613 virtual)\n",
      "I0228 01:35:15.665547 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (633477 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:15.686926 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (638708 virtual)\n",
      "I0228 01:35:15.706543 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (643955 virtual)\n",
      "I0228 01:35:15.716900 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (649300 virtual)\n",
      "I0228 01:35:15.722042 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (654966 virtual)\n",
      "I0228 01:35:15.727196 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (659939 virtual)\n",
      "I0228 01:35:15.754696 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (664420 virtual)\n",
      "I0228 01:35:15.766542 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (669693 virtual)\n",
      "I0228 01:35:15.771691 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (674924 virtual)\n",
      "I0228 01:35:15.776922 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (680178 virtual)\n",
      "I0228 01:35:15.779848 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (685132 virtual)\n",
      "I0228 01:35:15.814474 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (690053 virtual)\n",
      "I0228 01:35:15.827813 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (695186 virtual)\n",
      "I0228 01:35:15.833435 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (700311 virtual)\n",
      "I0228 01:35:15.838524 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (705599 virtual)\n",
      "I0228 01:35:15.848066 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (710662 virtual)\n",
      "I0228 01:35:15.862509 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (715870 virtual)\n",
      "I0228 01:35:15.890918 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (721370 virtual)\n",
      "I0228 01:35:15.899947 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (726297 virtual)\n",
      "I0228 01:35:15.904940 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (731744 virtual)\n",
      "I0228 01:35:15.910063 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (737135 virtual)\n",
      "I0228 01:35:15.919266 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (742801 virtual)\n",
      "I0228 01:35:15.949703 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (747628 virtual)\n",
      "I0228 01:35:15.960454 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (752937 virtual)\n",
      "I0228 01:35:15.964686 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (757926 virtual)\n",
      "I0228 01:35:15.969723 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (763551 virtual)\n",
      "I0228 01:35:15.975804 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (768518 virtual)\n",
      "I0228 01:35:16.009341 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (774505 virtual)\n",
      "I0228 01:35:16.016390 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (780299 virtual)\n",
      "I0228 01:35:16.020659 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (786017 virtual)\n",
      "I0228 01:35:16.035472 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (791944 virtual)\n",
      "I0228 01:35:16.039625 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (798037 virtual)\n",
      "I0228 01:35:16.060705 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (803519 virtual)\n",
      "I0228 01:35:16.074344 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (808916 virtual)\n",
      "I0228 01:35:16.079714 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (814753 virtual)\n",
      "I0228 01:35:16.095339 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (820152 virtual)\n",
      "I0228 01:35:16.101929 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (826333 virtual)\n",
      "I0228 01:35:16.131961 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (831938 virtual)\n",
      "I0228 01:35:16.142062 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (837330 virtual)\n",
      "I0228 01:35:16.146589 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (842736 virtual)\n",
      "I0228 01:35:16.162730 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (848818 virtual)\n",
      "I0228 01:35:16.169384 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (854765 virtual)\n",
      "I0228 01:35:16.195068 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (860640 virtual)\n",
      "I0228 01:35:16.203414 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (866484 virtual)\n",
      "I0228 01:35:16.214995 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (872030 virtual)\n",
      "I0228 01:35:16.224289 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (878282 virtual)\n",
      "I0228 01:35:16.241329 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (884006 virtual)\n",
      "I0228 01:35:16.259905 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (890014 virtual)\n",
      "I0228 01:35:16.268478 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (895914 virtual)\n",
      "I0228 01:35:16.286954 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (901583 virtual)\n",
      "I0228 01:35:16.293027 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (907878 virtual)\n",
      "I0228 01:35:16.307367 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (914555 virtual)\n",
      "I0228 01:35:16.327676 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (918598 virtual)\n",
      "I0228 01:35:16.334894 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (922686 virtual)\n",
      "I0228 01:35:16.350702 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (925092 virtual)\n",
      "I0228 01:35:16.360171 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (931297 virtual)\n",
      "I0228 01:35:16.370783 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (935621 virtual)\n",
      "I0228 01:35:16.393042 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (939579 virtual)\n",
      "I0228 01:35:16.401923 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (943330 virtual)\n",
      "I0228 01:35:16.416720 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (947135 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:16.427616 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (950776 virtual)\n",
      "I0228 01:35:16.438748 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (954357 virtual)\n",
      "I0228 01:35:16.447668 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (958073 virtual)\n",
      "I0228 01:35:16.451507 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (961981 virtual)\n",
      "I0228 01:35:16.454450 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (965286 virtual)\n",
      "I0228 01:35:16.486578 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (969567 virtual)\n",
      "I0228 01:35:16.490865 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (973194 virtual)\n",
      "I0228 01:35:16.494687 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (976932 virtual)\n",
      "I0228 01:35:16.497412 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (980543 virtual)\n",
      "I0228 01:35:16.500221 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (984370 virtual)\n",
      "I0228 01:35:16.524484 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (987873 virtual)\n",
      "I0228 01:35:16.530392 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (991755 virtual)\n",
      "I0228 01:35:16.535193 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (995510 virtual)\n",
      "I0228 01:35:16.539185 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (999077 virtual)\n",
      "I0228 01:35:16.541870 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1002977 virtual)\n",
      "I0228 01:35:16.570377 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1006968 virtual)\n",
      "I0228 01:35:16.574649 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1010397 virtual)\n",
      "I0228 01:35:16.579550 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1014171 virtual)\n",
      "I0228 01:35:16.583452 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1017994 virtual)\n",
      "I0228 01:35:16.586114 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1022466 virtual)\n",
      "I0228 01:35:16.608448 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1027600 virtual)\n",
      "I0228 01:35:16.612451 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1031495 virtual)\n",
      "I0228 01:35:16.616603 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1035483 virtual)\n",
      "I0228 01:35:16.619438 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1039238 virtual)\n",
      "I0228 01:35:16.622394 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1043830 virtual)\n",
      "I0228 01:35:16.647216 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1047820 virtual)\n",
      "I0228 01:35:16.651482 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1051431 virtual)\n",
      "I0228 01:35:16.656287 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1055844 virtual)\n",
      "I0228 01:35:16.659825 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1059634 virtual)\n",
      "I0228 01:35:16.669698 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1063642 virtual)\n",
      "I0228 01:35:16.694523 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1067439 virtual)\n",
      "I0228 01:35:16.698156 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1071201 virtual)\n",
      "I0228 01:35:16.702337 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1075914 virtual)\n",
      "I0228 01:35:16.706397 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1079862 virtual)\n",
      "I0228 01:35:16.720054 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1084535 virtual)\n",
      "I0228 01:35:16.737383 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1088904 virtual)\n",
      "I0228 01:35:16.742410 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1093465 virtual)\n",
      "I0228 01:35:16.746577 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1097297 virtual)\n",
      "I0228 01:35:16.750568 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1100768 virtual)\n",
      "I0228 01:35:16.761092 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1105180 virtual)\n",
      "I0228 01:35:16.778409 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1109286 virtual)\n",
      "I0228 01:35:16.782696 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1112952 virtual)\n",
      "I0228 01:35:16.788435 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1117305 virtual)\n",
      "I0228 01:35:16.791553 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1121435 virtual)\n",
      "I0228 01:35:16.813616 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1126568 virtual)\n",
      "I0228 01:35:16.823898 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1130252 virtual)\n",
      "I0228 01:35:16.828552 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1135070 virtual)\n",
      "I0228 01:35:16.833236 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1139797 virtual)\n",
      "I0228 01:35:16.836791 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1144338 virtual)\n",
      "I0228 01:35:16.857731 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1149782 virtual)\n",
      "I0228 01:35:16.862530 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1153656 virtual)\n",
      "I0228 01:35:16.867944 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1157219 virtual)\n",
      "I0228 01:35:16.877864 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1161143 virtual)\n",
      "I0228 01:35:16.881143 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1165711 virtual)\n",
      "I0228 01:35:16.901046 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1169383 virtual)\n",
      "I0228 01:35:16.915340 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1174327 virtual)\n",
      "I0228 01:35:16.920138 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1178912 virtual)\n",
      "I0228 01:35:16.929451 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1182686 virtual)\n",
      "I0228 01:35:16.932877 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1187406 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:16.958623 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1192274 virtual)\n",
      "I0228 01:35:16.962632 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1196088 virtual)\n",
      "I0228 01:35:16.967135 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1200724 virtual)\n",
      "I0228 01:35:16.970813 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1205063 virtual)\n",
      "I0228 01:35:16.981000 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1209463 virtual)\n",
      "I0228 01:35:16.998987 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1213500 virtual)\n",
      "I0228 01:35:17.008655 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1217729 virtual)\n",
      "I0228 01:35:17.012527 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1221551 virtual)\n",
      "I0228 01:35:17.016652 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1226240 virtual)\n",
      "I0228 01:35:17.028270 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1230409 virtual)\n",
      "I0228 01:35:17.047600 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1234174 virtual)\n",
      "I0228 01:35:17.052476 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1237430 virtual)\n",
      "I0228 01:35:17.056193 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1240720 virtual)\n",
      "I0228 01:35:17.060231 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1244890 virtual)\n",
      "I0228 01:35:17.069199 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1248764 virtual)\n",
      "I0228 01:35:17.086735 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1253497 virtual)\n",
      "I0228 01:35:17.093915 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1258598 virtual)\n",
      "I0228 01:35:17.099577 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1262910 virtual)\n",
      "I0228 01:35:17.104898 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1267018 virtual)\n",
      "I0228 01:35:17.115576 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1271125 virtual)\n",
      "I0228 01:35:17.123691 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1275184 virtual)\n",
      "I0228 01:35:17.127923 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1279023 virtual)\n",
      "I0228 01:35:17.131805 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1282933 virtual)\n",
      "I0228 01:35:17.146730 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1287008 virtual)\n",
      "I0228 01:35:17.150618 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1291670 virtual)\n",
      "I0228 01:35:17.176827 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1295316 virtual)\n",
      "I0228 01:35:17.182197 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1299893 virtual)\n",
      "I0228 01:35:17.186745 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1304084 virtual)\n",
      "I0228 01:35:17.189985 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1307494 virtual)\n",
      "I0228 01:35:17.192749 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1311597 virtual)\n",
      "I0228 01:35:17.217508 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1316604 virtual)\n",
      "I0228 01:35:17.221274 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1321038 virtual)\n",
      "I0228 01:35:17.225191 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1324382 virtual)\n",
      "I0228 01:35:17.229806 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1329062 virtual)\n",
      "I0228 01:35:17.243259 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1332966 virtual)\n",
      "I0228 01:35:17.257607 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1336270 virtual)\n",
      "I0228 01:35:17.261598 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1340267 virtual)\n",
      "I0228 01:35:17.265324 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1344528 virtual)\n",
      "I0228 01:35:17.268827 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1348573 virtual)\n",
      "I0228 01:35:17.287058 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1353287 virtual)\n",
      "I0228 01:35:17.300104 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1357538 virtual)\n",
      "I0228 01:35:17.306981 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1362380 virtual)\n",
      "I0228 01:35:17.311554 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1366531 virtual)\n",
      "I0228 01:35:17.316380 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1368981 virtual)\n",
      "I0228 01:35:17.325531 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1374219 virtual)\n",
      "I0228 01:35:17.340098 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1379484 virtual)\n",
      "I0228 01:35:17.348327 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1383672 virtual)\n",
      "I0228 01:35:17.356135 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1388281 virtual)\n",
      "I0228 01:35:17.372488 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1392268 virtual)\n",
      "I0228 01:35:17.376745 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1397448 virtual)\n",
      "I0228 01:35:17.382303 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1401088 virtual)\n",
      "I0228 01:35:17.401194 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1406064 virtual)\n",
      "I0228 01:35:17.405826 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1411123 virtual)\n",
      "I0228 01:35:17.409991 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1415207 virtual)\n",
      "I0228 01:35:17.434866 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1419554 virtual)\n",
      "I0228 01:35:17.440795 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1424306 virtual)\n",
      "I0228 01:35:17.446289 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1428659 virtual)\n",
      "I0228 01:35:17.449220 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1433166 virtual)\n",
      "I0228 01:35:17.453607 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1437657 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:17.478962 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1442108 virtual)\n",
      "I0228 01:35:17.493417 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1446769 virtual)\n",
      "I0228 01:35:17.498103 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1452479 virtual)\n",
      "I0228 01:35:17.502572 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1457229 virtual)\n",
      "I0228 01:35:17.507421 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1462982 virtual)\n",
      "I0228 01:35:17.527523 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1467890 virtual)\n",
      "I0228 01:35:17.544029 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1473200 virtual)\n",
      "I0228 01:35:17.549465 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1479129 virtual)\n",
      "I0228 01:35:17.554682 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1486684 virtual)\n",
      "I0228 01:35:17.559535 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1493107 virtual)\n",
      "I0228 01:35:17.575452 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1498057 virtual)\n",
      "I0228 01:35:17.593899 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1502926 virtual)\n",
      "I0228 01:35:17.599149 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1508197 virtual)\n",
      "I0228 01:35:17.610611 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1513434 virtual)\n",
      "I0228 01:35:17.617959 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1518250 virtual)\n",
      "I0228 01:35:17.627925 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1523252 virtual)\n",
      "I0228 01:35:17.649581 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1528334 virtual)\n",
      "I0228 01:35:17.656794 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1538304 virtual)\n",
      "I0228 01:35:17.680368 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1543945 virtual)\n",
      "I0228 01:35:17.684732 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1549114 virtual)\n",
      "I0228 01:35:17.692506 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1554947 virtual)\n",
      "I0228 01:35:17.699584 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1559918 virtual)\n",
      "I0228 01:35:17.718156 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1563741 virtual)\n",
      "I0228 01:35:17.736380 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1568489 virtual)\n",
      "I0228 01:35:17.741074 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1573484 virtual)\n",
      "I0228 01:35:17.748483 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1578115 virtual)\n",
      "I0228 01:35:17.752192 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1583091 virtual)\n",
      "I0228 01:35:17.794022 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1588541 virtual)\n",
      "I0228 01:35:17.798915 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1594651 virtual)\n",
      "I0228 01:35:17.805988 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1600501 virtual)\n",
      "I0228 01:35:17.811220 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1606021 virtual)\n",
      "I0228 01:35:17.821367 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1611536 virtual)\n",
      "I0228 01:35:17.837147 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1617560 virtual)\n",
      "I0228 01:35:17.850848 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1623313 virtual)\n",
      "I0228 01:35:17.860495 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1627670 virtual)\n",
      "I0228 01:35:17.866322 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1633314 virtual)\n",
      "I0228 01:35:17.869967 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1638169 virtual)\n",
      "I0228 01:35:17.894715 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1643540 virtual)\n",
      "I0228 01:35:17.920060 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1648719 virtual)\n",
      "I0228 01:35:17.926416 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1654273 virtual)\n",
      "I0228 01:35:17.931781 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1659341 virtual)\n",
      "I0228 01:35:17.934728 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1665105 virtual)\n",
      "I0228 01:35:17.958965 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1669953 virtual)\n",
      "I0228 01:35:17.973611 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1676306 virtual)\n",
      "I0228 01:35:17.981297 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1684416 virtual)\n",
      "I0228 01:35:17.988203 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1693039 virtual)\n",
      "I0228 01:35:17.992013 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1701205 virtual)\n",
      "I0228 01:35:18.015896 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1709194 virtual)\n",
      "I0228 01:35:18.027804 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1715132 virtual)\n",
      "I0228 01:35:18.040948 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1719916 virtual)\n",
      "I0228 01:35:18.047257 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1726530 virtual)\n",
      "I0228 01:35:18.054435 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1732045 virtual)\n",
      "I0228 01:35:18.069494 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1737964 virtual)\n",
      "I0228 01:35:18.101935 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1744048 virtual)\n",
      "I0228 01:35:18.135163 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1749666 virtual)\n",
      "I0228 01:35:18.146996 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1754763 virtual)\n",
      "I0228 01:35:18.152104 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1760421 virtual)\n",
      "I0228 01:35:18.163730 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1765513 virtual)\n",
      "I0228 01:35:18.175531 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1770604 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:18.189391 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1775586 virtual)\n",
      "I0228 01:35:18.207414 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1780666 virtual)\n",
      "I0228 01:35:18.221145 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1786068 virtual)\n",
      "I0228 01:35:18.227627 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1791515 virtual)\n",
      "I0228 01:35:18.251316 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1796181 virtual)\n",
      "I0228 01:35:18.255407 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1801024 virtual)\n",
      "I0228 01:35:18.262478 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1805906 virtual)\n",
      "I0228 01:35:18.287672 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1811201 virtual)\n",
      "I0228 01:35:18.293461 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1816185 virtual)\n",
      "I0228 01:35:18.302516 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1821407 virtual)\n",
      "I0228 01:35:18.311852 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1826439 virtual)\n",
      "I0228 01:35:18.324828 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1831657 virtual)\n",
      "I0228 01:35:18.350521 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1836872 virtual)\n",
      "I0228 01:35:18.355419 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1841774 virtual)\n",
      "I0228 01:35:18.360007 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1846634 virtual)\n",
      "I0228 01:35:18.370129 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1851967 virtual)\n",
      "I0228 01:35:18.384411 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1857040 virtual)\n",
      "I0228 01:35:18.408586 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1862619 virtual)\n",
      "I0228 01:35:18.413964 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1867466 virtual)\n",
      "I0228 01:35:18.417890 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1872318 virtual)\n",
      "I0228 01:35:18.427104 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1876922 virtual)\n",
      "I0228 01:35:18.443083 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1882268 virtual)\n",
      "I0228 01:35:18.468319 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1887279 virtual)\n",
      "I0228 01:35:18.480318 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1892244 virtual)\n",
      "I0228 01:35:18.484611 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1897046 virtual)\n",
      "I0228 01:35:18.489644 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1902313 virtual)\n",
      "I0228 01:35:18.503229 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1907884 virtual)\n",
      "I0228 01:35:18.528449 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1913017 virtual)\n",
      "I0228 01:35:18.533689 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1917759 virtual)\n",
      "I0228 01:35:18.538615 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1923199 virtual)\n",
      "I0228 01:35:18.541981 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1928433 virtual)\n",
      "I0228 01:35:18.564048 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1933233 virtual)\n",
      "I0228 01:35:18.587725 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1937712 virtual)\n",
      "I0228 01:35:18.592601 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1942602 virtual)\n",
      "I0228 01:35:18.596794 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1947772 virtual)\n",
      "I0228 01:35:18.604083 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1952922 virtual)\n",
      "I0228 01:35:18.628788 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1958010 virtual)\n",
      "I0228 01:35:18.643704 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1962868 virtual)\n",
      "I0228 01:35:18.650407 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1967665 virtual)\n",
      "I0228 01:35:18.656039 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1973140 virtual)\n",
      "I0228 01:35:18.664054 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (1978281 virtual)\n",
      "I0228 01:35:18.687208 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (1983431 virtual)\n",
      "I0228 01:35:18.701488 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (1988126 virtual)\n",
      "I0228 01:35:18.708476 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (1993234 virtual)\n",
      "I0228 01:35:18.716651 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (1998822 virtual)\n",
      "I0228 01:35:18.723880 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (2003769 virtual)\n",
      "I0228 01:35:18.747064 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2009323 virtual)\n",
      "I0228 01:35:18.751113 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2014496 virtual)\n",
      "I0228 01:35:18.763269 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2019894 virtual)\n",
      "I0228 01:35:18.781289 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2024841 virtual)\n",
      "I0228 01:35:18.784109 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2030082 virtual)\n",
      "I0228 01:35:18.807489 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2035259 virtual)\n",
      "I0228 01:35:18.809486 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2040350 virtual)\n",
      "I0228 01:35:18.825303 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2045120 virtual)\n",
      "I0228 01:35:18.840129 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2049995 virtual)\n",
      "I0228 01:35:18.847606 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2054841 virtual)\n",
      "I0228 01:35:18.870990 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2059537 virtual)\n",
      "I0228 01:35:18.887251 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2064463 virtual)\n",
      "I0228 01:35:18.895226 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2069148 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:18.900893 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2073976 virtual)\n",
      "I0228 01:35:18.909005 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2079340 virtual)\n",
      "I0228 01:35:18.933618 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2084278 virtual)\n",
      "I0228 01:35:18.947632 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2089762 virtual)\n",
      "I0228 01:35:18.957237 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2094816 virtual)\n",
      "I0228 01:35:18.961160 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2100033 virtual)\n",
      "I0228 01:35:18.966529 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2105335 virtual)\n",
      "I0228 01:35:18.992658 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2110633 virtual)\n",
      "I0228 01:35:19.006631 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2115939 virtual)\n",
      "I0228 01:35:19.014173 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2120516 virtual)\n",
      "I0228 01:35:19.018360 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2125601 virtual)\n",
      "I0228 01:35:19.027094 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2130382 virtual)\n",
      "I0228 01:35:19.050786 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2135127 virtual)\n",
      "I0228 01:35:19.067641 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2140162 virtual)\n",
      "I0228 01:35:19.078017 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2145103 virtual)\n",
      "I0228 01:35:19.081650 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2150302 virtual)\n",
      "I0228 01:35:19.086299 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2155584 virtual)\n",
      "I0228 01:35:19.113135 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2160822 virtual)\n",
      "I0228 01:35:19.128987 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2166008 virtual)\n",
      "I0228 01:35:19.134023 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2171235 virtual)\n",
      "I0228 01:35:19.138251 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2176085 virtual)\n",
      "I0228 01:35:19.147361 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2181259 virtual)\n",
      "I0228 01:35:19.171231 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2186102 virtual)\n",
      "I0228 01:35:19.189209 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2190964 virtual)\n",
      "I0228 01:35:19.199348 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2196119 virtual)\n",
      "I0228 01:35:19.203531 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2200822 virtual)\n",
      "I0228 01:35:19.210961 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2206100 virtual)\n",
      "I0228 01:35:19.231195 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2211507 virtual)\n",
      "I0228 01:35:19.254706 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2216582 virtual)\n",
      "I0228 01:35:19.260233 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2221868 virtual)\n",
      "I0228 01:35:19.263779 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2226804 virtual)\n",
      "I0228 01:35:19.272686 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2232309 virtual)\n",
      "I0228 01:35:19.291635 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2237148 virtual)\n",
      "I0228 01:35:19.307282 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2241841 virtual)\n",
      "I0228 01:35:19.315031 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2247228 virtual)\n",
      "I0228 01:35:19.321710 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2252251 virtual)\n",
      "I0228 01:35:19.336572 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2257305 virtual)\n",
      "I0228 01:35:19.354586 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2262428 virtual)\n",
      "I0228 01:35:19.364390 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2267455 virtual)\n",
      "I0228 01:35:19.378349 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2272541 virtual)\n",
      "I0228 01:35:19.384452 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2277475 virtual)\n",
      "I0228 01:35:19.399634 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2282439 virtual)\n",
      "I0228 01:35:19.412485 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2287400 virtual)\n",
      "I0228 01:35:19.422011 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2292484 virtual)\n",
      "I0228 01:35:19.439085 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2297564 virtual)\n",
      "I0228 01:35:19.443786 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2302324 virtual)\n",
      "I0228 01:35:19.457357 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2307620 virtual)\n",
      "I0228 01:35:19.470297 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2312443 virtual)\n",
      "I0228 01:35:19.484999 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2317599 virtual)\n",
      "I0228 01:35:19.497228 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2323002 virtual)\n",
      "I0228 01:35:19.503656 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2328277 virtual)\n",
      "I0228 01:35:19.513452 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2333170 virtual)\n",
      "I0228 01:35:19.528056 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2338480 virtual)\n",
      "I0228 01:35:19.538033 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2343463 virtual)\n",
      "I0228 01:35:19.559138 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2348537 virtual)\n",
      "I0228 01:35:19.563363 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2353992 virtual)\n",
      "I0228 01:35:19.574352 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2359377 virtual)\n",
      "I0228 01:35:19.583507 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2364419 virtual)\n",
      "I0228 01:35:19.598335 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2369235 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:19.622038 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2373954 virtual)\n",
      "I0228 01:35:19.626134 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2379004 virtual)\n",
      "I0228 01:35:19.630264 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2384242 virtual)\n",
      "I0228 01:35:19.646430 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2389122 virtual)\n",
      "I0228 01:35:19.658113 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2394295 virtual)\n",
      "I0228 01:35:19.678280 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2399746 virtual)\n",
      "I0228 01:35:19.692494 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2404645 virtual)\n",
      "I0228 01:35:19.696287 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2409735 virtual)\n",
      "I0228 01:35:19.704435 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2414838 virtual)\n",
      "I0228 01:35:19.714053 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2419774 virtual)\n",
      "I0228 01:35:19.733061 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2424517 virtual)\n",
      "I0228 01:35:19.752401 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2429800 virtual)\n",
      "I0228 01:35:19.756879 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2435096 virtual)\n",
      "I0228 01:35:19.763064 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2440379 virtual)\n",
      "I0228 01:35:19.776402 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2445447 virtual)\n",
      "I0228 01:35:19.797806 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2450471 virtual)\n",
      "I0228 01:35:19.807370 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2455468 virtual)\n",
      "I0228 01:35:19.817223 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2460638 virtual)\n",
      "I0228 01:35:19.825281 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2465474 virtual)\n",
      "I0228 01:35:19.833288 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2470420 virtual)\n",
      "I0228 01:35:19.853518 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2475676 virtual)\n",
      "I0228 01:35:19.870827 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2480976 virtual)\n",
      "I0228 01:35:19.878801 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2486374 virtual)\n",
      "I0228 01:35:19.887690 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2491505 virtual)\n",
      "I0228 01:35:19.892722 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2496444 virtual)\n",
      "I0228 01:35:19.919493 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2501585 virtual)\n",
      "I0228 01:35:19.932871 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2506108 virtual)\n",
      "I0228 01:35:19.938377 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2510963 virtual)\n",
      "I0228 01:35:19.943431 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2515934 virtual)\n",
      "I0228 01:35:19.951160 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2521607 virtual)\n",
      "I0228 01:35:19.984189 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2526716 virtual)\n",
      "I0228 01:35:19.988582 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2531915 virtual)\n",
      "I0228 01:35:20.000142 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2536629 virtual)\n",
      "I0228 01:35:20.005744 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2542218 virtual)\n",
      "I0228 01:35:20.010401 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2546961 virtual)\n",
      "I0228 01:35:20.039632 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2551589 virtual)\n",
      "I0228 01:35:20.044068 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2556712 virtual)\n",
      "I0228 01:35:20.058505 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2561875 virtual)\n",
      "I0228 01:35:20.063585 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2566839 virtual)\n",
      "I0228 01:35:20.079579 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2572067 virtual)\n",
      "I0228 01:35:20.099347 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2576930 virtual)\n",
      "I0228 01:35:20.103544 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2582320 virtual)\n",
      "I0228 01:35:20.113360 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2587676 virtual)\n",
      "I0228 01:35:20.128708 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2592438 virtual)\n",
      "I0228 01:35:20.134089 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2597486 virtual)\n",
      "I0228 01:35:20.151595 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2603047 virtual)\n",
      "I0228 01:35:20.162982 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2608365 virtual)\n",
      "I0228 01:35:20.175706 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2613586 virtual)\n",
      "I0228 01:35:20.185914 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2618994 virtual)\n",
      "I0228 01:35:20.197554 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2624145 virtual)\n",
      "I0228 01:35:20.210264 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2629734 virtual)\n",
      "I0228 01:35:20.225557 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2634657 virtual)\n",
      "I0228 01:35:20.235316 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2639564 virtual)\n",
      "I0228 01:35:20.240784 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2644511 virtual)\n",
      "I0228 01:35:20.262102 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2649701 virtual)\n",
      "I0228 01:35:20.278776 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2654382 virtual)\n",
      "I0228 01:35:20.287759 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2659410 virtual)\n",
      "I0228 01:35:20.296304 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2664308 virtual)\n",
      "I0228 01:35:20.304606 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2669300 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:35:20.323760 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2674614 virtual)\n",
      "I0228 01:35:20.343813 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2679748 virtual)\n",
      "I0228 01:35:20.348940 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2685317 virtual)\n",
      "I0228 01:35:20.353977 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2690438 virtual)\n",
      "I0228 01:35:20.360716 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2696001 virtual)\n",
      "I0228 01:35:20.394859 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2700857 virtual)\n",
      "I0228 01:35:20.400571 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2706306 virtual)\n",
      "I0228 01:35:20.403590 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2711545 virtual)\n",
      "I0228 01:35:20.419650 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2716401 virtual)\n",
      "I0228 01:35:20.430921 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2721522 virtual)\n",
      "I0228 01:35:20.446469 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2726487 virtual)\n",
      "I0228 01:35:20.462677 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2732016 virtual)\n",
      "I0228 01:35:20.468266 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2737544 virtual)\n",
      "I0228 01:35:20.480422 140370323834688 text_analysis.py:506] 552 batches submitted to accumulate stats from 35328 documents (2743133 virtual)\n",
      "I0228 01:35:20.493216 140370323834688 text_analysis.py:506] 553 batches submitted to accumulate stats from 35392 documents (2748305 virtual)\n",
      "I0228 01:35:20.504272 140370323834688 text_analysis.py:506] 554 batches submitted to accumulate stats from 35456 documents (2753362 virtual)\n",
      "I0228 01:35:20.524741 140370323834688 text_analysis.py:506] 555 batches submitted to accumulate stats from 35520 documents (2758452 virtual)\n",
      "I0228 01:35:20.530276 140370323834688 text_analysis.py:506] 556 batches submitted to accumulate stats from 35584 documents (2763442 virtual)\n",
      "I0228 01:35:20.541803 140370323834688 text_analysis.py:506] 557 batches submitted to accumulate stats from 35648 documents (2768440 virtual)\n",
      "I0228 01:35:20.552629 140370323834688 text_analysis.py:506] 558 batches submitted to accumulate stats from 35712 documents (2774112 virtual)\n",
      "I0228 01:35:20.561404 140370323834688 text_analysis.py:506] 559 batches submitted to accumulate stats from 35776 documents (2779090 virtual)\n",
      "I0228 01:35:20.588111 140370323834688 text_analysis.py:506] 560 batches submitted to accumulate stats from 35840 documents (2784034 virtual)\n",
      "I0228 01:35:20.592654 140370323834688 text_analysis.py:506] 561 batches submitted to accumulate stats from 35904 documents (2785993 virtual)\n",
      "I0228 01:35:20.662137 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:35:20.673880 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:35:20.678110 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:35:20.679665 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:35:20.710292 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:35:20.665208 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:35:20.682010 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:35:20.677708 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:35:20.717213 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:35:20.686742 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:35:21.108043 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:35:21.132752 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2786331 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores\n",
      "{'epoch': 99, 'cv': 0.68469537247964, 'umass': -3.9175606751005545, 'uci': -0.03296035184366751, 'npmi': 0.07209367144151291, 'rbo': 1.0, 'td': 0.996, 'train_loss': 641.9431243150289, 'topics': [['c0267454', 'salivary', 'ref', 'c0605290', 'heparin', 'c0536858', 'c0521990', 'c1446219', 'c0020933', 'c0026231', 'c0851891', 'c0949907', 'c0032821', 'c0751651', 'excretion', 'cessation', 'c0444584', 'c0245109', 'cow', 'c1443650', 'c0117438', 'horse', 'somatic', 'c0205112', 'shareholder', 'thirty-three'], ['c0011900', 'c0012634', 'child', 'c0015967', 'c0032285', 'clinical', 'common', 'c1457887', 'c0546788', 'c0003232', 'presentation', 'cause', 'c0809949', 'c0010076', 'manifestation', 'c0221423', 'c0019993', 'c3714514', 'sars-cov-2', 'c0035236', 'c0019994', 'disclosure', 'c1446409', 'c0039082', 'c0035648', 'confirm'], ['c0543467', 'c0025080', 'c0031150', 'postoperative', 'undergo', 'perform', 'c0002940', 'c0005898', 'c0728940', 'c0038930', 'c0009566', 'operative', 'procedure', 'c0229962', 'c0850292', 'recurrence', 'c0582175', 'technique', 'c1522577', 'aneurysm', 'consecutive', 'c0019080', 'surgical', 'c0547070', 'complication', 'c4039858'], ['compare', 'difference', 'c0243095', 'significant', 'c0199470', 'significantly', 'receive', 'associate', 'c0032042', 'decrease', 'measurement', 'concentration', 'measure', 'c0034108', 'assess', 'c0021708', 'c0008976', 'primary', 'odds', 'confidence', 'c0430022', 'association', 'versus', 'determine', 'correlation', 'c0235195'], ['crisis', 'policy', 'economic', 'political', 'threat', 'disaster', 'market', 'c1561598', 'public', 'challenge', 'international', 'face', 'national', 'supply', 'trade', 'economy', 'emergency', 'sector', 'argue', 'c0018104', 'financial', 'food', 'governance', 'management', 'security', 'society'], ['c0042210', 'c1167622', 'c1514562', 'c1254351', 'c0030956', 'c0029224', 'c0014442', 'c0003320', 'c0003316', 'potential', 'c0678594', 'c0020971', 'c0042736', 'bind', 'c0003250', 'active', 'c0596901', 'c1706082', 'candidate', 'novel', 'affinity', 'c0003241', 'c0034861', 'interaction', 'potent', 'drug'], ['c1171362', 'role', 'c0007634', 'c0025929', 'c0007613', 'c0079189', 'activation', 'mechanism', 'induce', 'c0017262', 'c0021747', 'c3539881', 'c0024432', 'c0021368', 'c0162638', 'pathway', 'activate', 'c0023810', 'c0039194', 'c0041904', 'mouse', 'induction', 'c1456820', 'c3714787', 'c0035696', 'c0014597'], ['c3161035', 'propose', 'c0002045', 'c0025663', 'c0150098', 'performance', 'automate', 'c0679083', 'accuracy', 'machine', 'prediction', 'compute', 'image', 'c0037585', 'input', 'solve', 'c1704254', 'base', 'solution', 'learn', 'representation', 'apply', 'c0037589', 'computational', 'algorithm', 'application'], ['c0679646', 'search', 'conduct', 'c2603343', 'train', 'report', 'c1257890', 'c0027361', 'include', 'evidence', 'c0086388', 'c0038951', 'c0025353', 'c0242356', 'impact', 'c0242481', 'c0018724', 'c0184661', 'c0030971', 'c0003467', 'need', 'c0282574', 'c0282122', 'recommendation', 'measure', 'psychological'], ['c1705920', 'c0042776', 'c0032098', 'sample', 'c0003062', 'c0684063', 'c0017446', 'c0005595', 'c0017428', 'genetic', 'c0442726', 'c0017337', 'c0039005', 'c0086418', 'c0029347', 'c0012984', 'c1764827', 'c0007452', 'host', 'diversity', 'genotype', 'population', 'c0242781', 'pathogen', 'c0015733', 'isolate']]}\n",
      "Epoch: [101/250]\tSamples: [3709326/9181500]\tTrain Loss: 642.0989710967707\tTime: 0:00:04.432353\n",
      "Epoch: [102/250]\tSamples: [3746052/9181500]\tTrain Loss: 642.1818384582789\tTime: 0:00:04.578965\n",
      "Epoch: [103/250]\tSamples: [3782778/9181500]\tTrain Loss: 642.1102915211635\tTime: 0:00:04.528299\n",
      "Epoch: [104/250]\tSamples: [3819504/9181500]\tTrain Loss: 641.8322937513615\tTime: 0:00:04.509270\n",
      "Epoch: [105/250]\tSamples: [3856230/9181500]\tTrain Loss: 641.8775034418736\tTime: 0:00:04.768023\n",
      "Epoch: [106/250]\tSamples: [3892956/9181500]\tTrain Loss: 642.0166025729388\tTime: 0:00:04.768105\n",
      "Epoch: [107/250]\tSamples: [3929682/9181500]\tTrain Loss: 642.0987343350079\tTime: 0:00:04.834788\n",
      "Epoch: [108/250]\tSamples: [3966408/9181500]\tTrain Loss: 641.90337354667\tTime: 0:00:04.761801\n",
      "Epoch: [109/250]\tSamples: [4003134/9181500]\tTrain Loss: 642.0048312801217\tTime: 0:00:04.750964\n",
      "Epoch: [110/250]\tSamples: [4039860/9181500]\tTrain Loss: 642.113656707782\tTime: 0:00:04.799816\n",
      "Epoch: [111/250]\tSamples: [4076586/9181500]\tTrain Loss: 641.8880458283982\tTime: 0:00:04.751527\n",
      "Epoch: [112/250]\tSamples: [4113312/9181500]\tTrain Loss: 641.7855489384224\tTime: 0:00:04.714325\n",
      "Epoch: [113/250]\tSamples: [4150038/9181500]\tTrain Loss: 642.1122866591447\tTime: 0:00:04.761327\n",
      "Epoch: [114/250]\tSamples: [4186764/9181500]\tTrain Loss: 641.7748400954024\tTime: 0:00:04.771733\n",
      "Epoch: [115/250]\tSamples: [4223490/9181500]\tTrain Loss: 642.1926621722349\tTime: 0:00:04.778733\n",
      "Epoch: [116/250]\tSamples: [4260216/9181500]\tTrain Loss: 641.9153724541197\tTime: 0:00:04.736693\n",
      "Epoch: [117/250]\tSamples: [4296942/9181500]\tTrain Loss: 641.9519629953916\tTime: 0:00:04.778010\n",
      "Epoch: [118/250]\tSamples: [4333668/9181500]\tTrain Loss: 642.1389588396163\tTime: 0:00:04.764285\n",
      "Epoch: [119/250]\tSamples: [4370394/9181500]\tTrain Loss: 641.7874970644094\tTime: 0:00:04.767743\n",
      "Epoch: [120/250]\tSamples: [4407120/9181500]\tTrain Loss: 641.955261173964\tTime: 0:00:04.808444\n",
      "Epoch: [121/250]\tSamples: [4443846/9181500]\tTrain Loss: 641.8409004562503\tTime: 0:00:04.786917\n",
      "Epoch: [122/250]\tSamples: [4480572/9181500]\tTrain Loss: 642.0200187069447\tTime: 0:00:04.732718\n",
      "Epoch: [123/250]\tSamples: [4517298/9181500]\tTrain Loss: 642.0597305468197\tTime: 0:00:04.743168\n",
      "Epoch: [124/250]\tSamples: [4554024/9181500]\tTrain Loss: 641.9441518780973\tTime: 0:00:04.745944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:19.406069 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [125/250]\tSamples: [4590750/9181500]\tTrain Loss: 642.0540743231975\tTime: 0:00:04.772433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:20.229146 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:37:20.909868 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:37:21.729547 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:37:22.268353 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:37:22.273981 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:37:23.020609 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:37:23.695091 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:37:24.495706 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:37:25.010767 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:37:25.015114 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:37:25.789508 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:37:26.462948 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:37:27.259604 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:37:27.772944 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:37:27.779480 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:37:28.524628 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:37:29.194867 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:37:29.983895 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:37:30.504852 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:37:30.526529 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:37:31.076308 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (-35296 virtual)\n",
      "I0228 01:37:31.491716 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (-209150 virtual)\n",
      "I0228 01:37:31.818707 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (-496628 virtual)\n",
      "I0228 01:37:31.822319 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (-496196 virtual)\n",
      "I0228 01:37:31.853432 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (-501778 virtual)\n",
      "I0228 01:37:31.952202 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (-529412 virtual)\n",
      "I0228 01:37:31.960654 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (-527624 virtual)\n",
      "I0228 01:37:31.964772 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (-525320 virtual)\n",
      "I0228 01:37:31.968756 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (-523770 virtual)\n",
      "I0228 01:37:31.972239 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (-522098 virtual)\n",
      "I0228 01:37:32.701467 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:37:32.703177 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:37:32.704811 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:37:32.704899 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:37:32.706099 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:37:32.708332 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:37:32.714501 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:37:32.710976 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:37:32.707587 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:37:32.707290 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:37:33.139481 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:37:33.159670 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 229646 virtual documents\n",
      "I0228 01:37:35.040217 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 1000 documents\n",
      "I0228 01:37:35.055844 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 2000 documents\n",
      "I0228 01:37:35.071690 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 3000 documents\n",
      "I0228 01:37:35.092329 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 4000 documents\n",
      "I0228 01:37:35.108285 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 5000 documents\n",
      "I0228 01:37:35.123394 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 6000 documents\n",
      "I0228 01:37:35.137769 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 7000 documents\n",
      "I0228 01:37:35.152436 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 8000 documents\n",
      "I0228 01:37:35.168554 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 9000 documents\n",
      "I0228 01:37:35.186009 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 10000 documents\n",
      "I0228 01:37:35.204638 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 11000 documents\n",
      "I0228 01:37:35.220359 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 12000 documents\n",
      "I0228 01:37:35.239469 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 13000 documents\n",
      "I0228 01:37:35.253558 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 14000 documents\n",
      "I0228 01:37:35.267446 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 15000 documents\n",
      "I0228 01:37:35.290213 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 16000 documents\n",
      "I0228 01:37:35.303120 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 17000 documents\n",
      "I0228 01:37:35.316877 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 18000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:35.330198 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 19000 documents\n",
      "I0228 01:37:35.345365 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 20000 documents\n",
      "I0228 01:37:35.362001 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 21000 documents\n",
      "I0228 01:37:35.379122 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 22000 documents\n",
      "I0228 01:37:35.397468 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 23000 documents\n",
      "I0228 01:37:35.415516 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 24000 documents\n",
      "I0228 01:37:35.432174 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 25000 documents\n",
      "I0228 01:37:35.448111 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 26000 documents\n",
      "I0228 01:37:35.463620 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 27000 documents\n",
      "I0228 01:37:35.479051 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 28000 documents\n",
      "I0228 01:37:35.494264 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 29000 documents\n",
      "I0228 01:37:35.509254 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 30000 documents\n",
      "I0228 01:37:35.524086 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 31000 documents\n",
      "I0228 01:37:35.538858 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 32000 documents\n",
      "I0228 01:37:35.553589 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 33000 documents\n",
      "I0228 01:37:35.569451 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 34000 documents\n",
      "I0228 01:37:35.584739 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 35000 documents\n",
      "I0228 01:37:35.599865 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 36000 documents\n",
      "I0228 01:37:35.754266 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:37:36.098826 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (4980 virtual)\n",
      "I0228 01:37:36.103698 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10307 virtual)\n",
      "I0228 01:37:36.106482 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16414 virtual)\n",
      "I0228 01:37:36.110083 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21215 virtual)\n",
      "I0228 01:37:36.114541 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27030 virtual)\n",
      "I0228 01:37:36.116828 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32248 virtual)\n",
      "I0228 01:37:36.119067 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37815 virtual)\n",
      "I0228 01:37:36.121617 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43542 virtual)\n",
      "I0228 01:37:36.124322 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48771 virtual)\n",
      "I0228 01:37:36.126462 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54492 virtual)\n",
      "I0228 01:37:36.169687 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60355 virtual)\n",
      "I0228 01:37:36.176174 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65625 virtual)\n",
      "I0228 01:37:36.178879 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70729 virtual)\n",
      "I0228 01:37:36.183489 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (76010 virtual)\n",
      "I0228 01:37:36.199029 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81592 virtual)\n",
      "I0228 01:37:36.229843 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87709 virtual)\n",
      "I0228 01:37:36.237964 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93613 virtual)\n",
      "I0228 01:37:36.243213 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (99143 virtual)\n",
      "I0228 01:37:36.245310 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104268 virtual)\n",
      "I0228 01:37:36.266170 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109705 virtual)\n",
      "I0228 01:37:36.294121 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (115221 virtual)\n",
      "I0228 01:37:36.299520 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120476 virtual)\n",
      "I0228 01:37:36.313209 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (126088 virtual)\n",
      "I0228 01:37:36.324888 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (131919 virtual)\n",
      "I0228 01:37:36.330547 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137604 virtual)\n",
      "I0228 01:37:36.360291 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (142973 virtual)\n",
      "I0228 01:37:36.364952 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148447 virtual)\n",
      "I0228 01:37:36.370508 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154604 virtual)\n",
      "I0228 01:37:36.382730 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160530 virtual)\n",
      "I0228 01:37:36.392058 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (165880 virtual)\n",
      "I0228 01:37:36.416292 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (171258 virtual)\n",
      "I0228 01:37:36.421864 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176822 virtual)\n",
      "I0228 01:37:36.432507 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (182091 virtual)\n",
      "I0228 01:37:36.445945 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (187219 virtual)\n",
      "I0228 01:37:36.454091 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (193064 virtual)\n",
      "I0228 01:37:36.475450 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (198292 virtual)\n",
      "I0228 01:37:36.483422 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (203832 virtual)\n",
      "I0228 01:37:36.501426 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (209515 virtual)\n",
      "I0228 01:37:36.510412 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (214914 virtual)\n",
      "I0228 01:37:36.514724 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220485 virtual)\n",
      "I0228 01:37:36.533291 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (226058 virtual)\n",
      "I0228 01:37:36.545413 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (233504 virtual)\n",
      "I0228 01:37:36.559859 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (238870 virtual)\n",
      "I0228 01:37:36.569829 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (244845 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:36.575023 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (250318 virtual)\n",
      "I0228 01:37:36.591480 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (255802 virtual)\n",
      "I0228 01:37:36.607815 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (260970 virtual)\n",
      "I0228 01:37:36.626166 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (266221 virtual)\n",
      "I0228 01:37:36.630156 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (271704 virtual)\n",
      "I0228 01:37:36.634893 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (277096 virtual)\n",
      "I0228 01:37:36.653397 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (282511 virtual)\n",
      "I0228 01:37:36.678292 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288459 virtual)\n",
      "I0228 01:37:36.687659 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (293456 virtual)\n",
      "I0228 01:37:36.691716 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (298968 virtual)\n",
      "I0228 01:37:36.695950 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304432 virtual)\n",
      "I0228 01:37:36.711782 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (309677 virtual)\n",
      "I0228 01:37:36.738787 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (315390 virtual)\n",
      "I0228 01:37:36.746118 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (321169 virtual)\n",
      "I0228 01:37:36.753769 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (326295 virtual)\n",
      "I0228 01:37:36.758207 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (332652 virtual)\n",
      "I0228 01:37:36.771552 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (338508 virtual)\n",
      "I0228 01:37:36.803218 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (344100 virtual)\n",
      "I0228 01:37:36.808038 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (349693 virtual)\n",
      "I0228 01:37:36.813927 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (355577 virtual)\n",
      "I0228 01:37:36.817033 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (361631 virtual)\n",
      "I0228 01:37:36.830556 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (367136 virtual)\n",
      "I0228 01:37:36.865597 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (372820 virtual)\n",
      "I0228 01:37:36.870330 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (378654 virtual)\n",
      "I0228 01:37:36.875943 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (384563 virtual)\n",
      "I0228 01:37:36.886094 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (389449 virtual)\n",
      "I0228 01:37:36.893374 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (394800 virtual)\n",
      "I0228 01:37:36.927287 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398563 virtual)\n",
      "I0228 01:37:36.932322 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (401968 virtual)\n",
      "I0228 01:37:36.938210 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (406250 virtual)\n",
      "I0228 01:37:36.950200 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (410476 virtual)\n",
      "I0228 01:37:36.953437 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (414872 virtual)\n",
      "I0228 01:37:36.992267 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (419407 virtual)\n",
      "I0228 01:37:36.996817 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (423734 virtual)\n",
      "I0228 01:37:37.001188 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (428051 virtual)\n",
      "I0228 01:37:37.005400 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (432495 virtual)\n",
      "I0228 01:37:37.009752 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (437405 virtual)\n",
      "I0228 01:37:37.036055 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (442278 virtual)\n",
      "I0228 01:37:37.040133 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (446920 virtual)\n",
      "I0228 01:37:37.047775 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (451045 virtual)\n",
      "I0228 01:37:37.050486 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (454872 virtual)\n",
      "I0228 01:37:37.053299 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (458842 virtual)\n",
      "I0228 01:37:37.082701 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (462017 virtual)\n",
      "I0228 01:37:37.087009 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (466599 virtual)\n",
      "I0228 01:37:37.095389 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (471054 virtual)\n",
      "I0228 01:37:37.098386 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (475668 virtual)\n",
      "I0228 01:37:37.101609 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (480260 virtual)\n",
      "I0228 01:37:37.133796 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (484900 virtual)\n",
      "I0228 01:37:37.138218 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (489516 virtual)\n",
      "I0228 01:37:37.142600 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (494310 virtual)\n",
      "I0228 01:37:37.145762 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (498962 virtual)\n",
      "I0228 01:37:37.148599 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (503293 virtual)\n",
      "I0228 01:37:37.167022 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (508183 virtual)\n",
      "I0228 01:37:37.183505 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (513020 virtual)\n",
      "I0228 01:37:37.186664 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (517949 virtual)\n",
      "I0228 01:37:37.197177 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (522232 virtual)\n",
      "I0228 01:37:37.200444 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (526844 virtual)\n",
      "I0228 01:37:37.218305 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (531040 virtual)\n",
      "I0228 01:37:37.235627 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (535233 virtual)\n",
      "I0228 01:37:37.238842 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (539113 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:37.242237 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (543724 virtual)\n",
      "I0228 01:37:37.245019 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (547971 virtual)\n",
      "I0228 01:37:37.272017 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (552427 virtual)\n",
      "I0228 01:37:37.288953 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (556930 virtual)\n",
      "I0228 01:37:37.292023 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (561261 virtual)\n",
      "I0228 01:37:37.295179 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (565560 virtual)\n",
      "I0228 01:37:37.298320 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (569767 virtual)\n",
      "I0228 01:37:37.319966 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (574295 virtual)\n",
      "I0228 01:37:37.331587 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (577803 virtual)\n",
      "I0228 01:37:37.335942 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (580958 virtual)\n",
      "I0228 01:37:37.340498 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (584575 virtual)\n",
      "I0228 01:37:37.345891 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (589765 virtual)\n",
      "I0228 01:37:37.365851 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (595506 virtual)\n",
      "I0228 01:37:37.380689 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (600711 virtual)\n",
      "I0228 01:37:37.386490 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (605700 virtual)\n",
      "I0228 01:37:37.391346 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (611361 virtual)\n",
      "I0228 01:37:37.395452 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (616539 virtual)\n",
      "I0228 01:37:37.414443 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (621589 virtual)\n",
      "I0228 01:37:37.418987 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (625992 virtual)\n",
      "I0228 01:37:37.422871 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (630556 virtual)\n",
      "I0228 01:37:37.429662 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (634713 virtual)\n",
      "I0228 01:37:37.453677 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (639986 virtual)\n",
      "I0228 01:37:37.476665 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (645465 virtual)\n",
      "I0228 01:37:37.483043 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (650619 virtual)\n",
      "I0228 01:37:37.486614 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (656251 virtual)\n",
      "I0228 01:37:37.491805 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (660895 virtual)\n",
      "I0228 01:37:37.514662 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (665703 virtual)\n",
      "I0228 01:37:37.523303 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (671020 virtual)\n",
      "I0228 01:37:37.528508 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (676441 virtual)\n",
      "I0228 01:37:37.533461 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (681432 virtual)\n",
      "I0228 01:37:37.537736 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (686386 virtual)\n",
      "I0228 01:37:37.572898 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (691600 virtual)\n",
      "I0228 01:37:37.580358 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (696615 virtual)\n",
      "I0228 01:37:37.584544 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (701725 virtual)\n",
      "I0228 01:37:37.589045 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (707057 virtual)\n",
      "I0228 01:37:37.594322 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (712354 virtual)\n",
      "I0228 01:37:37.625811 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (717568 virtual)\n",
      "I0228 01:37:37.639412 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (722694 virtual)\n",
      "I0228 01:37:37.644258 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (728029 virtual)\n",
      "I0228 01:37:37.648394 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (733386 virtual)\n",
      "I0228 01:37:37.651774 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (738844 virtual)\n",
      "I0228 01:37:37.680970 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (744199 virtual)\n",
      "I0228 01:37:37.696485 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (749082 virtual)\n",
      "I0228 01:37:37.701893 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (754630 virtual)\n",
      "I0228 01:37:37.706082 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (759741 virtual)\n",
      "I0228 01:37:37.709168 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (765005 virtual)\n",
      "I0228 01:37:37.734827 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (770593 virtual)\n",
      "I0228 01:37:37.750262 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (776434 virtual)\n",
      "I0228 01:37:37.755785 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (782287 virtual)\n",
      "I0228 01:37:37.761343 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (788245 virtual)\n",
      "I0228 01:37:37.764581 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (794125 virtual)\n",
      "I0228 01:37:37.787232 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (799667 virtual)\n",
      "I0228 01:37:37.799382 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (805221 virtual)\n",
      "I0228 01:37:37.806145 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (810752 virtual)\n",
      "I0228 01:37:37.810626 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (816594 virtual)\n",
      "I0228 01:37:37.821358 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (822205 virtual)\n",
      "I0228 01:37:37.852597 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (828416 virtual)\n",
      "I0228 01:37:37.863397 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (833731 virtual)\n",
      "I0228 01:37:37.871822 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (839127 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:37.873858 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (844590 virtual)\n",
      "I0228 01:37:37.884078 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (850814 virtual)\n",
      "I0228 01:37:37.910205 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (856896 virtual)\n",
      "I0228 01:37:37.924858 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (862544 virtual)\n",
      "I0228 01:37:37.931130 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (868440 virtual)\n",
      "I0228 01:37:37.938005 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (874269 virtual)\n",
      "I0228 01:37:37.944737 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (880118 virtual)\n",
      "I0228 01:37:37.978253 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (886178 virtual)\n",
      "I0228 01:37:37.983538 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (892226 virtual)\n",
      "I0228 01:37:37.992674 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (897594 virtual)\n",
      "I0228 01:37:37.995789 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (903578 virtual)\n",
      "I0228 01:37:38.008111 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (910850 virtual)\n",
      "I0228 01:37:38.043880 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (916011 virtual)\n",
      "I0228 01:37:38.047892 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (919967 virtual)\n",
      "I0228 01:37:38.051708 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (922500 virtual)\n",
      "I0228 01:37:38.057138 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (927061 virtual)\n",
      "I0228 01:37:38.068725 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (933007 virtual)\n",
      "I0228 01:37:38.105172 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (936878 virtual)\n",
      "I0228 01:37:38.109360 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (940764 virtual)\n",
      "I0228 01:37:38.113898 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (944668 virtual)\n",
      "I0228 01:37:38.118653 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (948192 virtual)\n",
      "I0228 01:37:38.138504 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (952013 virtual)\n",
      "I0228 01:37:38.147395 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (955866 virtual)\n",
      "I0228 01:37:38.153239 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (959483 virtual)\n",
      "I0228 01:37:38.160590 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (962701 virtual)\n",
      "I0228 01:37:38.169562 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (967050 virtual)\n",
      "I0228 01:37:38.188011 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (970975 virtual)\n",
      "I0228 01:37:38.200206 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (974707 virtual)\n",
      "I0228 01:37:38.203313 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (978303 virtual)\n",
      "I0228 01:37:38.205888 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (982022 virtual)\n",
      "I0228 01:37:38.208533 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (985776 virtual)\n",
      "I0228 01:37:38.227900 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (989496 virtual)\n",
      "I0228 01:37:38.241929 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (993206 virtual)\n",
      "I0228 01:37:38.245287 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (996745 virtual)\n",
      "I0228 01:37:38.248014 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (1000342 virtual)\n",
      "I0228 01:37:38.252474 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1004794 virtual)\n",
      "I0228 01:37:38.268517 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1008355 virtual)\n",
      "I0228 01:37:38.273912 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1011962 virtual)\n",
      "I0228 01:37:38.281281 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1015755 virtual)\n",
      "I0228 01:37:38.283973 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1020178 virtual)\n",
      "I0228 01:37:38.290189 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1025435 virtual)\n",
      "I0228 01:37:38.308361 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1029280 virtual)\n",
      "I0228 01:37:38.312623 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1033219 virtual)\n",
      "I0228 01:37:38.317137 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1037093 virtual)\n",
      "I0228 01:37:38.321666 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1041065 virtual)\n",
      "I0228 01:37:38.335066 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1045686 virtual)\n",
      "I0228 01:37:38.343526 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1049424 virtual)\n",
      "I0228 01:37:38.349254 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1053710 virtual)\n",
      "I0228 01:37:38.352860 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1057610 virtual)\n",
      "I0228 01:37:38.363664 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1061601 virtual)\n",
      "I0228 01:37:38.382604 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1065506 virtual)\n",
      "I0228 01:37:38.386937 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1069202 virtual)\n",
      "I0228 01:37:38.391946 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1073645 virtual)\n",
      "I0228 01:37:38.395023 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1077670 virtual)\n",
      "I0228 01:37:38.406444 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1082463 virtual)\n",
      "I0228 01:37:38.422869 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1086578 virtual)\n",
      "I0228 01:37:38.429992 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1091313 virtual)\n",
      "I0228 01:37:38.434597 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1095274 virtual)\n",
      "I0228 01:37:38.438273 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1098758 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:38.445939 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1102997 virtual)\n",
      "I0228 01:37:38.463447 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1107220 virtual)\n",
      "I0228 01:37:38.468268 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1110815 virtual)\n",
      "I0228 01:37:38.475932 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1115236 virtual)\n",
      "I0228 01:37:38.480169 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1119242 virtual)\n",
      "I0228 01:37:38.497853 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1124435 virtual)\n",
      "I0228 01:37:38.506686 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1128247 virtual)\n",
      "I0228 01:37:38.511240 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1133007 virtual)\n",
      "I0228 01:37:38.515363 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1137844 virtual)\n",
      "I0228 01:37:38.519741 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1142390 virtual)\n",
      "I0228 01:37:38.538769 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1147834 virtual)\n",
      "I0228 01:37:38.547852 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1151708 virtual)\n",
      "I0228 01:37:38.551733 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1155363 virtual)\n",
      "I0228 01:37:38.560145 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1159402 virtual)\n",
      "I0228 01:37:38.563702 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1163900 virtual)\n",
      "I0228 01:37:38.586124 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1167573 virtual)\n",
      "I0228 01:37:38.595279 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1172522 virtual)\n",
      "I0228 01:37:38.599970 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1177081 virtual)\n",
      "I0228 01:37:38.610028 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1180851 virtual)\n",
      "I0228 01:37:38.613561 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1185590 virtual)\n",
      "I0228 01:37:38.637957 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1190660 virtual)\n",
      "I0228 01:37:38.641695 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1194432 virtual)\n",
      "I0228 01:37:38.646010 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1199069 virtual)\n",
      "I0228 01:37:38.650867 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1203440 virtual)\n",
      "I0228 01:37:38.661106 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1207846 virtual)\n",
      "I0228 01:37:38.676206 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1211879 virtual)\n",
      "I0228 01:37:38.686811 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1216321 virtual)\n",
      "I0228 01:37:38.690425 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1219984 virtual)\n",
      "I0228 01:37:38.694112 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1224535 virtual)\n",
      "I0228 01:37:38.708057 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1228761 virtual)\n",
      "I0228 01:37:38.723834 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1232549 virtual)\n",
      "I0228 01:37:38.728333 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1235441 virtual)\n",
      "I0228 01:37:38.732140 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1238933 virtual)\n",
      "I0228 01:37:38.734982 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1243511 virtual)\n",
      "I0228 01:37:38.746220 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1247244 virtual)\n",
      "I0228 01:37:38.763161 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1252551 virtual)\n",
      "I0228 01:37:38.767214 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1257080 virtual)\n",
      "I0228 01:37:38.774452 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1261369 virtual)\n",
      "I0228 01:37:38.777444 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1265757 virtual)\n",
      "I0228 01:37:38.788748 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1269653 virtual)\n",
      "I0228 01:37:38.792265 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1273741 virtual)\n",
      "I0228 01:37:38.799767 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1277624 virtual)\n",
      "I0228 01:37:38.807510 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1281525 virtual)\n",
      "I0228 01:37:38.820030 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1286170 virtual)\n",
      "I0228 01:37:38.824648 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1290226 virtual)\n",
      "I0228 01:37:38.845635 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1294434 virtual)\n",
      "I0228 01:37:38.849565 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1298891 virtual)\n",
      "I0228 01:37:38.853415 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1302563 virtual)\n",
      "I0228 01:37:38.860020 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1306180 virtual)\n",
      "I0228 01:37:38.862993 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1310957 virtual)\n",
      "I0228 01:37:38.887398 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1315726 virtual)\n",
      "I0228 01:37:38.891949 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1319228 virtual)\n",
      "I0228 01:37:38.896836 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1323477 virtual)\n",
      "I0228 01:37:38.899580 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1327545 virtual)\n",
      "I0228 01:37:38.905827 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1331155 virtual)\n",
      "I0228 01:37:38.924648 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1335012 virtual)\n",
      "I0228 01:37:38.931063 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1338900 virtual)\n",
      "I0228 01:37:38.935559 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1343121 virtual)\n",
      "I0228 01:37:38.939873 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1347705 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:38.950683 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1352112 virtual)\n",
      "I0228 01:37:38.965878 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1356521 virtual)\n",
      "I0228 01:37:38.970863 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1361608 virtual)\n",
      "I0228 01:37:38.974311 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1363077 virtual)\n",
      "I0228 01:37:38.977619 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1367726 virtual)\n",
      "I0228 01:37:38.985110 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1373464 virtual)\n",
      "I0228 01:37:39.002874 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1377770 virtual)\n",
      "I0228 01:37:39.007265 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1382596 virtual)\n",
      "I0228 01:37:39.015189 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1386661 virtual)\n",
      "I0228 01:37:39.020181 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1391633 virtual)\n",
      "I0228 01:37:39.026451 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1395919 virtual)\n",
      "I0228 01:37:39.033255 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1400490 virtual)\n",
      "I0228 01:37:39.043244 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1405423 virtual)\n",
      "I0228 01:37:39.063154 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1409939 virtual)\n",
      "I0228 01:37:39.069835 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1413977 virtual)\n",
      "I0228 01:37:39.078963 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1418718 virtual)\n",
      "I0228 01:37:39.086371 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1423762 virtual)\n",
      "I0228 01:37:39.094421 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1427613 virtual)\n",
      "I0228 01:37:39.102378 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1432483 virtual)\n",
      "I0228 01:37:39.123244 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1437005 virtual)\n",
      "I0228 01:37:39.127778 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1441269 virtual)\n",
      "I0228 01:37:39.132887 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1446646 virtual)\n",
      "I0228 01:37:39.146433 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1451652 virtual)\n",
      "I0228 01:37:39.150084 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1457746 virtual)\n",
      "I0228 01:37:39.167771 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1462255 virtual)\n",
      "I0228 01:37:39.176792 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1468104 virtual)\n",
      "I0228 01:37:39.184306 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1473274 virtual)\n",
      "I0228 01:37:39.188335 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1480972 virtual)\n",
      "I0228 01:37:39.203831 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1487804 virtual)\n",
      "I0228 01:37:39.216283 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1492869 virtual)\n",
      "I0228 01:37:39.219930 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1497683 virtual)\n",
      "I0228 01:37:39.233502 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1503159 virtual)\n",
      "I0228 01:37:39.245344 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1508236 virtual)\n",
      "I0228 01:37:39.262714 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1513378 virtual)\n",
      "I0228 01:37:39.266907 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1518465 virtual)\n",
      "I0228 01:37:39.275536 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1523404 virtual)\n",
      "I0228 01:37:39.283896 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1533422 virtual)\n",
      "I0228 01:37:39.313891 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1539021 virtual)\n",
      "I0228 01:37:39.321625 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1544152 virtual)\n",
      "I0228 01:37:39.325637 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1549912 virtual)\n",
      "I0228 01:37:39.333243 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1554856 virtual)\n",
      "I0228 01:37:39.342276 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1558751 virtual)\n",
      "I0228 01:37:39.366655 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1563562 virtual)\n",
      "I0228 01:37:39.392609 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1568557 virtual)\n",
      "I0228 01:37:39.398011 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1573260 virtual)\n",
      "I0228 01:37:39.401002 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1578188 virtual)\n",
      "I0228 01:37:39.424914 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1583784 virtual)\n",
      "I0228 01:37:39.430954 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1589779 virtual)\n",
      "I0228 01:37:39.434107 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1595773 virtual)\n",
      "I0228 01:37:39.437128 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1601329 virtual)\n",
      "I0228 01:37:39.440635 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1606838 virtual)\n",
      "I0228 01:37:39.466672 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1612899 virtual)\n",
      "I0228 01:37:39.482043 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1618673 virtual)\n",
      "I0228 01:37:39.487867 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1622942 virtual)\n",
      "I0228 01:37:39.493329 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1628688 virtual)\n",
      "I0228 01:37:39.497153 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1633450 virtual)\n",
      "I0228 01:37:39.524167 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1638904 virtual)\n",
      "I0228 01:37:39.542559 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1644152 virtual)\n",
      "I0228 01:37:39.547587 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1649669 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:39.552402 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1654902 virtual)\n",
      "I0228 01:37:39.557406 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1660455 virtual)\n",
      "I0228 01:37:39.585240 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1665620 virtual)\n",
      "I0228 01:37:39.593405 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1672188 virtual)\n",
      "I0228 01:37:39.600892 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1680376 virtual)\n",
      "I0228 01:37:39.605698 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1689080 virtual)\n",
      "I0228 01:37:39.610162 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1697030 virtual)\n",
      "I0228 01:37:39.639420 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1705102 virtual)\n",
      "I0228 01:37:39.646000 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1710560 virtual)\n",
      "I0228 01:37:39.657389 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1715822 virtual)\n",
      "I0228 01:37:39.662309 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1721772 virtual)\n",
      "I0228 01:37:39.668139 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1727517 virtual)\n",
      "I0228 01:37:39.693840 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1733568 virtual)\n",
      "I0228 01:37:39.723846 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1739518 virtual)\n",
      "I0228 01:37:39.748220 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1745299 virtual)\n",
      "I0228 01:37:39.754678 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1750344 virtual)\n",
      "I0228 01:37:39.759585 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1755930 virtual)\n",
      "I0228 01:37:39.782988 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1760839 virtual)\n",
      "I0228 01:37:39.787290 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1766109 virtual)\n",
      "I0228 01:37:39.803590 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1770746 virtual)\n",
      "I0228 01:37:39.818526 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1776229 virtual)\n",
      "I0228 01:37:39.823855 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1781746 virtual)\n",
      "I0228 01:37:39.845088 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1786866 virtual)\n",
      "I0228 01:37:39.851581 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1791789 virtual)\n",
      "I0228 01:37:39.867296 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1796442 virtual)\n",
      "I0228 01:37:39.871283 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1801679 virtual)\n",
      "I0228 01:37:39.881935 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1806645 virtual)\n",
      "I0228 01:37:39.898688 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1811736 virtual)\n",
      "I0228 01:37:39.910149 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1817240 virtual)\n",
      "I0228 01:37:39.918730 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1822134 virtual)\n",
      "I0228 01:37:39.932058 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1827367 virtual)\n",
      "I0228 01:37:39.941962 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1832367 virtual)\n",
      "I0228 01:37:39.956465 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1837089 virtual)\n",
      "I0228 01:37:39.962382 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1842367 virtual)\n",
      "I0228 01:37:39.971259 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1847649 virtual)\n",
      "I0228 01:37:39.992482 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1852555 virtual)\n",
      "I0228 01:37:39.996814 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1858375 virtual)\n",
      "I0228 01:37:40.009825 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1862839 virtual)\n",
      "I0228 01:37:40.020877 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1867920 virtual)\n",
      "I0228 01:37:40.025148 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1872629 virtual)\n",
      "I0228 01:37:40.049618 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1877715 virtual)\n",
      "I0228 01:37:40.053716 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1882593 virtual)\n",
      "I0228 01:37:40.062471 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1887705 virtual)\n",
      "I0228 01:37:40.078395 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1892655 virtual)\n",
      "I0228 01:37:40.082854 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1898121 virtual)\n",
      "I0228 01:37:40.106309 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1903582 virtual)\n",
      "I0228 01:37:40.112715 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1908658 virtual)\n",
      "I0228 01:37:40.116842 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1913347 virtual)\n",
      "I0228 01:37:40.130100 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1918874 virtual)\n",
      "I0228 01:37:40.134080 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1923898 virtual)\n",
      "I0228 01:37:40.164932 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1928636 virtual)\n",
      "I0228 01:37:40.169454 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1933163 virtual)\n",
      "I0228 01:37:40.179557 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1938304 virtual)\n",
      "I0228 01:37:40.184808 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1943562 virtual)\n",
      "I0228 01:37:40.194777 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1948478 virtual)\n",
      "I0228 01:37:40.223200 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1953672 virtual)\n",
      "I0228 01:37:40.228586 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1958485 virtual)\n",
      "I0228 01:37:40.232460 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1963711 virtual)\n",
      "I0228 01:37:40.252355 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1968987 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:40.255937 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1973895 virtual)\n",
      "I0228 01:37:40.273750 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1979130 virtual)\n",
      "I0228 01:37:40.282075 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (1984053 virtual)\n",
      "I0228 01:37:40.286528 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (1989361 virtual)\n",
      "I0228 01:37:40.303818 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (1994785 virtual)\n",
      "I0228 01:37:40.307927 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (1999774 virtual)\n",
      "I0228 01:37:40.328019 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (2005063 virtual)\n",
      "I0228 01:37:40.332861 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (2010344 virtual)\n",
      "I0228 01:37:40.344807 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2015615 virtual)\n",
      "I0228 01:37:40.357385 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2020888 virtual)\n",
      "I0228 01:37:40.362038 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2026109 virtual)\n",
      "I0228 01:37:40.387098 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2031459 virtual)\n",
      "I0228 01:37:40.391391 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2036416 virtual)\n",
      "I0228 01:37:40.407246 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2040785 virtual)\n",
      "I0228 01:37:40.413767 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2045985 virtual)\n",
      "I0228 01:37:40.418846 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2050757 virtual)\n",
      "I0228 01:37:40.445623 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2055364 virtual)\n",
      "I0228 01:37:40.449693 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2060182 virtual)\n",
      "I0228 01:37:40.465677 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2065009 virtual)\n",
      "I0228 01:37:40.474772 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2070264 virtual)\n",
      "I0228 01:37:40.479249 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2075314 virtual)\n",
      "I0228 01:37:40.503918 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2080582 virtual)\n",
      "I0228 01:37:40.508042 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2085704 virtual)\n",
      "I0228 01:37:40.513984 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2090748 virtual)\n",
      "I0228 01:37:40.532258 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2096181 virtual)\n",
      "I0228 01:37:40.536733 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2101312 virtual)\n",
      "I0228 01:37:40.556700 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2106904 virtual)\n",
      "I0228 01:37:40.560297 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2111891 virtual)\n",
      "I0228 01:37:40.567654 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2116664 virtual)\n",
      "I0228 01:37:40.590173 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2121520 virtual)\n",
      "I0228 01:37:40.594769 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2126597 virtual)\n",
      "I0228 01:37:40.612707 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2130928 virtual)\n",
      "I0228 01:37:40.618192 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2136044 virtual)\n",
      "I0228 01:37:40.623071 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2141508 virtual)\n",
      "I0228 01:37:40.646098 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2146326 virtual)\n",
      "I0228 01:37:40.652228 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2151951 virtual)\n",
      "I0228 01:37:40.669645 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2156930 virtual)\n",
      "I0228 01:37:40.678399 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2162278 virtual)\n",
      "I0228 01:37:40.681808 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2167118 virtual)\n",
      "I0228 01:37:40.698960 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2172025 virtual)\n",
      "I0228 01:37:40.709128 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2177277 virtual)\n",
      "I0228 01:37:40.719170 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2182388 virtual)\n",
      "I0228 01:37:40.731433 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2187238 virtual)\n",
      "I0228 01:37:40.742676 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2192042 virtual)\n",
      "I0228 01:37:40.752258 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2197174 virtual)\n",
      "I0228 01:37:40.771800 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2202256 virtual)\n",
      "I0228 01:37:40.777141 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2207513 virtual)\n",
      "I0228 01:37:40.792417 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2212870 virtual)\n",
      "I0228 01:37:40.797075 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2218226 virtual)\n",
      "I0228 01:37:40.806241 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2223343 virtual)\n",
      "I0228 01:37:40.826448 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2228376 virtual)\n",
      "I0228 01:37:40.832232 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2233208 virtual)\n",
      "I0228 01:37:40.846614 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2238173 virtual)\n",
      "I0228 01:37:40.850959 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2243608 virtual)\n",
      "I0228 01:37:40.864253 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2248663 virtual)\n",
      "I0228 01:37:40.883355 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2253896 virtual)\n",
      "I0228 01:37:40.888544 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2258698 virtual)\n",
      "I0228 01:37:40.905479 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2263990 virtual)\n",
      "I0228 01:37:40.914089 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2268795 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:40.950344 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2273916 virtual)\n",
      "I0228 01:37:40.953446 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2278843 virtual)\n",
      "I0228 01:37:40.956415 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2283831 virtual)\n",
      "I0228 01:37:40.965255 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2288762 virtual)\n",
      "I0228 01:37:40.974663 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2293654 virtual)\n",
      "I0228 01:37:40.978735 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2298538 virtual)\n",
      "I0228 01:37:40.996046 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2303904 virtual)\n",
      "I0228 01:37:40.999850 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2308936 virtual)\n",
      "I0228 01:37:41.018131 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2314150 virtual)\n",
      "I0228 01:37:41.029648 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2319289 virtual)\n",
      "I0228 01:37:41.034440 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2324537 virtual)\n",
      "I0228 01:37:41.050551 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2329722 virtual)\n",
      "I0228 01:37:41.054886 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2334874 virtual)\n",
      "I0228 01:37:41.074890 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2339735 virtual)\n",
      "I0228 01:37:41.088308 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2345234 virtual)\n",
      "I0228 01:37:41.093069 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2350627 virtual)\n",
      "I0228 01:37:41.106046 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2355793 virtual)\n",
      "I0228 01:37:41.110487 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2360859 virtual)\n",
      "I0228 01:37:41.143602 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2365610 virtual)\n",
      "I0228 01:37:41.151282 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2370389 virtual)\n",
      "I0228 01:37:41.163891 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2376029 virtual)\n",
      "I0228 01:37:41.165829 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2380794 virtual)\n",
      "I0228 01:37:41.168067 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2385760 virtual)\n",
      "I0228 01:37:41.185198 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2391428 virtual)\n",
      "I0228 01:37:41.214341 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2396339 virtual)\n",
      "I0228 01:37:41.217179 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2401262 virtual)\n",
      "I0228 01:37:41.220756 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2406297 virtual)\n",
      "I0228 01:37:41.224135 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2411243 virtual)\n",
      "I0228 01:37:41.238540 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2416225 virtual)\n",
      "I0228 01:37:41.264163 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2421275 virtual)\n",
      "I0228 01:37:41.275642 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2426576 virtual)\n",
      "I0228 01:37:41.282960 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2431992 virtual)\n",
      "I0228 01:37:41.286233 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2437173 virtual)\n",
      "I0228 01:37:41.302803 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2441930 virtual)\n",
      "I0228 01:37:41.319001 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2447165 virtual)\n",
      "I0228 01:37:41.329538 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2452451 virtual)\n",
      "I0228 01:37:41.333972 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2457196 virtual)\n",
      "I0228 01:37:41.338436 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2462076 virtual)\n",
      "I0228 01:37:41.358457 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2467324 virtual)\n",
      "I0228 01:37:41.375371 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2472048 virtual)\n",
      "I0228 01:37:41.391308 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2477511 virtual)\n",
      "I0228 01:37:41.395390 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2482954 virtual)\n",
      "I0228 01:37:41.399220 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2487960 virtual)\n",
      "I0228 01:37:41.411802 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2493406 virtual)\n",
      "I0228 01:37:41.433047 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2497948 virtual)\n",
      "I0228 01:37:41.448075 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2502664 virtual)\n",
      "I0228 01:37:41.453746 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2507583 virtual)\n",
      "I0228 01:37:41.459793 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2513140 virtual)\n",
      "I0228 01:37:41.470674 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2518227 virtual)\n",
      "I0228 01:37:41.483555 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2523444 virtual)\n",
      "I0228 01:37:41.504831 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2528270 virtual)\n",
      "I0228 01:37:41.509608 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2533406 virtual)\n",
      "I0228 01:37:41.514567 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2538691 virtual)\n",
      "I0228 01:37:41.530687 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2543520 virtual)\n",
      "I0228 01:37:41.535172 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2548344 virtual)\n",
      "I0228 01:37:41.557580 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2553315 virtual)\n",
      "I0228 01:37:41.562187 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2558408 virtual)\n",
      "I0228 01:37:41.578004 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2563697 virtual)\n",
      "I0228 01:37:41.585001 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2568730 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:41.589037 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2573752 virtual)\n",
      "I0228 01:37:41.610309 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2579217 virtual)\n",
      "I0228 01:37:41.618061 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2584111 virtual)\n",
      "I0228 01:37:41.637202 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2589146 virtual)\n",
      "I0228 01:37:41.641635 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2594463 virtual)\n",
      "I0228 01:37:41.646246 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2599877 virtual)\n",
      "I0228 01:37:41.665749 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2605286 virtual)\n",
      "I0228 01:37:41.676148 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2610534 virtual)\n",
      "I0228 01:37:41.695528 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2615845 virtual)\n",
      "I0228 01:37:41.699947 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2621347 virtual)\n",
      "I0228 01:37:41.704302 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2626478 virtual)\n",
      "I0228 01:37:41.726559 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2631257 virtual)\n",
      "I0228 01:37:41.730646 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2636352 virtual)\n",
      "I0228 01:37:41.751941 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2641313 virtual)\n",
      "I0228 01:37:41.757648 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2646223 virtual)\n",
      "I0228 01:37:41.763729 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2651176 virtual)\n",
      "I0228 01:37:41.785715 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2656231 virtual)\n",
      "I0228 01:37:41.790425 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2661061 virtual)\n",
      "I0228 01:37:41.812361 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2666328 virtual)\n",
      "I0228 01:37:41.817697 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2671413 virtual)\n",
      "I0228 01:37:41.822163 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2676970 virtual)\n",
      "I0228 01:37:41.837087 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2682186 virtual)\n",
      "I0228 01:37:41.846071 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2687655 virtual)\n",
      "I0228 01:37:41.867107 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2692881 virtual)\n",
      "I0228 01:37:41.872771 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2698065 virtual)\n",
      "I0228 01:37:41.879450 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2703418 virtual)\n",
      "I0228 01:37:41.893533 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2708266 virtual)\n",
      "I0228 01:37:41.899128 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2713714 virtual)\n",
      "I0228 01:37:41.924241 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2718428 virtual)\n",
      "I0228 01:37:41.930307 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2723877 virtual)\n",
      "I0228 01:37:41.939053 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2729395 virtual)\n",
      "I0228 01:37:41.950157 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2735096 virtual)\n",
      "I0228 01:37:41.958952 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2740398 virtual)\n",
      "I0228 01:37:41.981874 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2745434 virtual)\n",
      "I0228 01:37:41.987388 140370323834688 text_analysis.py:506] 552 batches submitted to accumulate stats from 35328 documents (2750616 virtual)\n",
      "I0228 01:37:41.998775 140370323834688 text_analysis.py:506] 553 batches submitted to accumulate stats from 35392 documents (2755421 virtual)\n",
      "I0228 01:37:42.008922 140370323834688 text_analysis.py:506] 554 batches submitted to accumulate stats from 35456 documents (2760437 virtual)\n",
      "I0228 01:37:42.020272 140370323834688 text_analysis.py:506] 555 batches submitted to accumulate stats from 35520 documents (2766058 virtual)\n",
      "I0228 01:37:42.033774 140370323834688 text_analysis.py:506] 556 batches submitted to accumulate stats from 35584 documents (2771173 virtual)\n",
      "I0228 01:37:42.046457 140370323834688 text_analysis.py:506] 557 batches submitted to accumulate stats from 35648 documents (2776273 virtual)\n",
      "I0228 01:37:42.061466 140370323834688 text_analysis.py:506] 558 batches submitted to accumulate stats from 35712 documents (2779110 virtual)\n",
      "I0228 01:37:42.123938 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:37:42.142301 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:37:42.146100 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:37:42.148803 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:37:42.149271 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:37:42.162623 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:37:42.127092 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:37:42.145549 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:37:42.165032 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:37:42.159203 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:37:42.571718 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:37:42.588372 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2779432 virtual documents\n",
      "I0228 01:37:42.671784 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:37:43.039746 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (4980 virtual)\n",
      "I0228 01:37:43.044122 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10307 virtual)\n",
      "I0228 01:37:43.046699 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16414 virtual)\n",
      "I0228 01:37:43.050271 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21215 virtual)\n",
      "I0228 01:37:43.056191 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27030 virtual)\n",
      "I0228 01:37:43.061901 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32248 virtual)\n",
      "I0228 01:37:43.067162 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37815 virtual)\n",
      "I0228 01:37:43.069809 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43542 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:43.072300 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48771 virtual)\n",
      "I0228 01:37:43.074959 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54492 virtual)\n",
      "I0228 01:37:43.104065 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60355 virtual)\n",
      "I0228 01:37:43.108066 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65625 virtual)\n",
      "I0228 01:37:43.112285 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70729 virtual)\n",
      "I0228 01:37:43.116885 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (76010 virtual)\n",
      "I0228 01:37:43.130467 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81592 virtual)\n",
      "I0228 01:37:43.164868 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87709 virtual)\n",
      "I0228 01:37:43.170305 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93613 virtual)\n",
      "I0228 01:37:43.174531 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (99143 virtual)\n",
      "I0228 01:37:43.177595 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104268 virtual)\n",
      "I0228 01:37:43.201520 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109705 virtual)\n",
      "I0228 01:37:43.227857 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (115221 virtual)\n",
      "I0228 01:37:43.232012 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120476 virtual)\n",
      "I0228 01:37:43.235630 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (126088 virtual)\n",
      "I0228 01:37:43.239024 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (131919 virtual)\n",
      "I0228 01:37:43.272660 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137604 virtual)\n",
      "I0228 01:37:43.294061 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (142973 virtual)\n",
      "I0228 01:37:43.298711 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148447 virtual)\n",
      "I0228 01:37:43.303914 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154604 virtual)\n",
      "I0228 01:37:43.309014 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160530 virtual)\n",
      "I0228 01:37:43.330378 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (165880 virtual)\n",
      "I0228 01:37:43.350394 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (171258 virtual)\n",
      "I0228 01:37:43.356359 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176822 virtual)\n",
      "I0228 01:37:43.360505 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (182091 virtual)\n",
      "I0228 01:37:43.363882 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (187219 virtual)\n",
      "I0228 01:37:43.393602 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (193064 virtual)\n",
      "I0228 01:37:43.409910 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (198292 virtual)\n",
      "I0228 01:37:43.418219 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (203832 virtual)\n",
      "I0228 01:37:43.422879 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (209515 virtual)\n",
      "I0228 01:37:43.426769 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (214914 virtual)\n",
      "I0228 01:37:43.452683 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220485 virtual)\n",
      "I0228 01:37:43.468333 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (226058 virtual)\n",
      "I0228 01:37:43.480197 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (233504 virtual)\n",
      "I0228 01:37:43.483790 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (238870 virtual)\n",
      "I0228 01:37:43.487936 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (244845 virtual)\n",
      "I0228 01:37:43.517421 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (250318 virtual)\n",
      "I0228 01:37:43.527888 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (255802 virtual)\n",
      "I0228 01:37:43.541937 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (260970 virtual)\n",
      "I0228 01:37:43.547045 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (266221 virtual)\n",
      "I0228 01:37:43.551415 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (271704 virtual)\n",
      "I0228 01:37:43.579009 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (277096 virtual)\n",
      "I0228 01:37:43.589398 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (282511 virtual)\n",
      "I0228 01:37:43.606763 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288459 virtual)\n",
      "I0228 01:37:43.612554 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (293456 virtual)\n",
      "I0228 01:37:43.617238 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (298968 virtual)\n",
      "I0228 01:37:43.639907 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304432 virtual)\n",
      "I0228 01:37:43.648446 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (309677 virtual)\n",
      "I0228 01:37:43.668044 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (315390 virtual)\n",
      "I0228 01:37:43.673653 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (321169 virtual)\n",
      "I0228 01:37:43.680163 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (326295 virtual)\n",
      "I0228 01:37:43.703828 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (332652 virtual)\n",
      "I0228 01:37:43.709649 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (338508 virtual)\n",
      "I0228 01:37:43.723964 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (344100 virtual)\n",
      "I0228 01:37:43.733572 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (349693 virtual)\n",
      "I0228 01:37:43.740037 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (355577 virtual)\n",
      "I0228 01:37:43.765370 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (361631 virtual)\n",
      "I0228 01:37:43.770011 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (367136 virtual)\n",
      "I0228 01:37:43.787927 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (372820 virtual)\n",
      "I0228 01:37:43.795260 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (378654 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:43.800518 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (384563 virtual)\n",
      "I0228 01:37:43.832744 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (389449 virtual)\n",
      "I0228 01:37:43.838766 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (394800 virtual)\n",
      "I0228 01:37:43.847808 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398563 virtual)\n",
      "I0228 01:37:43.856929 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (401968 virtual)\n",
      "I0228 01:37:43.864599 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (406250 virtual)\n",
      "I0228 01:37:43.897691 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (410476 virtual)\n",
      "I0228 01:37:43.901504 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (414872 virtual)\n",
      "I0228 01:37:43.911713 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (419407 virtual)\n",
      "I0228 01:37:43.923902 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (423734 virtual)\n",
      "I0228 01:37:43.933207 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (428051 virtual)\n",
      "I0228 01:37:43.948808 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (432495 virtual)\n",
      "I0228 01:37:43.954123 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (437405 virtual)\n",
      "I0228 01:37:43.958676 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (442278 virtual)\n",
      "I0228 01:37:43.963555 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (446920 virtual)\n",
      "I0228 01:37:43.982311 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (451045 virtual)\n",
      "I0228 01:37:43.993267 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (454872 virtual)\n",
      "I0228 01:37:44.001559 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (458842 virtual)\n",
      "I0228 01:37:44.005384 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (462017 virtual)\n",
      "I0228 01:37:44.031415 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (466599 virtual)\n",
      "I0228 01:37:44.039424 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (471054 virtual)\n",
      "I0228 01:37:44.046136 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (475668 virtual)\n",
      "I0228 01:37:44.055084 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (480260 virtual)\n",
      "I0228 01:37:44.057950 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (484900 virtual)\n",
      "I0228 01:37:44.061329 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (489516 virtual)\n",
      "I0228 01:37:44.077981 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (494310 virtual)\n",
      "I0228 01:37:44.081949 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (498962 virtual)\n",
      "I0228 01:37:44.089212 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (503293 virtual)\n",
      "I0228 01:37:44.091996 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (508183 virtual)\n",
      "I0228 01:37:44.109425 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (513020 virtual)\n",
      "I0228 01:37:44.127718 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (517949 virtual)\n",
      "I0228 01:37:44.133488 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (522232 virtual)\n",
      "I0228 01:37:44.141491 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (526844 virtual)\n",
      "I0228 01:37:44.144265 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (531040 virtual)\n",
      "I0228 01:37:44.161427 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (535233 virtual)\n",
      "I0228 01:37:44.179099 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (539113 virtual)\n",
      "I0228 01:37:44.183432 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (543724 virtual)\n",
      "I0228 01:37:44.190074 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (547971 virtual)\n",
      "I0228 01:37:44.195566 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (552427 virtual)\n",
      "I0228 01:37:44.214754 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (556930 virtual)\n",
      "I0228 01:37:44.230641 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (561261 virtual)\n",
      "I0228 01:37:44.234649 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (565560 virtual)\n",
      "I0228 01:37:44.243355 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (569767 virtual)\n",
      "I0228 01:37:44.246288 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (574295 virtual)\n",
      "I0228 01:37:44.261166 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (577803 virtual)\n",
      "I0228 01:37:44.275554 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (580958 virtual)\n",
      "I0228 01:37:44.289145 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (584575 virtual)\n",
      "I0228 01:37:44.291735 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (589765 virtual)\n",
      "I0228 01:37:44.294253 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (595506 virtual)\n",
      "I0228 01:37:44.311414 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (600711 virtual)\n",
      "I0228 01:37:44.328681 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (605700 virtual)\n",
      "I0228 01:37:44.335174 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (611361 virtual)\n",
      "I0228 01:37:44.339113 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (616539 virtual)\n",
      "I0228 01:37:44.344265 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (621589 virtual)\n",
      "I0228 01:37:44.351883 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (625992 virtual)\n",
      "I0228 01:37:44.370542 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (630556 virtual)\n",
      "I0228 01:37:44.378019 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (634713 virtual)\n",
      "I0228 01:37:44.397092 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (639986 virtual)\n",
      "I0228 01:37:44.405201 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (645465 virtual)\n",
      "I0228 01:37:44.410514 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (650619 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:44.428894 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (656251 virtual)\n",
      "I0228 01:37:44.439193 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (660895 virtual)\n",
      "I0228 01:37:44.456931 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (665703 virtual)\n",
      "I0228 01:37:44.462369 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (671020 virtual)\n",
      "I0228 01:37:44.466812 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (676441 virtual)\n",
      "I0228 01:37:44.481986 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (681432 virtual)\n",
      "I0228 01:37:44.485552 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (686386 virtual)\n",
      "I0228 01:37:44.513921 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (691600 virtual)\n",
      "I0228 01:37:44.517939 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (696615 virtual)\n",
      "I0228 01:37:44.522113 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (701725 virtual)\n",
      "I0228 01:37:44.530005 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (707057 virtual)\n",
      "I0228 01:37:44.545551 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (712354 virtual)\n",
      "I0228 01:37:44.566904 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (717568 virtual)\n",
      "I0228 01:37:44.576668 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (722694 virtual)\n",
      "I0228 01:37:44.580921 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (728029 virtual)\n",
      "I0228 01:37:44.586803 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (733386 virtual)\n",
      "I0228 01:37:44.600114 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (738844 virtual)\n",
      "I0228 01:37:44.622317 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (744199 virtual)\n",
      "I0228 01:37:44.633791 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (749082 virtual)\n",
      "I0228 01:37:44.640626 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (754630 virtual)\n",
      "I0228 01:37:44.644965 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (759741 virtual)\n",
      "I0228 01:37:44.661236 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (765005 virtual)\n",
      "I0228 01:37:44.679113 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (770593 virtual)\n",
      "I0228 01:37:44.688969 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (776434 virtual)\n",
      "I0228 01:37:44.696601 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (782287 virtual)\n",
      "I0228 01:37:44.701039 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (788245 virtual)\n",
      "I0228 01:37:44.721986 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (794125 virtual)\n",
      "I0228 01:37:44.731426 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (799667 virtual)\n",
      "I0228 01:37:44.738480 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (805221 virtual)\n",
      "I0228 01:37:44.751893 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (810752 virtual)\n",
      "I0228 01:37:44.755644 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (816594 virtual)\n",
      "I0228 01:37:44.781604 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (822205 virtual)\n",
      "I0228 01:37:44.795132 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (828416 virtual)\n",
      "I0228 01:37:44.804520 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (833731 virtual)\n",
      "I0228 01:37:44.816375 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (839127 virtual)\n",
      "I0228 01:37:44.820128 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (844590 virtual)\n",
      "I0228 01:37:44.846658 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (850814 virtual)\n",
      "I0228 01:37:44.853784 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (856896 virtual)\n",
      "I0228 01:37:44.864255 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (862544 virtual)\n",
      "I0228 01:37:44.875353 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (868440 virtual)\n",
      "I0228 01:37:44.885276 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (874269 virtual)\n",
      "I0228 01:37:44.909608 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (880118 virtual)\n",
      "I0228 01:37:44.921927 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (886178 virtual)\n",
      "I0228 01:37:44.926438 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (892226 virtual)\n",
      "I0228 01:37:44.936615 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (897594 virtual)\n",
      "I0228 01:37:44.943740 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (903578 virtual)\n",
      "I0228 01:37:44.973992 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (910850 virtual)\n",
      "I0228 01:37:44.985697 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (916011 virtual)\n",
      "I0228 01:37:44.989639 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (919967 virtual)\n",
      "I0228 01:37:44.995033 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (922500 virtual)\n",
      "I0228 01:37:45.007687 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (927061 virtual)\n",
      "I0228 01:37:45.035257 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (933007 virtual)\n",
      "I0228 01:37:45.048398 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (936878 virtual)\n",
      "I0228 01:37:45.054204 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (940764 virtual)\n",
      "I0228 01:37:45.059935 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (944668 virtual)\n",
      "I0228 01:37:45.070595 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (948192 virtual)\n",
      "I0228 01:37:45.081613 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (952013 virtual)\n",
      "I0228 01:37:45.096219 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (955866 virtual)\n",
      "I0228 01:37:45.104173 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (959483 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:45.114488 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (962701 virtual)\n",
      "I0228 01:37:45.122458 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (967050 virtual)\n",
      "I0228 01:37:45.136705 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (970975 virtual)\n",
      "I0228 01:37:45.145407 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (974707 virtual)\n",
      "I0228 01:37:45.149467 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (978303 virtual)\n",
      "I0228 01:37:45.154743 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (982022 virtual)\n",
      "I0228 01:37:45.158920 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (985776 virtual)\n",
      "I0228 01:37:45.176718 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (989496 virtual)\n",
      "I0228 01:37:45.185976 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (993206 virtual)\n",
      "I0228 01:37:45.189492 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (996745 virtual)\n",
      "I0228 01:37:45.193347 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (1000342 virtual)\n",
      "I0228 01:37:45.205667 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1004794 virtual)\n",
      "I0228 01:37:45.217337 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1008355 virtual)\n",
      "I0228 01:37:45.224769 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1011962 virtual)\n",
      "I0228 01:37:45.228893 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1015755 virtual)\n",
      "I0228 01:37:45.236209 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1020178 virtual)\n",
      "I0228 01:37:45.244397 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1025435 virtual)\n",
      "I0228 01:37:45.257275 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1029280 virtual)\n",
      "I0228 01:37:45.261729 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1033219 virtual)\n",
      "I0228 01:37:45.266113 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1037093 virtual)\n",
      "I0228 01:37:45.274180 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1041065 virtual)\n",
      "I0228 01:37:45.290604 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1045686 virtual)\n",
      "I0228 01:37:45.292986 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1049424 virtual)\n",
      "I0228 01:37:45.298096 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1053710 virtual)\n",
      "I0228 01:37:45.302140 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1057610 virtual)\n",
      "I0228 01:37:45.331935 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1061601 virtual)\n",
      "I0228 01:37:45.334630 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1065506 virtual)\n",
      "I0228 01:37:45.340054 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1069202 virtual)\n",
      "I0228 01:37:45.342901 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1073645 virtual)\n",
      "I0228 01:37:45.345664 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1077670 virtual)\n",
      "I0228 01:37:45.379213 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1082463 virtual)\n",
      "I0228 01:37:45.381852 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1086578 virtual)\n",
      "I0228 01:37:45.384972 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1091313 virtual)\n",
      "I0228 01:37:45.388079 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1095274 virtual)\n",
      "I0228 01:37:45.391107 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1098758 virtual)\n",
      "I0228 01:37:45.417912 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1102997 virtual)\n",
      "I0228 01:37:45.420805 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1107220 virtual)\n",
      "I0228 01:37:45.426625 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1110815 virtual)\n",
      "I0228 01:37:45.429102 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1115236 virtual)\n",
      "I0228 01:37:45.431533 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1119242 virtual)\n",
      "I0228 01:37:45.464037 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1124435 virtual)\n",
      "I0228 01:37:45.467693 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1128247 virtual)\n",
      "I0228 01:37:45.471150 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1133007 virtual)\n",
      "I0228 01:37:45.476965 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1137844 virtual)\n",
      "I0228 01:37:45.481067 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1142390 virtual)\n",
      "I0228 01:37:45.505078 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1147834 virtual)\n",
      "I0228 01:37:45.508723 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1151708 virtual)\n",
      "I0228 01:37:45.512575 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1155363 virtual)\n",
      "I0228 01:37:45.515413 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1159402 virtual)\n",
      "I0228 01:37:45.518246 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1163900 virtual)\n",
      "I0228 01:37:45.543689 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1167573 virtual)\n",
      "I0228 01:37:45.561396 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1172522 virtual)\n",
      "I0228 01:37:45.565456 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1177081 virtual)\n",
      "I0228 01:37:45.589513 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1180851 virtual)\n",
      "I0228 01:37:45.602282 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1185590 virtual)\n",
      "I0228 01:37:45.605091 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1190660 virtual)\n",
      "I0228 01:37:45.607648 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1194432 virtual)\n",
      "I0228 01:37:45.610351 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1199069 virtual)\n",
      "I0228 01:37:45.612963 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1203440 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:45.621332 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1207846 virtual)\n",
      "I0228 01:37:45.641552 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1211879 virtual)\n",
      "I0228 01:37:45.645026 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1216321 virtual)\n",
      "I0228 01:37:45.649020 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1219984 virtual)\n",
      "I0228 01:37:45.656446 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1224535 virtual)\n",
      "I0228 01:37:45.668637 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1228761 virtual)\n",
      "I0228 01:37:45.679769 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1232549 virtual)\n",
      "I0228 01:37:45.693132 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1235441 virtual)\n",
      "I0228 01:37:45.696595 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1238933 virtual)\n",
      "I0228 01:37:45.701606 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1243511 virtual)\n",
      "I0228 01:37:45.707734 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1247244 virtual)\n",
      "I0228 01:37:45.718405 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1252551 virtual)\n",
      "I0228 01:37:45.731143 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1257080 virtual)\n",
      "I0228 01:37:45.740665 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1261369 virtual)\n",
      "I0228 01:37:45.745406 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1265757 virtual)\n",
      "I0228 01:37:45.751502 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1269653 virtual)\n",
      "I0228 01:37:45.754126 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1273741 virtual)\n",
      "I0228 01:37:45.756706 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1277624 virtual)\n",
      "I0228 01:37:45.773844 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1281525 virtual)\n",
      "I0228 01:37:45.785869 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1286170 virtual)\n",
      "I0228 01:37:45.789996 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1290226 virtual)\n",
      "I0228 01:37:45.802428 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1294434 virtual)\n",
      "I0228 01:37:45.806672 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1298891 virtual)\n",
      "I0228 01:37:45.819180 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1302563 virtual)\n",
      "I0228 01:37:45.827002 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1306180 virtual)\n",
      "I0228 01:37:45.830996 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1310957 virtual)\n",
      "I0228 01:37:45.843799 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1315726 virtual)\n",
      "I0228 01:37:45.847888 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1319228 virtual)\n",
      "I0228 01:37:45.857011 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1323477 virtual)\n",
      "I0228 01:37:45.865412 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1327545 virtual)\n",
      "I0228 01:37:45.874631 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1331155 virtual)\n",
      "I0228 01:37:45.888550 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1335012 virtual)\n",
      "I0228 01:37:45.892599 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1338900 virtual)\n",
      "I0228 01:37:45.897073 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1343121 virtual)\n",
      "I0228 01:37:45.901348 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1347705 virtual)\n",
      "I0228 01:37:45.921011 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1352112 virtual)\n",
      "I0228 01:37:45.925197 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1356521 virtual)\n",
      "I0228 01:37:45.932842 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1361608 virtual)\n",
      "I0228 01:37:45.935496 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1363077 virtual)\n",
      "I0228 01:37:45.938234 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1367726 virtual)\n",
      "I0228 01:37:45.956694 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1373464 virtual)\n",
      "I0228 01:37:45.960223 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1377770 virtual)\n",
      "I0228 01:37:45.970451 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1382596 virtual)\n",
      "I0228 01:37:45.978609 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1386661 virtual)\n",
      "I0228 01:37:45.984385 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1391633 virtual)\n",
      "I0228 01:37:45.996962 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1395919 virtual)\n",
      "I0228 01:37:46.001614 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1400490 virtual)\n",
      "I0228 01:37:46.006314 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1405423 virtual)\n",
      "I0228 01:37:46.026104 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1409939 virtual)\n",
      "I0228 01:37:46.034695 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1413977 virtual)\n",
      "I0228 01:37:46.043967 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1418718 virtual)\n",
      "I0228 01:37:46.050725 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1423762 virtual)\n",
      "I0228 01:37:46.058413 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1427613 virtual)\n",
      "I0228 01:37:46.065006 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1432483 virtual)\n",
      "I0228 01:37:46.088387 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1437005 virtual)\n",
      "I0228 01:37:46.092792 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1441269 virtual)\n",
      "I0228 01:37:46.097808 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1446646 virtual)\n",
      "I0228 01:37:46.110275 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1451652 virtual)\n",
      "I0228 01:37:46.114509 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1457746 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:46.131481 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1462255 virtual)\n",
      "I0228 01:37:46.136615 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1468104 virtual)\n",
      "I0228 01:37:46.148173 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1473274 virtual)\n",
      "I0228 01:37:46.152649 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1480972 virtual)\n",
      "I0228 01:37:46.161821 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1487804 virtual)\n",
      "I0228 01:37:46.179517 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1492869 virtual)\n",
      "I0228 01:37:46.182002 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1497683 virtual)\n",
      "I0228 01:37:46.202326 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1503159 virtual)\n",
      "I0228 01:37:46.205205 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1508236 virtual)\n",
      "I0228 01:37:46.227302 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1513378 virtual)\n",
      "I0228 01:37:46.240296 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1518465 virtual)\n",
      "I0228 01:37:46.244780 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1523404 virtual)\n",
      "I0228 01:37:46.254861 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1533422 virtual)\n",
      "I0228 01:37:46.289121 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1539021 virtual)\n",
      "I0228 01:37:46.292913 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1544152 virtual)\n",
      "I0228 01:37:46.295454 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1549912 virtual)\n",
      "I0228 01:37:46.298918 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1554856 virtual)\n",
      "I0228 01:37:46.315906 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1558751 virtual)\n",
      "I0228 01:37:46.345088 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1563562 virtual)\n",
      "I0228 01:37:46.349742 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1568557 virtual)\n",
      "I0228 01:37:46.354447 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1573260 virtual)\n",
      "I0228 01:37:46.358281 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1578188 virtual)\n",
      "I0228 01:37:46.399483 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1583784 virtual)\n",
      "I0228 01:37:46.404000 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1589779 virtual)\n",
      "I0228 01:37:46.408762 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1595773 virtual)\n",
      "I0228 01:37:46.411720 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1601329 virtual)\n",
      "I0228 01:37:46.427038 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1606838 virtual)\n",
      "I0228 01:37:46.441512 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1612899 virtual)\n",
      "I0228 01:37:46.452183 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1618673 virtual)\n",
      "I0228 01:37:46.458536 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1622942 virtual)\n",
      "I0228 01:37:46.462587 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1628688 virtual)\n",
      "I0228 01:37:46.475570 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1633450 virtual)\n",
      "I0228 01:37:46.498629 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1638904 virtual)\n",
      "I0228 01:37:46.517034 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1644152 virtual)\n",
      "I0228 01:37:46.522526 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1649669 virtual)\n",
      "I0228 01:37:46.528279 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1654902 virtual)\n",
      "I0228 01:37:46.532922 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1660455 virtual)\n",
      "I0228 01:37:46.560427 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1665620 virtual)\n",
      "I0228 01:37:46.568984 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1672188 virtual)\n",
      "I0228 01:37:46.576232 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1680376 virtual)\n",
      "I0228 01:37:46.583208 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1689080 virtual)\n",
      "I0228 01:37:46.586855 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1697030 virtual)\n",
      "I0228 01:37:46.614681 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1705102 virtual)\n",
      "I0228 01:37:46.623058 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1710560 virtual)\n",
      "I0228 01:37:46.633726 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1715822 virtual)\n",
      "I0228 01:37:46.639146 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1721772 virtual)\n",
      "I0228 01:37:46.645690 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1727517 virtual)\n",
      "I0228 01:37:46.669039 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1733568 virtual)\n",
      "I0228 01:37:46.708109 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1739518 virtual)\n",
      "I0228 01:37:46.726104 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1745299 virtual)\n",
      "I0228 01:37:46.737105 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1750344 virtual)\n",
      "I0228 01:37:46.741188 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1755930 virtual)\n",
      "I0228 01:37:46.758523 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1760839 virtual)\n",
      "I0228 01:37:46.771643 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1766109 virtual)\n",
      "I0228 01:37:46.782417 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1770746 virtual)\n",
      "I0228 01:37:46.798428 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1776229 virtual)\n",
      "I0228 01:37:46.803110 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1781746 virtual)\n",
      "I0228 01:37:46.820689 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1786866 virtual)\n",
      "I0228 01:37:46.835932 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1791789 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:46.846830 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1796442 virtual)\n",
      "I0228 01:37:46.851910 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1801679 virtual)\n",
      "I0228 01:37:46.866303 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1806645 virtual)\n",
      "I0228 01:37:46.873577 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1811736 virtual)\n",
      "I0228 01:37:46.894491 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1817240 virtual)\n",
      "I0228 01:37:46.899451 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1822134 virtual)\n",
      "I0228 01:37:46.913272 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1827367 virtual)\n",
      "I0228 01:37:46.928005 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1832367 virtual)\n",
      "I0228 01:37:46.932378 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1837089 virtual)\n",
      "I0228 01:37:46.946640 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1842367 virtual)\n",
      "I0228 01:37:46.952075 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1847649 virtual)\n",
      "I0228 01:37:46.974413 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1852555 virtual)\n",
      "I0228 01:37:46.982160 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1858375 virtual)\n",
      "I0228 01:37:46.986687 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1862839 virtual)\n",
      "I0228 01:37:47.003985 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1867920 virtual)\n",
      "I0228 01:37:47.008278 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1872629 virtual)\n",
      "I0228 01:37:47.032919 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1877715 virtual)\n",
      "I0228 01:37:47.036950 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1882593 virtual)\n",
      "I0228 01:37:47.041335 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1887705 virtual)\n",
      "I0228 01:37:47.061647 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1892655 virtual)\n",
      "I0228 01:37:47.065945 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1898121 virtual)\n",
      "I0228 01:37:47.088647 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1903582 virtual)\n",
      "I0228 01:37:47.092750 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1908658 virtual)\n",
      "I0228 01:37:47.101081 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1913347 virtual)\n",
      "I0228 01:37:47.112742 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1918874 virtual)\n",
      "I0228 01:37:47.118187 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1923898 virtual)\n",
      "I0228 01:37:47.146804 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1928636 virtual)\n",
      "I0228 01:37:47.149739 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1933163 virtual)\n",
      "I0228 01:37:47.157577 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1938304 virtual)\n",
      "I0228 01:37:47.167719 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1943562 virtual)\n",
      "I0228 01:37:47.197643 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1948478 virtual)\n",
      "I0228 01:37:47.207220 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1953672 virtual)\n",
      "I0228 01:37:47.212061 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1958485 virtual)\n",
      "I0228 01:37:47.217246 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1963711 virtual)\n",
      "I0228 01:37:47.227358 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1968987 virtual)\n",
      "I0228 01:37:47.254251 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1973895 virtual)\n",
      "I0228 01:37:47.258471 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1979130 virtual)\n",
      "I0228 01:37:47.262500 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (1984053 virtual)\n",
      "I0228 01:37:47.272445 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (1989361 virtual)\n",
      "I0228 01:37:47.298604 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (1994785 virtual)\n",
      "I0228 01:37:47.307939 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (1999774 virtual)\n",
      "I0228 01:37:47.316491 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (2005063 virtual)\n",
      "I0228 01:37:47.320624 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (2010344 virtual)\n",
      "I0228 01:37:47.337215 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2015615 virtual)\n",
      "I0228 01:37:47.357621 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2020888 virtual)\n",
      "I0228 01:37:47.363286 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2026109 virtual)\n",
      "I0228 01:37:47.373179 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2031459 virtual)\n",
      "I0228 01:37:47.376065 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2036416 virtual)\n",
      "I0228 01:37:47.399580 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2040785 virtual)\n",
      "I0228 01:37:47.415546 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2045985 virtual)\n",
      "I0228 01:37:47.421205 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2050757 virtual)\n",
      "I0228 01:37:47.433811 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2055364 virtual)\n",
      "I0228 01:37:47.436716 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2060182 virtual)\n",
      "I0228 01:37:47.458302 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2065009 virtual)\n",
      "I0228 01:37:47.477569 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2070264 virtual)\n",
      "I0228 01:37:47.483108 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2075314 virtual)\n",
      "I0228 01:37:47.492017 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2080582 virtual)\n",
      "I0228 01:37:47.494907 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2085704 virtual)\n",
      "I0228 01:37:47.506984 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2090748 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:47.538607 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2096181 virtual)\n",
      "I0228 01:37:47.543247 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2101312 virtual)\n",
      "I0228 01:37:47.548096 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2106904 virtual)\n",
      "I0228 01:37:47.552551 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2111891 virtual)\n",
      "I0228 01:37:47.560491 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2116664 virtual)\n",
      "I0228 01:37:47.595525 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2121520 virtual)\n",
      "I0228 01:37:47.600283 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2126597 virtual)\n",
      "I0228 01:37:47.604960 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2130928 virtual)\n",
      "I0228 01:37:47.609931 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2136044 virtual)\n",
      "I0228 01:37:47.616151 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2141508 virtual)\n",
      "I0228 01:37:47.652871 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2146326 virtual)\n",
      "I0228 01:37:47.658558 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2151951 virtual)\n",
      "I0228 01:37:47.663098 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2156930 virtual)\n",
      "I0228 01:37:47.667493 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2162278 virtual)\n",
      "I0228 01:37:47.671070 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2167118 virtual)\n",
      "I0228 01:37:47.706105 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2172025 virtual)\n",
      "I0228 01:37:47.711266 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2177277 virtual)\n",
      "I0228 01:37:47.717298 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2182388 virtual)\n",
      "I0228 01:37:47.725160 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2187238 virtual)\n",
      "I0228 01:37:47.735213 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2192042 virtual)\n",
      "I0228 01:37:47.759778 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2197174 virtual)\n",
      "I0228 01:37:47.770747 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2202256 virtual)\n",
      "I0228 01:37:47.775856 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2207513 virtual)\n",
      "I0228 01:37:47.788576 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2212870 virtual)\n",
      "I0228 01:37:47.791621 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2218226 virtual)\n",
      "I0228 01:37:47.814232 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2223343 virtual)\n",
      "I0228 01:37:47.828516 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2228376 virtual)\n",
      "I0228 01:37:47.834290 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2233208 virtual)\n",
      "I0228 01:37:47.844001 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2238173 virtual)\n",
      "I0228 01:37:47.846966 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2243608 virtual)\n",
      "I0228 01:37:47.872390 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2248663 virtual)\n",
      "I0228 01:37:47.884599 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2253896 virtual)\n",
      "I0228 01:37:47.890274 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2258698 virtual)\n",
      "I0228 01:37:47.904859 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2263990 virtual)\n",
      "I0228 01:37:47.910521 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2268795 virtual)\n",
      "I0228 01:37:47.931622 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2273916 virtual)\n",
      "I0228 01:37:47.941142 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2278843 virtual)\n",
      "I0228 01:37:47.947399 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2283831 virtual)\n",
      "I0228 01:37:47.962152 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2288762 virtual)\n",
      "I0228 01:37:47.967168 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2293654 virtual)\n",
      "I0228 01:37:47.986315 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2298538 virtual)\n",
      "I0228 01:37:47.996315 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2303904 virtual)\n",
      "I0228 01:37:48.001905 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2308936 virtual)\n",
      "I0228 01:37:48.021349 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2314150 virtual)\n",
      "I0228 01:37:48.025394 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2319289 virtual)\n",
      "I0228 01:37:48.042151 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2324537 virtual)\n",
      "I0228 01:37:48.050326 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2329722 virtual)\n",
      "I0228 01:37:48.056520 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2334874 virtual)\n",
      "I0228 01:37:48.078891 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2339735 virtual)\n",
      "I0228 01:37:48.083917 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2345234 virtual)\n",
      "I0228 01:37:48.097143 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2350627 virtual)\n",
      "I0228 01:37:48.109675 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2355793 virtual)\n",
      "I0228 01:37:48.114273 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2360859 virtual)\n",
      "I0228 01:37:48.135829 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2365610 virtual)\n",
      "I0228 01:37:48.139572 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2370389 virtual)\n",
      "I0228 01:37:48.154616 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2376029 virtual)\n",
      "I0228 01:37:48.165656 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2380794 virtual)\n",
      "I0228 01:37:48.175915 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2385760 virtual)\n",
      "I0228 01:37:48.190347 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2391428 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:48.199167 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2396339 virtual)\n",
      "I0228 01:37:48.217220 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2401262 virtual)\n",
      "I0228 01:37:48.223138 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2406297 virtual)\n",
      "I0228 01:37:48.227715 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2411243 virtual)\n",
      "I0228 01:37:48.247730 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2416225 virtual)\n",
      "I0228 01:37:48.255048 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2421275 virtual)\n",
      "I0228 01:37:48.278739 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2426576 virtual)\n",
      "I0228 01:37:48.283894 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2431992 virtual)\n",
      "I0228 01:37:48.289098 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2437173 virtual)\n",
      "I0228 01:37:48.310710 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2441930 virtual)\n",
      "I0228 01:37:48.315309 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2447165 virtual)\n",
      "I0228 01:37:48.330231 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2452451 virtual)\n",
      "I0228 01:37:48.337882 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2457196 virtual)\n",
      "I0228 01:37:48.342228 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2462076 virtual)\n",
      "I0228 01:37:48.367075 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2467324 virtual)\n",
      "I0228 01:37:48.371353 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2472048 virtual)\n",
      "I0228 01:37:48.390892 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2477511 virtual)\n",
      "I0228 01:37:48.401687 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2482954 virtual)\n",
      "I0228 01:37:48.405837 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2487960 virtual)\n",
      "I0228 01:37:48.421223 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2493406 virtual)\n",
      "I0228 01:37:48.428972 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2497948 virtual)\n",
      "I0228 01:37:48.447360 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2502664 virtual)\n",
      "I0228 01:37:48.457794 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2507583 virtual)\n",
      "I0228 01:37:48.462725 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2513140 virtual)\n",
      "I0228 01:37:48.480244 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2518227 virtual)\n",
      "I0228 01:37:48.483127 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2523444 virtual)\n",
      "I0228 01:37:48.505427 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2528270 virtual)\n",
      "I0228 01:37:48.516491 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2533406 virtual)\n",
      "I0228 01:37:48.521021 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2538691 virtual)\n",
      "I0228 01:37:48.529079 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2543520 virtual)\n",
      "I0228 01:37:48.542119 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2548344 virtual)\n",
      "I0228 01:37:48.557219 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2553315 virtual)\n",
      "I0228 01:37:48.568272 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2558408 virtual)\n",
      "I0228 01:37:48.584275 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2563697 virtual)\n",
      "I0228 01:37:48.589222 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2568730 virtual)\n",
      "I0228 01:37:48.600439 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2573752 virtual)\n",
      "I0228 01:37:48.609487 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2579217 virtual)\n",
      "I0228 01:37:48.627812 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2584111 virtual)\n",
      "I0228 01:37:48.641914 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2589146 virtual)\n",
      "I0228 01:37:48.647938 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2594463 virtual)\n",
      "I0228 01:37:48.654239 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2599877 virtual)\n",
      "I0228 01:37:48.670861 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2605286 virtual)\n",
      "I0228 01:37:48.700734 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2610534 virtual)\n",
      "I0228 01:37:48.704953 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2615845 virtual)\n",
      "I0228 01:37:48.709308 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2621347 virtual)\n",
      "I0228 01:37:48.712618 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2626478 virtual)\n",
      "I0228 01:37:48.728205 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2631257 virtual)\n",
      "I0228 01:37:48.755078 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2636352 virtual)\n",
      "I0228 01:37:48.759635 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2641313 virtual)\n",
      "I0228 01:37:48.767203 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2646223 virtual)\n",
      "I0228 01:37:48.773472 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2651176 virtual)\n",
      "I0228 01:37:48.790400 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2656231 virtual)\n",
      "I0228 01:37:48.811440 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2661061 virtual)\n",
      "I0228 01:37:48.819969 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2666328 virtual)\n",
      "I0228 01:37:48.830474 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2671413 virtual)\n",
      "I0228 01:37:48.834218 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2676970 virtual)\n",
      "I0228 01:37:48.841307 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2682186 virtual)\n",
      "I0228 01:37:48.867550 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2687655 virtual)\n",
      "I0228 01:37:48.875503 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2692881 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:37:48.889252 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2698065 virtual)\n",
      "I0228 01:37:48.894204 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2703418 virtual)\n",
      "I0228 01:37:48.898886 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2708266 virtual)\n",
      "I0228 01:37:48.920536 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2713714 virtual)\n",
      "I0228 01:37:48.933198 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2718428 virtual)\n",
      "I0228 01:37:48.947988 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2723877 virtual)\n",
      "I0228 01:37:48.952966 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2729395 virtual)\n",
      "I0228 01:37:48.957108 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2735096 virtual)\n",
      "I0228 01:37:48.980259 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2740398 virtual)\n",
      "I0228 01:37:48.991539 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2745434 virtual)\n",
      "I0228 01:37:49.006958 140370323834688 text_analysis.py:506] 552 batches submitted to accumulate stats from 35328 documents (2750616 virtual)\n",
      "I0228 01:37:49.012475 140370323834688 text_analysis.py:506] 553 batches submitted to accumulate stats from 35392 documents (2755421 virtual)\n",
      "I0228 01:37:49.017413 140370323834688 text_analysis.py:506] 554 batches submitted to accumulate stats from 35456 documents (2760437 virtual)\n",
      "I0228 01:37:49.042046 140370323834688 text_analysis.py:506] 555 batches submitted to accumulate stats from 35520 documents (2766058 virtual)\n",
      "I0228 01:37:49.046396 140370323834688 text_analysis.py:506] 556 batches submitted to accumulate stats from 35584 documents (2771173 virtual)\n",
      "I0228 01:37:49.067851 140370323834688 text_analysis.py:506] 557 batches submitted to accumulate stats from 35648 documents (2776273 virtual)\n",
      "I0228 01:37:49.073657 140370323834688 text_analysis.py:506] 558 batches submitted to accumulate stats from 35712 documents (2779110 virtual)\n",
      "I0228 01:37:49.130721 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:37:49.158727 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:37:49.160653 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:37:49.164294 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:37:49.186421 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:37:49.188902 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:37:49.134967 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:37:49.162938 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:37:49.164240 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:37:49.175504 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:37:49.563056 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:37:49.579230 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2779432 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores\n",
      "{'epoch': 124, 'cv': 0.6861849666603685, 'umass': -4.128048406783079, 'uci': -0.043299170003908295, 'npmi': 0.07543832728056998, 'rbo': 1.0, 'td': 1.0, 'train_loss': 642.0540743231975, 'topics': [['c0267454', 'salivary', 'ref', 'c0605290', 'heparin', 'c0521990', 'c0020933', 'c1446219', 'c0851891', 'shareholder', 'c0032821', 'excretion', 'c0536858', 'c0038174', 'c0117438', 'c0444584', 'c0751651', 'c0026231', 'somatic', 'colitis', 'c0271737', 'c0024348', 'horse', 'disable', 'seventh', 'appreciable'], ['c0011900', 'c0012634', 'child', 'c0015967', 'c0032285', 'c0546788', 'c1457887', 'common', 'clinical', 'c0019993', 'presentation', 'cause', 'c0003232', 'c3714514', 'c0809949', 'c0010076', 'disclosure', 'c0035236', 'sars-cov-2', 'c0221423', 'c0231221', 'c0019994', 'hospitalize', 'manifestation', 'c0010200', 'confirm'], ['c0543467', 'c0025080', 'postoperative', 'c0031150', 'c0005898', 'c0728940', 'c0038930', 'procedure', 'undergo', 'c0002940', 'c0229962', 'c0009566', 'operative', 'perform', 'recurrence', 'c0850292', 'c0582175', 'conversion', 'c1522577', 'perioperative', 'complication', 'technique', 'c0019080', 'surgical', 'c0162522', 'consecutive'], ['compare', 'c0243095', 'difference', 'c0199470', 'significant', 'significantly', 'receive', 'c0032042', 'measurement', 'c0034108', 'determine', 'predictor', 'concentration', 'confidence', 'assess', 'associate', 'trial', 'primary', 'c0008976', 'decrease', 'c0235195', 'measure', 'c0005516', 'infant', 'c0918012', 'association'], ['crisis', 'policy', 'political', 'economic', 'threat', 'disaster', 'emergency', 'market', 'public', 'national', 'economy', 'sector', 'challenge', 'c1561598', 'face', 'international', 'argue', 'food', 'innovation', 'draw', 'supply', 'financial', 'c0015176', 'c0018104', 'inequality', 'c0242456'], ['c0042210', 'c1254351', 'c1514562', 'c1167622', 'c0030956', 'c0029224', 'c0003320', 'c0003250', 'c0003316', 'c1706082', 'c0020971', 'c0014442', 'potential', 'nanoparticles', 'affinity', 'active', 'c0042736', 'c0678594', 'bind', 'potent', 'drug', 'c0243077', 'novel', 'c0003241', 'c0017968', 'candidate'], ['c1171362', 'c0025929', 'c0007634', 'role', 'c0007613', 'c0079189', 'mechanism', 'c0024432', 'c0017262', 'c0021747', 'activation', 'induce', 'c3539881', 'c0162638', 'c0021368', 'induction', 'activate', 'c0023810', 'c0014597', 'c1327622', 'suppress', 'c0039194', 'c0035696', 'pathway', 'c3714787', 'mouse'], ['propose', 'c3161035', 'c0002045', 'c0025663', 'c0150098', 'machine', 'automate', 'accuracy', 'c0679083', 'prediction', 'performance', 'solve', 'c0037585', 'sensor', 'compute', 'input', 'representation', 'c1710191', 'image', 'solution', 'outperform', 'c1704254', 'c0037589', 'learn', 'equation', 'base'], ['c0679646', 'conduct', 'c2603343', 'train', 'search', 'report', 'c1257890', 'c0027361', 'include', 'c0086388', 'c0242481', 'evidence', 'c0038951', 'impact', 'c0242356', 'c0025353', 'psychological', 'c0003467', 'c0184661', 'identify', 'c0282122', 'c0018724', 'recommendation', 'c0030971', 'c0282574', 'c0376554'], ['c1705920', 'c0042776', 'c0684063', 'sample', 'c0032098', 'c0017428', 'c0003062', 'c0007452', 'genetic', 'c0017446', 'c0039005', 'c0442726', 'c0017337', 'c0005595', 'c1764827', 'c0242781', 'c0086418', 'c0012984', 'diversity', 'isolate', 'c0015733', 'c0029347', 'genotype', 'c1519068', 'population', 'c1511790']]}\n",
      "Epoch: [126/250]\tSamples: [4627476/9181500]\tTrain Loss: 641.88559322755\tTime: 0:00:04.468171\n",
      "Epoch: [127/250]\tSamples: [4664202/9181500]\tTrain Loss: 641.8636881377464\tTime: 0:00:04.554167\n",
      "Epoch: [128/250]\tSamples: [4700928/9181500]\tTrain Loss: 642.0091567026766\tTime: 0:00:04.509314\n",
      "Epoch: [129/250]\tSamples: [4737654/9181500]\tTrain Loss: 642.05623676857\tTime: 0:00:04.610870\n",
      "Epoch: [130/250]\tSamples: [4774380/9181500]\tTrain Loss: 642.0519691005691\tTime: 0:00:04.783099\n",
      "Epoch: [131/250]\tSamples: [4811106/9181500]\tTrain Loss: 641.8578122234588\tTime: 0:00:04.799952\n",
      "Epoch: [132/250]\tSamples: [4847832/9181500]\tTrain Loss: 641.9062103269822\tTime: 0:00:04.771022\n",
      "Epoch: [133/250]\tSamples: [4884558/9181500]\tTrain Loss: 642.2623330967843\tTime: 0:00:04.747778\n",
      "Epoch: [134/250]\tSamples: [4921284/9181500]\tTrain Loss: 641.9998910853346\tTime: 0:00:04.764492\n",
      "Epoch: [135/250]\tSamples: [4958010/9181500]\tTrain Loss: 642.1727684406143\tTime: 0:00:04.759360\n",
      "Epoch: [136/250]\tSamples: [4994736/9181500]\tTrain Loss: 641.8666697298917\tTime: 0:00:04.814404\n",
      "Epoch: [137/250]\tSamples: [5031462/9181500]\tTrain Loss: 642.1087428907654\tTime: 0:00:04.735329\n",
      "Epoch: [138/250]\tSamples: [5068188/9181500]\tTrain Loss: 641.974123619847\tTime: 0:00:04.802632\n",
      "Epoch: [139/250]\tSamples: [5104914/9181500]\tTrain Loss: 641.8241354685713\tTime: 0:00:04.736423\n",
      "Epoch: [140/250]\tSamples: [5141640/9181500]\tTrain Loss: 641.8384893365735\tTime: 0:00:04.812615\n",
      "Epoch: [141/250]\tSamples: [5178366/9181500]\tTrain Loss: 641.8229599559917\tTime: 0:00:04.773181\n",
      "Epoch: [142/250]\tSamples: [5215092/9181500]\tTrain Loss: 641.9430427353918\tTime: 0:00:04.788945\n",
      "Epoch: [143/250]\tSamples: [5251818/9181500]\tTrain Loss: 642.1033465094552\tTime: 0:00:04.774899\n",
      "Epoch: [144/250]\tSamples: [5288544/9181500]\tTrain Loss: 642.0786088406375\tTime: 0:00:04.740543\n",
      "Epoch: [145/250]\tSamples: [5325270/9181500]\tTrain Loss: 641.9641632460655\tTime: 0:00:04.795707\n",
      "Epoch: [146/250]\tSamples: [5361996/9181500]\tTrain Loss: 641.9300067986576\tTime: 0:00:04.744624\n",
      "Epoch: [147/250]\tSamples: [5398722/9181500]\tTrain Loss: 642.0341243260905\tTime: 0:00:04.780246\n",
      "Epoch: [148/250]\tSamples: [5435448/9181500]\tTrain Loss: 641.9878563338983\tTime: 0:00:04.825714\n",
      "Epoch: [149/250]\tSamples: [5472174/9181500]\tTrain Loss: 641.7652238026194\tTime: 0:00:04.776851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:39:48.109536 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [150/250]\tSamples: [5508900/9181500]\tTrain Loss: 642.1069097420765\tTime: 0:00:04.731868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:39:48.921825 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:39:49.611702 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:39:50.428169 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:39:50.959821 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:39:50.966311 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:39:51.711688 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:39:52.389098 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:39:53.186228 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:39:53.702526 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:39:53.707621 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:39:54.461965 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:39:55.153441 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:39:55.951477 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:39:56.473171 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:39:56.479681 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:39:57.227836 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:39:57.896416 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:39:58.699002 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:39:59.226151 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:39:59.248037 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:39:59.777053 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (-35173 virtual)\n",
      "I0228 01:40:00.196329 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (-208271 virtual)\n",
      "I0228 01:40:00.503568 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (-489578 virtual)\n",
      "I0228 01:40:00.507608 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (-488293 virtual)\n",
      "I0228 01:40:00.532989 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (-496732 virtual)\n",
      "I0228 01:40:00.538761 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (-494160 virtual)\n",
      "I0228 01:40:00.639407 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (-519023 virtual)\n",
      "I0228 01:40:00.644498 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (-516780 virtual)\n",
      "I0228 01:40:00.649076 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (-515048 virtual)\n",
      "I0228 01:40:00.652668 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (-513481 virtual)\n",
      "I0228 01:40:00.658475 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (-515394 virtual)\n",
      "I0228 01:40:01.367377 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:40:01.368035 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:40:01.369764 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:40:01.371151 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:40:01.370208 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:40:01.369888 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:40:01.373688 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:40:01.373705 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:40:01.373956 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:40:01.383687 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:40:01.805657 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:40:01.832427 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 229493 virtual documents\n",
      "I0228 01:40:04.125738 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 1000 documents\n",
      "I0228 01:40:04.141667 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 2000 documents\n",
      "I0228 01:40:04.157816 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 3000 documents\n",
      "I0228 01:40:04.173577 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 4000 documents\n",
      "I0228 01:40:04.188072 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 5000 documents\n",
      "I0228 01:40:04.201047 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 6000 documents\n",
      "I0228 01:40:04.215379 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 7000 documents\n",
      "I0228 01:40:04.229139 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 8000 documents\n",
      "I0228 01:40:04.244384 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 9000 documents\n",
      "I0228 01:40:04.260886 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 10000 documents\n",
      "I0228 01:40:04.277424 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 11000 documents\n",
      "I0228 01:40:04.291231 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 12000 documents\n",
      "I0228 01:40:04.303075 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 13000 documents\n",
      "I0228 01:40:04.315237 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 14000 documents\n",
      "I0228 01:40:04.328488 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 15000 documents\n",
      "I0228 01:40:04.340914 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 16000 documents\n",
      "I0228 01:40:04.352554 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 17000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:04.364667 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 18000 documents\n",
      "I0228 01:40:04.376433 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 19000 documents\n",
      "I0228 01:40:04.389570 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 20000 documents\n",
      "I0228 01:40:04.404102 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 21000 documents\n",
      "I0228 01:40:04.419442 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 22000 documents\n",
      "I0228 01:40:04.435758 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 23000 documents\n",
      "I0228 01:40:04.451876 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 24000 documents\n",
      "I0228 01:40:04.467088 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 25000 documents\n",
      "I0228 01:40:04.481937 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 26000 documents\n",
      "I0228 01:40:04.497131 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 27000 documents\n",
      "I0228 01:40:04.512291 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 28000 documents\n",
      "I0228 01:40:04.528778 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 29000 documents\n",
      "I0228 01:40:04.543635 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 30000 documents\n",
      "I0228 01:40:04.558395 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 31000 documents\n",
      "I0228 01:40:04.573307 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 32000 documents\n",
      "I0228 01:40:04.588479 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 33000 documents\n",
      "I0228 01:40:04.603119 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 34000 documents\n",
      "I0228 01:40:04.618184 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 35000 documents\n",
      "I0228 01:40:04.633138 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 36000 documents\n",
      "I0228 01:40:04.786853 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:40:05.157608 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (4980 virtual)\n",
      "I0228 01:40:05.161816 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10307 virtual)\n",
      "I0228 01:40:05.164638 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16414 virtual)\n",
      "I0228 01:40:05.169071 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21215 virtual)\n",
      "I0228 01:40:05.173601 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27030 virtual)\n",
      "I0228 01:40:05.177211 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32248 virtual)\n",
      "I0228 01:40:05.179437 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37686 virtual)\n",
      "I0228 01:40:05.181421 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43473 virtual)\n",
      "I0228 01:40:05.183646 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48754 virtual)\n",
      "I0228 01:40:05.187537 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54434 virtual)\n",
      "I0228 01:40:05.222149 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60342 virtual)\n",
      "I0228 01:40:05.227596 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65651 virtual)\n",
      "I0228 01:40:05.232787 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70698 virtual)\n",
      "I0228 01:40:05.235918 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (75894 virtual)\n",
      "I0228 01:40:05.255927 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81557 virtual)\n",
      "I0228 01:40:05.286190 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87674 virtual)\n",
      "I0228 01:40:05.293340 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93578 virtual)\n",
      "I0228 01:40:05.297652 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (99108 virtual)\n",
      "I0228 01:40:05.301909 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104231 virtual)\n",
      "I0228 01:40:05.316523 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109695 virtual)\n",
      "I0228 01:40:05.351040 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (115191 virtual)\n",
      "I0228 01:40:05.356211 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120441 virtual)\n",
      "I0228 01:40:05.362246 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (126090 virtual)\n",
      "I0228 01:40:05.366933 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (131910 virtual)\n",
      "I0228 01:40:05.383078 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137603 virtual)\n",
      "I0228 01:40:05.413046 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (142985 virtual)\n",
      "I0228 01:40:05.417353 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148466 virtual)\n",
      "I0228 01:40:05.422015 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154624 virtual)\n",
      "I0228 01:40:05.426089 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160599 virtual)\n",
      "I0228 01:40:05.445882 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (165988 virtual)\n",
      "I0228 01:40:05.471264 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (171311 virtual)\n",
      "I0228 01:40:05.475446 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176845 virtual)\n",
      "I0228 01:40:05.479671 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (182066 virtual)\n",
      "I0228 01:40:05.482693 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (187354 virtual)\n",
      "I0228 01:40:05.506947 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (193107 virtual)\n",
      "I0228 01:40:05.531245 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (198267 virtual)\n",
      "I0228 01:40:05.536621 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (203852 virtual)\n",
      "I0228 01:40:05.542893 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (209549 virtual)\n",
      "I0228 01:40:05.547456 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (214900 virtual)\n",
      "I0228 01:40:05.564366 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220572 virtual)\n",
      "I0228 01:40:05.591468 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (226108 virtual)\n",
      "I0228 01:40:05.596863 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (233627 virtual)\n",
      "I0228 01:40:05.601791 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (238932 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:05.608269 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (244870 virtual)\n",
      "I0228 01:40:05.628737 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (250369 virtual)\n",
      "I0228 01:40:05.650773 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (255873 virtual)\n",
      "I0228 01:40:05.658313 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (261077 virtual)\n",
      "I0228 01:40:05.665881 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (266264 virtual)\n",
      "I0228 01:40:05.670143 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (271790 virtual)\n",
      "I0228 01:40:05.695269 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (277301 virtual)\n",
      "I0228 01:40:05.712023 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (282647 virtual)\n",
      "I0228 01:40:05.727131 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288486 virtual)\n",
      "I0228 01:40:05.731938 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (293654 virtual)\n",
      "I0228 01:40:05.737094 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (298973 virtual)\n",
      "I0228 01:40:05.756207 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304516 virtual)\n",
      "I0228 01:40:05.775918 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (309973 virtual)\n",
      "I0228 01:40:05.784081 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (315703 virtual)\n",
      "I0228 01:40:05.788112 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (321326 virtual)\n",
      "I0228 01:40:05.797245 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (326690 virtual)\n",
      "I0228 01:40:05.817997 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (333005 virtual)\n",
      "I0228 01:40:05.836927 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (338914 virtual)\n",
      "I0228 01:40:05.844037 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (344387 virtual)\n",
      "I0228 01:40:05.849232 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (350057 virtual)\n",
      "I0228 01:40:05.855129 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (355993 virtual)\n",
      "I0228 01:40:05.880607 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (362153 virtual)\n",
      "I0228 01:40:05.897475 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (367539 virtual)\n",
      "I0228 01:40:05.903892 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (373207 virtual)\n",
      "I0228 01:40:05.909627 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (379108 virtual)\n",
      "I0228 01:40:05.916234 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (384947 virtual)\n",
      "I0228 01:40:05.953013 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (389876 virtual)\n",
      "I0228 01:40:05.960232 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (395028 virtual)\n",
      "I0228 01:40:05.962787 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398808 virtual)\n",
      "I0228 01:40:05.971490 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (402146 virtual)\n",
      "I0228 01:40:05.982808 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (406538 virtual)\n",
      "I0228 01:40:06.019298 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (410728 virtual)\n",
      "I0228 01:40:06.025401 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (415142 virtual)\n",
      "I0228 01:40:06.028123 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (419683 virtual)\n",
      "I0228 01:40:06.038043 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (424010 virtual)\n",
      "I0228 01:40:06.050638 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (428281 virtual)\n",
      "I0228 01:40:06.071031 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (432785 virtual)\n",
      "I0228 01:40:06.076102 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (437750 virtual)\n",
      "I0228 01:40:06.081167 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (442583 virtual)\n",
      "I0228 01:40:06.085173 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (447269 virtual)\n",
      "I0228 01:40:06.099731 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (451356 virtual)\n",
      "I0228 01:40:06.114923 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (455012 virtual)\n",
      "I0228 01:40:06.119410 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (459112 virtual)\n",
      "I0228 01:40:06.123831 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (462560 virtual)\n",
      "I0228 01:40:06.128544 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (466856 virtual)\n",
      "I0228 01:40:06.147140 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (471464 virtual)\n",
      "I0228 01:40:06.164469 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (476139 virtual)\n",
      "I0228 01:40:06.170016 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (480720 virtual)\n",
      "I0228 01:40:06.174179 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (485302 virtual)\n",
      "I0228 01:40:06.177943 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (489807 virtual)\n",
      "I0228 01:40:06.193415 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (494708 virtual)\n",
      "I0228 01:40:06.202471 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (499180 virtual)\n",
      "I0228 01:40:06.209159 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (503653 virtual)\n",
      "I0228 01:40:06.213261 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (508815 virtual)\n",
      "I0228 01:40:06.221619 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (513589 virtual)\n",
      "I0228 01:40:06.243633 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (518222 virtual)\n",
      "I0228 01:40:06.263159 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (522504 virtual)\n",
      "I0228 01:40:06.266673 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (527169 virtual)\n",
      "I0228 01:40:06.269943 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (531316 virtual)\n",
      "I0228 01:40:06.272843 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (535797 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:06.296362 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (539434 virtual)\n",
      "I0228 01:40:06.306697 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (543997 virtual)\n",
      "I0228 01:40:06.310763 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (548354 virtual)\n",
      "I0228 01:40:06.318357 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (552741 virtual)\n",
      "I0228 01:40:06.325825 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (557172 virtual)\n",
      "I0228 01:40:06.347749 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (561517 virtual)\n",
      "I0228 01:40:06.354800 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (565887 virtual)\n",
      "I0228 01:40:06.362492 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (570096 virtual)\n",
      "I0228 01:40:06.366567 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (574380 virtual)\n",
      "I0228 01:40:06.377698 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (578118 virtual)\n",
      "I0228 01:40:06.388306 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (581174 virtual)\n",
      "I0228 01:40:06.407461 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (584978 virtual)\n",
      "I0228 01:40:06.412754 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (590349 virtual)\n",
      "I0228 01:40:06.418182 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (596085 virtual)\n",
      "I0228 01:40:06.426568 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (601328 virtual)\n",
      "I0228 01:40:06.436685 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (606372 virtual)\n",
      "I0228 01:40:06.454560 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (611829 virtual)\n",
      "I0228 01:40:06.459742 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (616955 virtual)\n",
      "I0228 01:40:06.465358 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (622073 virtual)\n",
      "I0228 01:40:06.469292 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (626491 virtual)\n",
      "I0228 01:40:06.474790 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (630840 virtual)\n",
      "I0228 01:40:06.499378 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (635264 virtual)\n",
      "I0228 01:40:06.518197 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (640417 virtual)\n",
      "I0228 01:40:06.521430 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (645960 virtual)\n",
      "I0228 01:40:06.527938 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (651330 virtual)\n",
      "I0228 01:40:06.531850 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (656787 virtual)\n",
      "I0228 01:40:06.558660 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (661227 virtual)\n",
      "I0228 01:40:06.575149 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (666355 virtual)\n",
      "I0228 01:40:06.581339 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (671595 virtual)\n",
      "I0228 01:40:06.584842 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (676928 virtual)\n",
      "I0228 01:40:06.588254 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (682009 virtual)\n",
      "I0228 01:40:06.607717 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (686779 virtual)\n",
      "I0228 01:40:06.631518 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (692090 virtual)\n",
      "I0228 01:40:06.635697 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (696983 virtual)\n",
      "I0228 01:40:06.640024 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (702496 virtual)\n",
      "I0228 01:40:06.643144 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (707424 virtual)\n",
      "I0228 01:40:06.650127 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (712803 virtual)\n",
      "I0228 01:40:06.688298 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (717950 virtual)\n",
      "I0228 01:40:06.693863 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (723167 virtual)\n",
      "I0228 01:40:06.699155 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (728381 virtual)\n",
      "I0228 01:40:06.702141 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (733927 virtual)\n",
      "I0228 01:40:06.705069 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (739623 virtual)\n",
      "I0228 01:40:06.744089 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (744534 virtual)\n",
      "I0228 01:40:06.748238 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (749592 virtual)\n",
      "I0228 01:40:06.752099 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (755113 virtual)\n",
      "I0228 01:40:06.759771 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (760241 virtual)\n",
      "I0228 01:40:06.763185 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (765416 virtual)\n",
      "I0228 01:40:06.795931 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (771145 virtual)\n",
      "I0228 01:40:06.801532 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (776982 virtual)\n",
      "I0228 01:40:06.807221 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (782850 virtual)\n",
      "I0228 01:40:06.817416 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (788710 virtual)\n",
      "I0228 01:40:06.821754 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (794649 virtual)\n",
      "I0228 01:40:06.842597 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (800120 virtual)\n",
      "I0228 01:40:06.850631 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (805958 virtual)\n",
      "I0228 01:40:06.858999 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (811385 virtual)\n",
      "I0228 01:40:06.872709 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (817080 virtual)\n",
      "I0228 01:40:06.877370 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (822917 virtual)\n",
      "I0228 01:40:06.905410 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (828844 virtual)\n",
      "I0228 01:40:06.916906 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (834227 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:06.928606 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (839536 virtual)\n",
      "I0228 01:40:06.934988 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (845141 virtual)\n",
      "I0228 01:40:06.939725 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (851440 virtual)\n",
      "I0228 01:40:06.963221 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (857344 virtual)\n",
      "I0228 01:40:06.978603 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (863293 virtual)\n",
      "I0228 01:40:06.987930 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (869053 virtual)\n",
      "I0228 01:40:07.002558 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (874914 virtual)\n",
      "I0228 01:40:07.007709 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (880785 virtual)\n",
      "I0228 01:40:07.028730 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (886779 virtual)\n",
      "I0228 01:40:07.038915 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (892931 virtual)\n",
      "I0228 01:40:07.047786 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (898242 virtual)\n",
      "I0228 01:40:07.063144 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (903935 virtual)\n",
      "I0228 01:40:07.071631 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (911729 virtual)\n",
      "I0228 01:40:07.091970 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (916267 virtual)\n",
      "I0228 01:40:07.102594 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (920083 virtual)\n",
      "I0228 01:40:07.107124 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (922608 virtual)\n",
      "I0228 01:40:07.126074 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (928276 virtual)\n",
      "I0228 01:40:07.133759 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (933342 virtual)\n",
      "I0228 01:40:07.153249 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (937310 virtual)\n",
      "I0228 01:40:07.163219 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (941113 virtual)\n",
      "I0228 01:40:07.167424 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (945004 virtual)\n",
      "I0228 01:40:07.188220 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (948490 virtual)\n",
      "I0228 01:40:07.194829 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (952233 virtual)\n",
      "I0228 01:40:07.200333 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (956053 virtual)\n",
      "I0228 01:40:07.206047 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (959801 virtual)\n",
      "I0228 01:40:07.221975 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (963160 virtual)\n",
      "I0228 01:40:07.241898 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (967544 virtual)\n",
      "I0228 01:40:07.246104 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (971266 virtual)\n",
      "I0228 01:40:07.249894 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (974747 virtual)\n",
      "I0228 01:40:07.253514 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (978566 virtual)\n",
      "I0228 01:40:07.265371 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (982483 virtual)\n",
      "I0228 01:40:07.276333 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (985957 virtual)\n",
      "I0228 01:40:07.284584 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (989903 virtual)\n",
      "I0228 01:40:07.288103 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (993598 virtual)\n",
      "I0228 01:40:07.293492 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (997257 virtual)\n",
      "I0228 01:40:07.305403 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (1001243 virtual)\n",
      "I0228 01:40:07.320225 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1005208 virtual)\n",
      "I0228 01:40:07.324321 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1008653 virtual)\n",
      "I0228 01:40:07.328926 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1012607 virtual)\n",
      "I0228 01:40:07.333365 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1016811 virtual)\n",
      "I0228 01:40:07.346456 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1020911 virtual)\n",
      "I0228 01:40:07.354690 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1026083 virtual)\n",
      "I0228 01:40:07.364468 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1029909 virtual)\n",
      "I0228 01:40:07.368789 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1034126 virtual)\n",
      "I0228 01:40:07.371488 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1037862 virtual)\n",
      "I0228 01:40:07.386158 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1042711 virtual)\n",
      "I0228 01:40:07.394355 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1046385 virtual)\n",
      "I0228 01:40:07.396430 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1050218 virtual)\n",
      "I0228 01:40:07.414391 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1054547 virtual)\n",
      "I0228 01:40:07.420531 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1058670 virtual)\n",
      "I0228 01:40:07.426358 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1062415 virtual)\n",
      "I0228 01:40:07.435560 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1066394 virtual)\n",
      "I0228 01:40:07.442846 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1070328 virtual)\n",
      "I0228 01:40:07.458425 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1075117 virtual)\n",
      "I0228 01:40:07.462187 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1079216 virtual)\n",
      "I0228 01:40:07.472411 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1083412 virtual)\n",
      "I0228 01:40:07.476408 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1088316 virtual)\n",
      "I0228 01:40:07.482614 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1092435 virtual)\n",
      "I0228 01:40:07.498542 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1096200 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:07.501093 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1099737 virtual)\n",
      "I0228 01:40:07.509154 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1104193 virtual)\n",
      "I0228 01:40:07.516678 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1107969 virtual)\n",
      "I0228 01:40:07.522577 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1112460 virtual)\n",
      "I0228 01:40:07.543527 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1116677 virtual)\n",
      "I0228 01:40:07.547221 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1121461 virtual)\n",
      "I0228 01:40:07.554925 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1125860 virtual)\n",
      "I0228 01:40:07.559527 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1129916 virtual)\n",
      "I0228 01:40:07.562643 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1135134 virtual)\n",
      "I0228 01:40:07.580459 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1138533 virtual)\n",
      "I0228 01:40:07.585311 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1144387 virtual)\n",
      "I0228 01:40:07.596985 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1148913 virtual)\n",
      "I0228 01:40:07.600406 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1152592 virtual)\n",
      "I0228 01:40:07.607156 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1156248 virtual)\n",
      "I0228 01:40:07.625203 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1160661 virtual)\n",
      "I0228 01:40:07.635722 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1164853 virtual)\n",
      "I0228 01:40:07.640452 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1169353 virtual)\n",
      "I0228 01:40:07.645009 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1174111 virtual)\n",
      "I0228 01:40:07.661009 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1177933 virtual)\n",
      "I0228 01:40:07.664733 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1182385 virtual)\n",
      "I0228 01:40:07.682621 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1187424 virtual)\n",
      "I0228 01:40:07.687815 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1191861 virtual)\n",
      "I0228 01:40:07.696345 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1196541 virtual)\n",
      "I0228 01:40:07.701242 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1200512 virtual)\n",
      "I0228 01:40:07.706239 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1205055 virtual)\n",
      "I0228 01:40:07.723398 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1209074 virtual)\n",
      "I0228 01:40:07.733652 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1213325 virtual)\n",
      "I0228 01:40:07.738396 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1217396 virtual)\n",
      "I0228 01:40:07.744554 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1221875 virtual)\n",
      "I0228 01:40:07.748623 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1226483 virtual)\n",
      "I0228 01:40:07.770518 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1230080 virtual)\n",
      "I0228 01:40:07.777967 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1233427 virtual)\n",
      "I0228 01:40:07.782030 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1236691 virtual)\n",
      "I0228 01:40:07.786571 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1240966 virtual)\n",
      "I0228 01:40:07.789640 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1245024 virtual)\n",
      "I0228 01:40:07.806874 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1249976 virtual)\n",
      "I0228 01:40:07.818419 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1254745 virtual)\n",
      "I0228 01:40:07.825580 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1259292 virtual)\n",
      "I0228 01:40:07.830605 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1263317 virtual)\n",
      "I0228 01:40:07.835471 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1267333 virtual)\n",
      "I0228 01:40:07.840034 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1271425 virtual)\n",
      "I0228 01:40:07.847942 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1275304 virtual)\n",
      "I0228 01:40:07.855788 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1279069 virtual)\n",
      "I0228 01:40:07.867909 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1283599 virtual)\n",
      "I0228 01:40:07.872230 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1288118 virtual)\n",
      "I0228 01:40:07.890167 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1292059 virtual)\n",
      "I0228 01:40:07.894260 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1296673 virtual)\n",
      "I0228 01:40:07.903623 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1300501 virtual)\n",
      "I0228 01:40:07.907054 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1304007 virtual)\n",
      "I0228 01:40:07.910724 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1308734 virtual)\n",
      "I0228 01:40:07.931399 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1313593 virtual)\n",
      "I0228 01:40:07.935164 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1317270 virtual)\n",
      "I0228 01:40:07.941488 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1321240 virtual)\n",
      "I0228 01:40:07.948519 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1325515 virtual)\n",
      "I0228 01:40:07.952608 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1329159 virtual)\n",
      "I0228 01:40:07.969817 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1332821 virtual)\n",
      "I0228 01:40:07.976617 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1336872 virtual)\n",
      "I0228 01:40:07.980099 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1341016 virtual)\n",
      "I0228 01:40:07.986394 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1345576 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:07.994486 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1349951 virtual)\n",
      "I0228 01:40:08.013199 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1354301 virtual)\n",
      "I0228 01:40:08.018449 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1359588 virtual)\n",
      "I0228 01:40:08.022647 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1362039 virtual)\n",
      "I0228 01:40:08.026891 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1367415 virtual)\n",
      "I0228 01:40:08.029906 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1372595 virtual)\n",
      "I0228 01:40:08.048480 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1377005 virtual)\n",
      "I0228 01:40:08.052983 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1381831 virtual)\n",
      "I0228 01:40:08.060123 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1386065 virtual)\n",
      "I0228 01:40:08.071962 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1391330 virtual)\n",
      "I0228 01:40:08.075292 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1395144 virtual)\n",
      "I0228 01:40:08.087054 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1400182 virtual)\n",
      "I0228 01:40:08.090545 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1405205 virtual)\n",
      "I0228 01:40:08.111003 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1409351 virtual)\n",
      "I0228 01:40:08.125660 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1413923 virtual)\n",
      "I0228 01:40:08.130333 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1418820 virtual)\n",
      "I0228 01:40:08.134887 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1422765 virtual)\n",
      "I0228 01:40:08.138686 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1427567 virtual)\n",
      "I0228 01:40:08.152713 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1431996 virtual)\n",
      "I0228 01:40:08.167236 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1436613 virtual)\n",
      "I0228 01:40:08.185760 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1441411 virtual)\n",
      "I0228 01:40:08.189359 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1447354 virtual)\n",
      "I0228 01:40:08.193910 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1452310 virtual)\n",
      "I0228 01:40:08.196720 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1457410 virtual)\n",
      "I0228 01:40:08.214618 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1463081 virtual)\n",
      "I0228 01:40:08.224883 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1468124 virtual)\n",
      "I0228 01:40:08.235570 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1475222 virtual)\n",
      "I0228 01:40:08.240767 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1482907 virtual)\n",
      "I0228 01:40:08.244957 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1488104 virtual)\n",
      "I0228 01:40:08.261646 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1493243 virtual)\n",
      "I0228 01:40:08.275624 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1498517 virtual)\n",
      "I0228 01:40:08.282536 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1503644 virtual)\n",
      "I0228 01:40:08.294777 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1508984 virtual)\n",
      "I0228 01:40:08.298877 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1514088 virtual)\n",
      "I0228 01:40:08.318412 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1519001 virtual)\n",
      "I0228 01:40:08.322971 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1525668 virtual)\n",
      "I0228 01:40:08.350486 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1534640 virtual)\n",
      "I0228 01:40:08.354988 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1539925 virtual)\n",
      "I0228 01:40:08.364817 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1545853 virtual)\n",
      "I0228 01:40:08.368110 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1550829 virtual)\n",
      "I0228 01:40:08.372289 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1555309 virtual)\n",
      "I0228 01:40:08.404627 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1560020 virtual)\n",
      "I0228 01:40:08.409077 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1564761 virtual)\n",
      "I0228 01:40:08.413983 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1569695 virtual)\n",
      "I0228 01:40:08.418188 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1574553 virtual)\n",
      "I0228 01:40:08.438149 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1580021 virtual)\n",
      "I0228 01:40:08.464390 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1586174 virtual)\n",
      "I0228 01:40:08.468717 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1591959 virtual)\n",
      "I0228 01:40:08.473537 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1597555 virtual)\n",
      "I0228 01:40:08.484296 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1602978 virtual)\n",
      "I0228 01:40:08.492343 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1608950 virtual)\n",
      "I0228 01:40:08.513515 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1614881 virtual)\n",
      "I0228 01:40:08.519558 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1619468 virtual)\n",
      "I0228 01:40:08.525450 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1624907 virtual)\n",
      "I0228 01:40:08.529210 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1629797 virtual)\n",
      "I0228 01:40:08.547496 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1635176 virtual)\n",
      "I0228 01:40:08.576172 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1640359 virtual)\n",
      "I0228 01:40:08.580403 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1645923 virtual)\n",
      "I0228 01:40:08.584811 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1651014 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:08.589410 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1656809 virtual)\n",
      "I0228 01:40:08.605007 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1661699 virtual)\n",
      "I0228 01:40:08.627259 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1668043 virtual)\n",
      "I0228 01:40:08.633753 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1676177 virtual)\n",
      "I0228 01:40:08.640790 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1684820 virtual)\n",
      "I0228 01:40:08.644427 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1692952 virtual)\n",
      "I0228 01:40:08.657380 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1700919 virtual)\n",
      "I0228 01:40:08.679689 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1706837 virtual)\n",
      "I0228 01:40:08.689384 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1711639 virtual)\n",
      "I0228 01:40:08.693207 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1718206 virtual)\n",
      "I0228 01:40:08.696547 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1723765 virtual)\n",
      "I0228 01:40:08.708022 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1729807 virtual)\n",
      "I0228 01:40:08.750447 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1735939 virtual)\n",
      "I0228 01:40:08.779756 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1741557 virtual)\n",
      "I0228 01:40:08.784320 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1746505 virtual)\n",
      "I0228 01:40:08.788546 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1752070 virtual)\n",
      "I0228 01:40:08.796426 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1757324 virtual)\n",
      "I0228 01:40:08.819039 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1762501 virtual)\n",
      "I0228 01:40:08.830372 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1767330 virtual)\n",
      "I0228 01:40:08.836710 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1772477 virtual)\n",
      "I0228 01:40:08.850208 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1778037 virtual)\n",
      "I0228 01:40:08.857303 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1783368 virtual)\n",
      "I0228 01:40:08.883283 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1788120 virtual)\n",
      "I0228 01:40:08.888211 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1793051 virtual)\n",
      "I0228 01:40:08.893604 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1797957 virtual)\n",
      "I0228 01:40:08.912945 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1803104 virtual)\n",
      "I0228 01:40:08.917643 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1808209 virtual)\n",
      "I0228 01:40:08.937713 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1813744 virtual)\n",
      "I0228 01:40:08.941950 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1818698 virtual)\n",
      "I0228 01:40:08.948040 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1823847 virtual)\n",
      "I0228 01:40:08.974376 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1828976 virtual)\n",
      "I0228 01:40:08.979225 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1833772 virtual)\n",
      "I0228 01:40:08.987010 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1838795 virtual)\n",
      "I0228 01:40:08.997715 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1843983 virtual)\n",
      "I0228 01:40:09.004101 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1849049 virtual)\n",
      "I0228 01:40:09.027342 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1854734 virtual)\n",
      "I0228 01:40:09.030760 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1859500 virtual)\n",
      "I0228 01:40:09.046403 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1864475 virtual)\n",
      "I0228 01:40:09.050025 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1869216 virtual)\n",
      "I0228 01:40:09.058578 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1874395 virtual)\n",
      "I0228 01:40:09.082676 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1879231 virtual)\n",
      "I0228 01:40:09.087118 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1884331 virtual)\n",
      "I0228 01:40:09.100845 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1889287 virtual)\n",
      "I0228 01:40:09.106234 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1894562 virtual)\n",
      "I0228 01:40:09.114732 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1900058 virtual)\n",
      "I0228 01:40:09.137697 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1905205 virtual)\n",
      "I0228 01:40:09.146920 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1910028 virtual)\n",
      "I0228 01:40:09.155459 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1915616 virtual)\n",
      "I0228 01:40:09.159109 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1920620 virtual)\n",
      "I0228 01:40:09.176012 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1925304 virtual)\n",
      "I0228 01:40:09.191497 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1929899 virtual)\n",
      "I0228 01:40:09.208338 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1935069 virtual)\n",
      "I0228 01:40:09.212977 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1940261 virtual)\n",
      "I0228 01:40:09.218070 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1945217 virtual)\n",
      "I0228 01:40:09.231595 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1950356 virtual)\n",
      "I0228 01:40:09.248579 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1955262 virtual)\n",
      "I0228 01:40:09.264023 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1960457 virtual)\n",
      "I0228 01:40:09.269093 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1965792 virtual)\n",
      "I0228 01:40:09.285578 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1970700 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:09.289578 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1975969 virtual)\n",
      "I0228 01:40:09.298836 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1980917 virtual)\n",
      "I0228 01:40:09.324393 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1986221 virtual)\n",
      "I0228 01:40:09.329163 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (1991622 virtual)\n",
      "I0228 01:40:09.333773 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (1996556 virtual)\n",
      "I0228 01:40:09.341015 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (2001845 virtual)\n",
      "I0228 01:40:09.352146 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (2007126 virtual)\n",
      "I0228 01:40:09.381517 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (2012397 virtual)\n",
      "I0228 01:40:09.385806 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (2017670 virtual)\n",
      "I0228 01:40:09.390101 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2022891 virtual)\n",
      "I0228 01:40:09.400707 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2028241 virtual)\n",
      "I0228 01:40:09.407078 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2033198 virtual)\n",
      "I0228 01:40:09.442059 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2037631 virtual)\n",
      "I0228 01:40:09.446627 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2042930 virtual)\n",
      "I0228 01:40:09.451615 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2047634 virtual)\n",
      "I0228 01:40:09.458388 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2052212 virtual)\n",
      "I0228 01:40:09.466410 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2056997 virtual)\n",
      "I0228 01:40:09.500705 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2061924 virtual)\n",
      "I0228 01:40:09.505468 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2067156 virtual)\n",
      "I0228 01:40:09.510178 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2072179 virtual)\n",
      "I0228 01:40:09.516345 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2077455 virtual)\n",
      "I0228 01:40:09.523020 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2082598 virtual)\n",
      "I0228 01:40:09.548553 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2087665 virtual)\n",
      "I0228 01:40:09.562910 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2093156 virtual)\n",
      "I0228 01:40:09.568134 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2098300 virtual)\n",
      "I0228 01:40:09.574203 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2103841 virtual)\n",
      "I0228 01:40:09.578721 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2108719 virtual)\n",
      "I0228 01:40:09.603672 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2113552 virtual)\n",
      "I0228 01:40:09.616798 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2118363 virtual)\n",
      "I0228 01:40:09.621778 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2123429 virtual)\n",
      "I0228 01:40:09.625969 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2127869 virtual)\n",
      "I0228 01:40:09.631545 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2132967 virtual)\n",
      "I0228 01:40:09.658307 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2138365 virtual)\n",
      "I0228 01:40:09.676816 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2143410 virtual)\n",
      "I0228 01:40:09.682367 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2148983 virtual)\n",
      "I0228 01:40:09.685833 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2154038 virtual)\n",
      "I0228 01:40:09.688697 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2159221 virtual)\n",
      "I0228 01:40:09.713088 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2164022 virtual)\n",
      "I0228 01:40:09.727217 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2168994 virtual)\n",
      "I0228 01:40:09.733223 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2174218 virtual)\n",
      "I0228 01:40:09.739211 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2179308 virtual)\n",
      "I0228 01:40:09.745308 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2184186 virtual)\n",
      "I0228 01:40:09.774820 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2188893 virtual)\n",
      "I0228 01:40:09.781512 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2194366 virtual)\n",
      "I0228 01:40:09.788631 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2199360 virtual)\n",
      "I0228 01:40:09.794761 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2204644 virtual)\n",
      "I0228 01:40:09.803834 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2210331 virtual)\n",
      "I0228 01:40:09.825699 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2215212 virtual)\n",
      "I0228 01:40:09.837011 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2220493 virtual)\n",
      "I0228 01:40:09.847990 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2225556 virtual)\n",
      "I0228 01:40:09.852523 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2230367 virtual)\n",
      "I0228 01:40:09.858679 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2235479 virtual)\n",
      "I0228 01:40:09.876569 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2240754 virtual)\n",
      "I0228 01:40:09.897238 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2245704 virtual)\n",
      "I0228 01:40:09.903136 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2251056 virtual)\n",
      "I0228 01:40:09.907360 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2255820 virtual)\n",
      "I0228 01:40:09.922192 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2261246 virtual)\n",
      "I0228 01:40:09.931623 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2265916 virtual)\n",
      "I0228 01:40:09.956515 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2270947 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:09.962118 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2276003 virtual)\n",
      "I0228 01:40:09.973387 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2281034 virtual)\n",
      "I0228 01:40:09.978515 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2286250 virtual)\n",
      "I0228 01:40:09.990108 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2290792 virtual)\n",
      "I0228 01:40:10.008635 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2295818 virtual)\n",
      "I0228 01:40:10.014140 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2301067 virtual)\n",
      "I0228 01:40:10.018424 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2306240 virtual)\n",
      "I0228 01:40:10.036134 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2311268 virtual)\n",
      "I0228 01:40:10.043146 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2316522 virtual)\n",
      "I0228 01:40:10.062085 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2321728 virtual)\n",
      "I0228 01:40:10.068909 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2327105 virtual)\n",
      "I0228 01:40:10.072884 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2331950 virtual)\n",
      "I0228 01:40:10.095393 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2336988 virtual)\n",
      "I0228 01:40:10.100022 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2342255 virtual)\n",
      "I0228 01:40:10.118549 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2347916 virtual)\n",
      "I0228 01:40:10.125650 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2352910 virtual)\n",
      "I0228 01:40:10.129673 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2357961 virtual)\n",
      "I0228 01:40:10.149754 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2362645 virtual)\n",
      "I0228 01:40:10.154428 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2367552 virtual)\n",
      "I0228 01:40:10.175236 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2373090 virtual)\n",
      "I0228 01:40:10.179147 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2377877 virtual)\n",
      "I0228 01:40:10.184525 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2382898 virtual)\n",
      "I0228 01:40:10.203726 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2388646 virtual)\n",
      "I0228 01:40:10.210039 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2393437 virtual)\n",
      "I0228 01:40:10.233692 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2398554 virtual)\n",
      "I0228 01:40:10.238487 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2403502 virtual)\n",
      "I0228 01:40:10.243142 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2408567 virtual)\n",
      "I0228 01:40:10.256314 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2413467 virtual)\n",
      "I0228 01:40:10.264660 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2418513 virtual)\n",
      "I0228 01:40:10.293934 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2423842 virtual)\n",
      "I0228 01:40:10.297532 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2429143 virtual)\n",
      "I0228 01:40:10.300458 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2434239 virtual)\n",
      "I0228 01:40:10.317402 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2439212 virtual)\n",
      "I0228 01:40:10.321916 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2444390 virtual)\n",
      "I0228 01:40:10.347628 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2449666 virtual)\n",
      "I0228 01:40:10.350304 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2454270 virtual)\n",
      "I0228 01:40:10.358207 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2459256 virtual)\n",
      "I0228 01:40:10.372813 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2464486 virtual)\n",
      "I0228 01:40:10.377078 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2469586 virtual)\n",
      "I0228 01:40:10.406713 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2475075 virtual)\n",
      "I0228 01:40:10.410850 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2480169 virtual)\n",
      "I0228 01:40:10.418110 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2485292 virtual)\n",
      "I0228 01:40:10.427731 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2490647 virtual)\n",
      "I0228 01:40:10.433556 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2495218 virtual)\n",
      "I0228 01:40:10.460667 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2500030 virtual)\n",
      "I0228 01:40:10.465121 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2505000 virtual)\n",
      "I0228 01:40:10.473243 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2510702 virtual)\n",
      "I0228 01:40:10.485978 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2515918 virtual)\n",
      "I0228 01:40:10.490391 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2521127 virtual)\n",
      "I0228 01:40:10.518593 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2525856 virtual)\n",
      "I0228 01:40:10.523743 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2531160 virtual)\n",
      "I0228 01:40:10.533603 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2536146 virtual)\n",
      "I0228 01:40:10.536885 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2540993 virtual)\n",
      "I0228 01:40:10.543007 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2546058 virtual)\n",
      "I0228 01:40:10.573075 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2551091 virtual)\n",
      "I0228 01:40:10.576196 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2556133 virtual)\n",
      "I0228 01:40:10.598221 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2561381 virtual)\n",
      "I0228 01:40:10.601175 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2566087 virtual)\n",
      "I0228 01:40:10.604276 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2571611 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:10.623648 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2576995 virtual)\n",
      "I0228 01:40:10.635938 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2581826 virtual)\n",
      "I0228 01:40:10.646229 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2586780 virtual)\n",
      "I0228 01:40:10.650677 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2592324 virtual)\n",
      "I0228 01:40:10.665207 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2597697 virtual)\n",
      "I0228 01:40:10.676512 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2602980 virtual)\n",
      "I0228 01:40:10.691131 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2608378 virtual)\n",
      "I0228 01:40:10.709195 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2613636 virtual)\n",
      "I0228 01:40:10.712602 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2619138 virtual)\n",
      "I0228 01:40:10.722211 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2624137 virtual)\n",
      "I0228 01:40:10.739872 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2628974 virtual)\n",
      "I0228 01:40:10.744289 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2633951 virtual)\n",
      "I0228 01:40:10.760554 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2639190 virtual)\n",
      "I0228 01:40:10.770382 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2643909 virtual)\n",
      "I0228 01:40:10.780440 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2648853 virtual)\n",
      "I0228 01:40:10.797243 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2653724 virtual)\n",
      "I0228 01:40:10.802559 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2658706 virtual)\n",
      "I0228 01:40:10.821127 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2664083 virtual)\n",
      "I0228 01:40:10.830787 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2669141 virtual)\n",
      "I0228 01:40:10.835777 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2674755 virtual)\n",
      "I0228 01:40:10.849921 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2679832 virtual)\n",
      "I0228 01:40:10.855964 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2685335 virtual)\n",
      "I0228 01:40:10.880170 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2690379 virtual)\n",
      "I0228 01:40:10.885206 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2695828 virtual)\n",
      "I0228 01:40:10.890103 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2701067 virtual)\n",
      "I0228 01:40:10.902060 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2705963 virtual)\n",
      "I0228 01:40:10.911486 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2711061 virtual)\n",
      "I0228 01:40:10.936967 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2716228 virtual)\n",
      "I0228 01:40:10.941633 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2721670 virtual)\n",
      "I0228 01:40:10.951882 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2727232 virtual)\n",
      "I0228 01:40:10.959282 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2732887 virtual)\n",
      "I0228 01:40:10.974379 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2738131 virtual)\n",
      "I0228 01:40:10.994006 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2743160 virtual)\n",
      "I0228 01:40:11.001133 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2748231 virtual)\n",
      "I0228 01:40:11.010394 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2753148 virtual)\n",
      "I0228 01:40:11.017298 140370323834688 text_analysis.py:506] 552 batches submitted to accumulate stats from 35328 documents (2758378 virtual)\n",
      "I0228 01:40:11.026000 140370323834688 text_analysis.py:506] 553 batches submitted to accumulate stats from 35392 documents (2763882 virtual)\n",
      "I0228 01:40:11.050114 140370323834688 text_analysis.py:506] 554 batches submitted to accumulate stats from 35456 documents (2768927 virtual)\n",
      "I0228 01:40:11.059201 140370323834688 text_analysis.py:506] 555 batches submitted to accumulate stats from 35520 documents (2773784 virtual)\n",
      "I0228 01:40:11.069648 140370323834688 text_analysis.py:506] 556 batches submitted to accumulate stats from 35584 documents (2775322 virtual)\n",
      "I0228 01:40:11.132632 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:40:11.141887 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:40:11.147191 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:40:11.150153 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:40:11.158382 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:40:11.172469 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:40:11.137200 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:40:11.147488 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:40:11.183104 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:40:11.162814 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:40:11.583063 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:40:11.605024 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2775624 virtual documents\n",
      "I0228 01:40:11.689387 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:40:12.051709 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (4980 virtual)\n",
      "I0228 01:40:12.058277 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10307 virtual)\n",
      "I0228 01:40:12.061912 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16414 virtual)\n",
      "I0228 01:40:12.065728 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21215 virtual)\n",
      "I0228 01:40:12.069812 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27030 virtual)\n",
      "I0228 01:40:12.073649 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32248 virtual)\n",
      "I0228 01:40:12.076042 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37686 virtual)\n",
      "I0228 01:40:12.078377 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43473 virtual)\n",
      "I0228 01:40:12.080884 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48754 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:12.083004 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54434 virtual)\n",
      "I0228 01:40:12.110245 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60342 virtual)\n",
      "I0228 01:40:12.139528 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65651 virtual)\n",
      "I0228 01:40:12.147377 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70698 virtual)\n",
      "I0228 01:40:12.150933 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (75894 virtual)\n",
      "I0228 01:40:12.154873 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81557 virtual)\n",
      "I0228 01:40:12.170227 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87674 virtual)\n",
      "I0228 01:40:12.200777 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93578 virtual)\n",
      "I0228 01:40:12.207671 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (99108 virtual)\n",
      "I0228 01:40:12.213870 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104231 virtual)\n",
      "I0228 01:40:12.219687 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109695 virtual)\n",
      "I0228 01:40:12.232661 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (115191 virtual)\n",
      "I0228 01:40:12.258671 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120441 virtual)\n",
      "I0228 01:40:12.268596 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (126090 virtual)\n",
      "I0228 01:40:12.273754 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (131910 virtual)\n",
      "I0228 01:40:12.285765 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137603 virtual)\n",
      "I0228 01:40:12.304733 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (142985 virtual)\n",
      "I0228 01:40:12.323496 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148466 virtual)\n",
      "I0228 01:40:12.328814 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154624 virtual)\n",
      "I0228 01:40:12.334414 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160599 virtual)\n",
      "I0228 01:40:12.347492 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (165988 virtual)\n",
      "I0228 01:40:12.360767 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (171311 virtual)\n",
      "I0228 01:40:12.380470 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176845 virtual)\n",
      "I0228 01:40:12.389274 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (182066 virtual)\n",
      "I0228 01:40:12.407369 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (187354 virtual)\n",
      "I0228 01:40:12.414852 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (193107 virtual)\n",
      "I0228 01:40:12.421285 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (198267 virtual)\n",
      "I0228 01:40:12.442019 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (203852 virtual)\n",
      "I0228 01:40:12.455654 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (209549 virtual)\n",
      "I0228 01:40:12.466319 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (214900 virtual)\n",
      "I0228 01:40:12.471182 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220572 virtual)\n",
      "I0228 01:40:12.479414 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (226108 virtual)\n",
      "I0228 01:40:12.504085 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (233627 virtual)\n",
      "I0228 01:40:12.512130 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (238932 virtual)\n",
      "I0228 01:40:12.527682 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (244870 virtual)\n",
      "I0228 01:40:12.537453 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (250369 virtual)\n",
      "I0228 01:40:12.541875 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (255873 virtual)\n",
      "I0228 01:40:12.567437 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (261077 virtual)\n",
      "I0228 01:40:12.576712 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (266264 virtual)\n",
      "I0228 01:40:12.585833 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (271790 virtual)\n",
      "I0228 01:40:12.599402 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (277301 virtual)\n",
      "I0228 01:40:12.604011 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (282647 virtual)\n",
      "I0228 01:40:12.636256 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288486 virtual)\n",
      "I0228 01:40:12.640337 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (293654 virtual)\n",
      "I0228 01:40:12.650017 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (298973 virtual)\n",
      "I0228 01:40:12.659226 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304516 virtual)\n",
      "I0228 01:40:12.663095 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (309973 virtual)\n",
      "I0228 01:40:12.695554 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (315703 virtual)\n",
      "I0228 01:40:12.700715 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (321326 virtual)\n",
      "I0228 01:40:12.714658 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (326690 virtual)\n",
      "I0228 01:40:12.719929 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (333005 virtual)\n",
      "I0228 01:40:12.725483 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (338914 virtual)\n",
      "I0228 01:40:12.751353 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (344387 virtual)\n",
      "I0228 01:40:12.758217 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (350057 virtual)\n",
      "I0228 01:40:12.772080 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (355993 virtual)\n",
      "I0228 01:40:12.782971 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (362153 virtual)\n",
      "I0228 01:40:12.788380 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (367539 virtual)\n",
      "I0228 01:40:12.814892 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (373207 virtual)\n",
      "I0228 01:40:12.819171 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (379108 virtual)\n",
      "I0228 01:40:12.832884 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (384947 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:12.853021 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (389876 virtual)\n",
      "I0228 01:40:12.858143 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (395028 virtual)\n",
      "I0228 01:40:12.873236 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398808 virtual)\n",
      "I0228 01:40:12.879811 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (402146 virtual)\n",
      "I0228 01:40:12.899278 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (406538 virtual)\n",
      "I0228 01:40:12.912013 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (410728 virtual)\n",
      "I0228 01:40:12.920860 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (415142 virtual)\n",
      "I0228 01:40:12.936893 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (419683 virtual)\n",
      "I0228 01:40:12.946559 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (424010 virtual)\n",
      "I0228 01:40:12.963695 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (428281 virtual)\n",
      "I0228 01:40:12.968325 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (432785 virtual)\n",
      "I0228 01:40:12.974092 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (437750 virtual)\n",
      "I0228 01:40:12.984822 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (442583 virtual)\n",
      "I0228 01:40:12.988117 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (447269 virtual)\n",
      "I0228 01:40:13.010129 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (451356 virtual)\n",
      "I0228 01:40:13.013821 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (455012 virtual)\n",
      "I0228 01:40:13.021206 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (459112 virtual)\n",
      "I0228 01:40:13.028229 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (462560 virtual)\n",
      "I0228 01:40:13.031301 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (466856 virtual)\n",
      "I0228 01:40:13.057226 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (471464 virtual)\n",
      "I0228 01:40:13.062233 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (476139 virtual)\n",
      "I0228 01:40:13.074838 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (480720 virtual)\n",
      "I0228 01:40:13.079652 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (485302 virtual)\n",
      "I0228 01:40:13.084432 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (489807 virtual)\n",
      "I0228 01:40:13.099365 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (494708 virtual)\n",
      "I0228 01:40:13.103464 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (499180 virtual)\n",
      "I0228 01:40:13.112745 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (503653 virtual)\n",
      "I0228 01:40:13.116195 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (508815 virtual)\n",
      "I0228 01:40:13.127315 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (513589 virtual)\n",
      "I0228 01:40:13.148899 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (518222 virtual)\n",
      "I0228 01:40:13.156092 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (522504 virtual)\n",
      "I0228 01:40:13.162892 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (527169 virtual)\n",
      "I0228 01:40:13.169319 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (531316 virtual)\n",
      "I0228 01:40:13.178961 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (535797 virtual)\n",
      "I0228 01:40:13.201431 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (539434 virtual)\n",
      "I0228 01:40:13.206281 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (543997 virtual)\n",
      "I0228 01:40:13.211166 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (548354 virtual)\n",
      "I0228 01:40:13.226113 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (552741 virtual)\n",
      "I0228 01:40:13.232684 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (557172 virtual)\n",
      "I0228 01:40:13.252537 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (561517 virtual)\n",
      "I0228 01:40:13.257566 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (565887 virtual)\n",
      "I0228 01:40:13.263206 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (570096 virtual)\n",
      "I0228 01:40:13.274188 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (574380 virtual)\n",
      "I0228 01:40:13.282103 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (578118 virtual)\n",
      "I0228 01:40:13.293113 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (581174 virtual)\n",
      "I0228 01:40:13.311264 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (584978 virtual)\n",
      "I0228 01:40:13.315302 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (590349 virtual)\n",
      "I0228 01:40:13.321320 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (596085 virtual)\n",
      "I0228 01:40:13.331860 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (601328 virtual)\n",
      "I0228 01:40:13.341334 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (606372 virtual)\n",
      "I0228 01:40:13.354292 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (611829 virtual)\n",
      "I0228 01:40:13.368422 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (616955 virtual)\n",
      "I0228 01:40:13.374892 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (622073 virtual)\n",
      "I0228 01:40:13.378681 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (626491 virtual)\n",
      "I0228 01:40:13.381915 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (630840 virtual)\n",
      "I0228 01:40:13.398430 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (635264 virtual)\n",
      "I0228 01:40:13.428343 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (640417 virtual)\n",
      "I0228 01:40:13.435919 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (645960 virtual)\n",
      "I0228 01:40:13.439928 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (651330 virtual)\n",
      "I0228 01:40:13.443223 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (656787 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:13.456895 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (661227 virtual)\n",
      "I0228 01:40:13.483553 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (666355 virtual)\n",
      "I0228 01:40:13.489429 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (671595 virtual)\n",
      "I0228 01:40:13.495280 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (676928 virtual)\n",
      "I0228 01:40:13.498741 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (682009 virtual)\n",
      "I0228 01:40:13.505139 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (686779 virtual)\n",
      "I0228 01:40:13.540565 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (692090 virtual)\n",
      "I0228 01:40:13.545607 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (696983 virtual)\n",
      "I0228 01:40:13.551228 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (702496 virtual)\n",
      "I0228 01:40:13.555455 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (707424 virtual)\n",
      "I0228 01:40:13.558366 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (712803 virtual)\n",
      "I0228 01:40:13.607398 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (717950 virtual)\n",
      "I0228 01:40:13.611479 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (723167 virtual)\n",
      "I0228 01:40:13.614335 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (728381 virtual)\n",
      "I0228 01:40:13.619421 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (733927 virtual)\n",
      "I0228 01:40:13.627236 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (739623 virtual)\n",
      "I0228 01:40:13.654096 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (744534 virtual)\n",
      "I0228 01:40:13.658489 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (749592 virtual)\n",
      "I0228 01:40:13.663081 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (755113 virtual)\n",
      "I0228 01:40:13.666892 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (760241 virtual)\n",
      "I0228 01:40:13.669734 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (765416 virtual)\n",
      "I0228 01:40:13.707057 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (771145 virtual)\n",
      "I0228 01:40:13.711449 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (776982 virtual)\n",
      "I0228 01:40:13.716191 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (782850 virtual)\n",
      "I0228 01:40:13.724689 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (788710 virtual)\n",
      "I0228 01:40:13.727815 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (794649 virtual)\n",
      "I0228 01:40:13.754905 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (800120 virtual)\n",
      "I0228 01:40:13.759546 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (805958 virtual)\n",
      "I0228 01:40:13.766560 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (811385 virtual)\n",
      "I0228 01:40:13.780407 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (817080 virtual)\n",
      "I0228 01:40:13.785431 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (822917 virtual)\n",
      "I0228 01:40:13.819054 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (828844 virtual)\n",
      "I0228 01:40:13.827136 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (834227 virtual)\n",
      "I0228 01:40:13.835834 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (839536 virtual)\n",
      "I0228 01:40:13.843014 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (845141 virtual)\n",
      "I0228 01:40:13.847174 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (851440 virtual)\n",
      "I0228 01:40:13.877818 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (857344 virtual)\n",
      "I0228 01:40:13.887943 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (863293 virtual)\n",
      "I0228 01:40:13.894649 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (869053 virtual)\n",
      "I0228 01:40:13.905814 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (874914 virtual)\n",
      "I0228 01:40:13.911428 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (880785 virtual)\n",
      "I0228 01:40:13.945168 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (886779 virtual)\n",
      "I0228 01:40:13.950016 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (892931 virtual)\n",
      "I0228 01:40:13.954690 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (898242 virtual)\n",
      "I0228 01:40:13.966048 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (903935 virtual)\n",
      "I0228 01:40:13.974977 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (911729 virtual)\n",
      "I0228 01:40:14.009833 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (916267 virtual)\n",
      "I0228 01:40:14.014385 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (920083 virtual)\n",
      "I0228 01:40:14.018351 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (922608 virtual)\n",
      "I0228 01:40:14.027377 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (928276 virtual)\n",
      "I0228 01:40:14.035019 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (933342 virtual)\n",
      "I0228 01:40:14.069110 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (937310 virtual)\n",
      "I0228 01:40:14.072827 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (941113 virtual)\n",
      "I0228 01:40:14.076740 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (945004 virtual)\n",
      "I0228 01:40:14.087866 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (948490 virtual)\n",
      "I0228 01:40:14.105504 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (952233 virtual)\n",
      "I0228 01:40:14.111974 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (956053 virtual)\n",
      "I0228 01:40:14.116861 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (959801 virtual)\n",
      "I0228 01:40:14.120737 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (963160 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:14.151506 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (967544 virtual)\n",
      "I0228 01:40:14.155835 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (971266 virtual)\n",
      "I0228 01:40:14.159846 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (974747 virtual)\n",
      "I0228 01:40:14.163724 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (978566 virtual)\n",
      "I0228 01:40:14.167589 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (982483 virtual)\n",
      "I0228 01:40:14.185969 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (985957 virtual)\n",
      "I0228 01:40:14.190077 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (989903 virtual)\n",
      "I0228 01:40:14.195482 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (993598 virtual)\n",
      "I0228 01:40:14.198897 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (997257 virtual)\n",
      "I0228 01:40:14.201594 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (1001243 virtual)\n",
      "I0228 01:40:14.228289 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1005208 virtual)\n",
      "I0228 01:40:14.231849 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1008653 virtual)\n",
      "I0228 01:40:14.235782 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1012607 virtual)\n",
      "I0228 01:40:14.239206 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1016811 virtual)\n",
      "I0228 01:40:14.242608 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1020911 virtual)\n",
      "I0228 01:40:14.262863 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1026083 virtual)\n",
      "I0228 01:40:14.266491 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1029909 virtual)\n",
      "I0228 01:40:14.270516 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1034126 virtual)\n",
      "I0228 01:40:14.274625 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1037862 virtual)\n",
      "I0228 01:40:14.279776 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1042711 virtual)\n",
      "I0228 01:40:14.298543 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1046385 virtual)\n",
      "I0228 01:40:14.304300 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1050218 virtual)\n",
      "I0228 01:40:14.310575 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1054547 virtual)\n",
      "I0228 01:40:14.317731 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1058670 virtual)\n",
      "I0228 01:40:14.321300 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1062415 virtual)\n",
      "I0228 01:40:14.343505 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1066394 virtual)\n",
      "I0228 01:40:14.352657 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1070328 virtual)\n",
      "I0228 01:40:14.356790 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1075117 virtual)\n",
      "I0228 01:40:14.360580 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1079216 virtual)\n",
      "I0228 01:40:14.369707 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1083412 virtual)\n",
      "I0228 01:40:14.380240 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1088316 virtual)\n",
      "I0228 01:40:14.393444 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1092435 virtual)\n",
      "I0228 01:40:14.397898 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1096200 virtual)\n",
      "I0228 01:40:14.401850 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1099737 virtual)\n",
      "I0228 01:40:14.406906 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1104193 virtual)\n",
      "I0228 01:40:14.419728 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1107969 virtual)\n",
      "I0228 01:40:14.434109 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1112460 virtual)\n",
      "I0228 01:40:14.441089 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1116677 virtual)\n",
      "I0228 01:40:14.446844 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1121461 virtual)\n",
      "I0228 01:40:14.453020 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1125860 virtual)\n",
      "I0228 01:40:14.461522 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1129916 virtual)\n",
      "I0228 01:40:14.474811 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1135134 virtual)\n",
      "I0228 01:40:14.478624 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1138533 virtual)\n",
      "I0228 01:40:14.483220 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1144387 virtual)\n",
      "I0228 01:40:14.497136 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1148913 virtual)\n",
      "I0228 01:40:14.500480 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1152592 virtual)\n",
      "I0228 01:40:14.519985 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1156248 virtual)\n",
      "I0228 01:40:14.524723 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1160661 virtual)\n",
      "I0228 01:40:14.534154 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1164853 virtual)\n",
      "I0228 01:40:14.537805 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1169353 virtual)\n",
      "I0228 01:40:14.543769 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1174111 virtual)\n",
      "I0228 01:40:14.557224 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1177933 virtual)\n",
      "I0228 01:40:14.574952 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1182385 virtual)\n",
      "I0228 01:40:14.582636 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1187424 virtual)\n",
      "I0228 01:40:14.587142 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1191861 virtual)\n",
      "I0228 01:40:14.592451 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1196541 virtual)\n",
      "I0228 01:40:14.599573 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1200512 virtual)\n",
      "I0228 01:40:14.621580 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1205055 virtual)\n",
      "I0228 01:40:14.626080 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1209074 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:14.631285 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1213325 virtual)\n",
      "I0228 01:40:14.635287 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1217396 virtual)\n",
      "I0228 01:40:14.640542 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1221875 virtual)\n",
      "I0228 01:40:14.664947 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1226483 virtual)\n",
      "I0228 01:40:14.670579 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1230080 virtual)\n",
      "I0228 01:40:14.675225 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1233427 virtual)\n",
      "I0228 01:40:14.677989 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1236691 virtual)\n",
      "I0228 01:40:14.682161 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1240966 virtual)\n",
      "I0228 01:40:14.706992 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1245024 virtual)\n",
      "I0228 01:40:14.711350 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1249976 virtual)\n",
      "I0228 01:40:14.715979 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1254745 virtual)\n",
      "I0228 01:40:14.720822 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1259292 virtual)\n",
      "I0228 01:40:14.724264 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1263317 virtual)\n",
      "I0228 01:40:14.743340 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1267333 virtual)\n",
      "I0228 01:40:14.747095 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1271425 virtual)\n",
      "I0228 01:40:14.750952 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1275304 virtual)\n",
      "I0228 01:40:14.754664 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1279069 virtual)\n",
      "I0228 01:40:14.764444 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1283599 virtual)\n",
      "I0228 01:40:14.778440 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1288118 virtual)\n",
      "I0228 01:40:14.793619 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1292059 virtual)\n",
      "I0228 01:40:14.798591 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1296673 virtual)\n",
      "I0228 01:40:14.802938 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1300501 virtual)\n",
      "I0228 01:40:14.806820 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1304007 virtual)\n",
      "I0228 01:40:14.818351 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1308734 virtual)\n",
      "I0228 01:40:14.833888 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1313593 virtual)\n",
      "I0228 01:40:14.838399 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1317270 virtual)\n",
      "I0228 01:40:14.842952 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1321240 virtual)\n",
      "I0228 01:40:14.846209 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1325515 virtual)\n",
      "I0228 01:40:14.863956 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1329159 virtual)\n",
      "I0228 01:40:14.871361 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1332821 virtual)\n",
      "I0228 01:40:14.875551 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1336872 virtual)\n",
      "I0228 01:40:14.878865 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1341016 virtual)\n",
      "I0228 01:40:14.883455 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1345576 virtual)\n",
      "I0228 01:40:14.906925 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1349951 virtual)\n",
      "I0228 01:40:14.911176 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1354301 virtual)\n",
      "I0228 01:40:14.916404 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1359588 virtual)\n",
      "I0228 01:40:14.920083 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1362039 virtual)\n",
      "I0228 01:40:14.923737 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1367415 virtual)\n",
      "I0228 01:40:14.943399 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1372595 virtual)\n",
      "I0228 01:40:14.947201 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1377005 virtual)\n",
      "I0228 01:40:14.952928 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1381831 virtual)\n",
      "I0228 01:40:14.956983 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1386065 virtual)\n",
      "I0228 01:40:14.969984 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1391330 virtual)\n",
      "I0228 01:40:14.983559 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1395144 virtual)\n",
      "I0228 01:40:14.988100 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1400182 virtual)\n",
      "I0228 01:40:14.995310 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1405205 virtual)\n",
      "I0228 01:40:15.011725 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1409351 virtual)\n",
      "I0228 01:40:15.027472 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1413923 virtual)\n",
      "I0228 01:40:15.032484 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1418820 virtual)\n",
      "I0228 01:40:15.037417 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1422765 virtual)\n",
      "I0228 01:40:15.041117 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1427567 virtual)\n",
      "I0228 01:40:15.053763 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1431996 virtual)\n",
      "I0228 01:40:15.067873 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1436613 virtual)\n",
      "I0228 01:40:15.081717 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1441411 virtual)\n",
      "I0228 01:40:15.088076 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1447354 virtual)\n",
      "I0228 01:40:15.092562 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1452310 virtual)\n",
      "I0228 01:40:15.097335 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1457410 virtual)\n",
      "I0228 01:40:15.114753 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1463081 virtual)\n",
      "I0228 01:40:15.127025 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1468124 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:15.131896 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1475222 virtual)\n",
      "I0228 01:40:15.146436 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1482907 virtual)\n",
      "I0228 01:40:15.149309 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1488104 virtual)\n",
      "I0228 01:40:15.161185 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1493243 virtual)\n",
      "I0228 01:40:15.174793 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1498517 virtual)\n",
      "I0228 01:40:15.186291 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1503644 virtual)\n",
      "I0228 01:40:15.193160 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1508984 virtual)\n",
      "I0228 01:40:15.197351 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1514088 virtual)\n",
      "I0228 01:40:15.217606 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1519001 virtual)\n",
      "I0228 01:40:15.222534 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1525668 virtual)\n",
      "I0228 01:40:15.249547 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1534640 virtual)\n",
      "I0228 01:40:15.258609 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1539925 virtual)\n",
      "I0228 01:40:15.263536 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1545853 virtual)\n",
      "I0228 01:40:15.267723 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1550829 virtual)\n",
      "I0228 01:40:15.276161 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1555309 virtual)\n",
      "I0228 01:40:15.304006 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1560020 virtual)\n",
      "I0228 01:40:15.312605 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1564761 virtual)\n",
      "I0228 01:40:15.326719 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1569695 virtual)\n",
      "I0228 01:40:15.329772 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1574553 virtual)\n",
      "I0228 01:40:15.342954 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1580021 virtual)\n",
      "I0228 01:40:15.365193 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1586174 virtual)\n",
      "I0228 01:40:15.369584 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1591959 virtual)\n",
      "I0228 01:40:15.374144 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1597555 virtual)\n",
      "I0228 01:40:15.389941 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1602978 virtual)\n",
      "I0228 01:40:15.394462 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1608950 virtual)\n",
      "I0228 01:40:15.414296 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1614881 virtual)\n",
      "I0228 01:40:15.418550 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1619468 virtual)\n",
      "I0228 01:40:15.425112 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1624907 virtual)\n",
      "I0228 01:40:15.435454 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1629797 virtual)\n",
      "I0228 01:40:15.448698 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1635176 virtual)\n",
      "I0228 01:40:15.476493 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1640359 virtual)\n",
      "I0228 01:40:15.480984 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1645923 virtual)\n",
      "I0228 01:40:15.485075 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1651014 virtual)\n",
      "I0228 01:40:15.489144 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1656809 virtual)\n",
      "I0228 01:40:15.508228 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1661699 virtual)\n",
      "I0228 01:40:15.526691 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1668043 virtual)\n",
      "I0228 01:40:15.532875 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1676177 virtual)\n",
      "I0228 01:40:15.538930 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1684820 virtual)\n",
      "I0228 01:40:15.542222 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1692952 virtual)\n",
      "I0228 01:40:15.559609 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1700919 virtual)\n",
      "I0228 01:40:15.578745 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1706837 virtual)\n",
      "I0228 01:40:15.587785 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1711639 virtual)\n",
      "I0228 01:40:15.592770 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1718206 virtual)\n",
      "I0228 01:40:15.600306 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1723765 virtual)\n",
      "I0228 01:40:15.610759 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1729807 virtual)\n",
      "I0228 01:40:15.649355 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1735939 virtual)\n",
      "I0228 01:40:15.675817 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1741557 virtual)\n",
      "I0228 01:40:15.685208 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1746505 virtual)\n",
      "I0228 01:40:15.688214 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1752070 virtual)\n",
      "I0228 01:40:15.699566 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1757324 virtual)\n",
      "I0228 01:40:15.717229 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1762501 virtual)\n",
      "I0228 01:40:15.725382 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1767330 virtual)\n",
      "I0228 01:40:15.742935 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1772477 virtual)\n",
      "I0228 01:40:15.753226 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1778037 virtual)\n",
      "I0228 01:40:15.761323 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1783368 virtual)\n",
      "I0228 01:40:15.781206 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1788120 virtual)\n",
      "I0228 01:40:15.785454 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1793051 virtual)\n",
      "I0228 01:40:15.792544 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1797957 virtual)\n",
      "I0228 01:40:15.816094 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1803104 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:15.820569 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1808209 virtual)\n",
      "I0228 01:40:15.835908 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1813744 virtual)\n",
      "I0228 01:40:15.840717 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1818698 virtual)\n",
      "I0228 01:40:15.850763 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1823847 virtual)\n",
      "I0228 01:40:15.876697 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1828976 virtual)\n",
      "I0228 01:40:15.881522 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1833772 virtual)\n",
      "I0228 01:40:15.886545 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1838795 virtual)\n",
      "I0228 01:40:15.894403 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1843983 virtual)\n",
      "I0228 01:40:15.907271 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1849049 virtual)\n",
      "I0228 01:40:15.930779 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1854734 virtual)\n",
      "I0228 01:40:15.936141 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1859500 virtual)\n",
      "I0228 01:40:15.943912 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1864475 virtual)\n",
      "I0228 01:40:15.947009 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1869216 virtual)\n",
      "I0228 01:40:15.962868 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1874395 virtual)\n",
      "I0228 01:40:15.985832 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1879231 virtual)\n",
      "I0228 01:40:15.991030 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1884331 virtual)\n",
      "I0228 01:40:16.002880 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1889287 virtual)\n",
      "I0228 01:40:16.005877 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1894562 virtual)\n",
      "I0228 01:40:16.019808 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1900058 virtual)\n",
      "I0228 01:40:16.047337 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1905205 virtual)\n",
      "I0228 01:40:16.051839 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1910028 virtual)\n",
      "I0228 01:40:16.056358 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1915616 virtual)\n",
      "I0228 01:40:16.060801 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1920620 virtual)\n",
      "I0228 01:40:16.077252 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1925304 virtual)\n",
      "I0228 01:40:16.106195 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1929899 virtual)\n",
      "I0228 01:40:16.111195 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1935069 virtual)\n",
      "I0228 01:40:16.116332 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1940261 virtual)\n",
      "I0228 01:40:16.121546 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1945217 virtual)\n",
      "I0228 01:40:16.137192 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1950356 virtual)\n",
      "I0228 01:40:16.156998 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1955262 virtual)\n",
      "I0228 01:40:16.166002 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1960457 virtual)\n",
      "I0228 01:40:16.170228 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1965792 virtual)\n",
      "I0228 01:40:16.178342 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1970700 virtual)\n",
      "I0228 01:40:16.188354 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1975969 virtual)\n",
      "I0228 01:40:16.206641 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1980917 virtual)\n",
      "I0228 01:40:16.222809 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1986221 virtual)\n",
      "I0228 01:40:16.228131 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (1991622 virtual)\n",
      "I0228 01:40:16.233012 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (1996556 virtual)\n",
      "I0228 01:40:16.246694 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (2001845 virtual)\n",
      "I0228 01:40:16.259419 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (2007126 virtual)\n",
      "I0228 01:40:16.279400 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (2012397 virtual)\n",
      "I0228 01:40:16.284987 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (2017670 virtual)\n",
      "I0228 01:40:16.290118 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2022891 virtual)\n",
      "I0228 01:40:16.307294 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2028241 virtual)\n",
      "I0228 01:40:16.313804 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2033198 virtual)\n",
      "I0228 01:40:16.336316 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2037631 virtual)\n",
      "I0228 01:40:16.340504 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2042930 virtual)\n",
      "I0228 01:40:16.343949 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2047634 virtual)\n",
      "I0228 01:40:16.366523 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2052212 virtual)\n",
      "I0228 01:40:16.372950 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2056997 virtual)\n",
      "I0228 01:40:16.393730 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2061924 virtual)\n",
      "I0228 01:40:16.398068 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2067156 virtual)\n",
      "I0228 01:40:16.402355 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2072179 virtual)\n",
      "I0228 01:40:16.424916 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2077455 virtual)\n",
      "I0228 01:40:16.429912 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2082598 virtual)\n",
      "I0228 01:40:16.441823 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2087665 virtual)\n",
      "I0228 01:40:16.453982 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2093156 virtual)\n",
      "I0228 01:40:16.460715 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2098300 virtual)\n",
      "I0228 01:40:16.478380 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2103841 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:16.486231 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2108719 virtual)\n",
      "I0228 01:40:16.497349 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2113552 virtual)\n",
      "I0228 01:40:16.511950 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2118363 virtual)\n",
      "I0228 01:40:16.516902 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2123429 virtual)\n",
      "I0228 01:40:16.535441 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2127869 virtual)\n",
      "I0228 01:40:16.547252 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2132967 virtual)\n",
      "I0228 01:40:16.552617 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2138365 virtual)\n",
      "I0228 01:40:16.571103 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2143410 virtual)\n",
      "I0228 01:40:16.575212 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2148983 virtual)\n",
      "I0228 01:40:16.597128 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2154038 virtual)\n",
      "I0228 01:40:16.605712 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2159221 virtual)\n",
      "I0228 01:40:16.610046 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2164022 virtual)\n",
      "I0228 01:40:16.622034 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2168994 virtual)\n",
      "I0228 01:40:16.627918 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2174218 virtual)\n",
      "I0228 01:40:16.647484 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2179308 virtual)\n",
      "I0228 01:40:16.669187 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2184186 virtual)\n",
      "I0228 01:40:16.686734 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2188893 virtual)\n",
      "I0228 01:40:16.696106 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2194366 virtual)\n",
      "I0228 01:40:16.699102 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2199360 virtual)\n",
      "I0228 01:40:16.702179 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2204644 virtual)\n",
      "I0228 01:40:16.721962 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2210331 virtual)\n",
      "I0228 01:40:16.734355 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2215212 virtual)\n",
      "I0228 01:40:16.743787 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2220493 virtual)\n",
      "I0228 01:40:16.747734 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2225556 virtual)\n",
      "I0228 01:40:16.753730 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2230367 virtual)\n",
      "I0228 01:40:16.777847 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2235479 virtual)\n",
      "I0228 01:40:16.790773 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2240754 virtual)\n",
      "I0228 01:40:16.802806 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2245704 virtual)\n",
      "I0228 01:40:16.807737 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2251056 virtual)\n",
      "I0228 01:40:16.812684 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2255820 virtual)\n",
      "I0228 01:40:16.842485 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2261246 virtual)\n",
      "I0228 01:40:16.851213 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2265916 virtual)\n",
      "I0228 01:40:16.861699 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2270947 virtual)\n",
      "I0228 01:40:16.865156 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2276003 virtual)\n",
      "I0228 01:40:16.868963 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2281034 virtual)\n",
      "I0228 01:40:16.899021 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2286250 virtual)\n",
      "I0228 01:40:16.913489 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2290792 virtual)\n",
      "I0228 01:40:16.918398 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2295818 virtual)\n",
      "I0228 01:40:16.923664 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2301067 virtual)\n",
      "I0228 01:40:16.926572 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2306240 virtual)\n",
      "I0228 01:40:16.957441 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2311268 virtual)\n",
      "I0228 01:40:16.965145 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2316522 virtual)\n",
      "I0228 01:40:16.973511 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2321728 virtual)\n",
      "I0228 01:40:16.977833 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2327105 virtual)\n",
      "I0228 01:40:16.981124 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2331950 virtual)\n",
      "I0228 01:40:17.017632 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2336988 virtual)\n",
      "I0228 01:40:17.023421 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2342255 virtual)\n",
      "I0228 01:40:17.034583 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2347916 virtual)\n",
      "I0228 01:40:17.039015 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2352910 virtual)\n",
      "I0228 01:40:17.043317 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2357961 virtual)\n",
      "I0228 01:40:17.071032 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2362645 virtual)\n",
      "I0228 01:40:17.075199 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2367552 virtual)\n",
      "I0228 01:40:17.092756 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2373090 virtual)\n",
      "I0228 01:40:17.097208 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2377877 virtual)\n",
      "I0228 01:40:17.101892 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2382898 virtual)\n",
      "I0228 01:40:17.124686 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2388646 virtual)\n",
      "I0228 01:40:17.130926 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2393437 virtual)\n",
      "I0228 01:40:17.150377 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2398554 virtual)\n",
      "I0228 01:40:17.154128 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2403502 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:17.159140 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2408567 virtual)\n",
      "I0228 01:40:17.176636 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2413467 virtual)\n",
      "I0228 01:40:17.185832 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2418513 virtual)\n",
      "I0228 01:40:17.208405 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2423842 virtual)\n",
      "I0228 01:40:17.213487 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2429143 virtual)\n",
      "I0228 01:40:17.218504 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2434239 virtual)\n",
      "I0228 01:40:17.238718 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2439212 virtual)\n",
      "I0228 01:40:17.242814 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2444390 virtual)\n",
      "I0228 01:40:17.262878 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2449666 virtual)\n",
      "I0228 01:40:17.268285 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2454270 virtual)\n",
      "I0228 01:40:17.272151 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2459256 virtual)\n",
      "I0228 01:40:17.294572 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2464486 virtual)\n",
      "I0228 01:40:17.298596 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2469586 virtual)\n",
      "I0228 01:40:17.324706 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2475075 virtual)\n",
      "I0228 01:40:17.339041 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2480169 virtual)\n",
      "I0228 01:40:17.342323 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2485292 virtual)\n",
      "I0228 01:40:17.349888 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2490647 virtual)\n",
      "I0228 01:40:17.352725 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2495218 virtual)\n",
      "I0228 01:40:17.378142 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2500030 virtual)\n",
      "I0228 01:40:17.383320 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2505000 virtual)\n",
      "I0228 01:40:17.388234 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2510702 virtual)\n",
      "I0228 01:40:17.401867 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2515918 virtual)\n",
      "I0228 01:40:17.410051 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2521127 virtual)\n",
      "I0228 01:40:17.436751 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2525856 virtual)\n",
      "I0228 01:40:17.441111 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2531160 virtual)\n",
      "I0228 01:40:17.451380 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2536146 virtual)\n",
      "I0228 01:40:17.456724 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2540993 virtual)\n",
      "I0228 01:40:17.458498 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2546058 virtual)\n",
      "I0228 01:40:17.494503 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2551091 virtual)\n",
      "I0228 01:40:17.506075 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2556133 virtual)\n",
      "I0228 01:40:17.513352 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2561381 virtual)\n",
      "I0228 01:40:17.518575 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2566087 virtual)\n",
      "I0228 01:40:17.521685 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2571611 virtual)\n",
      "I0228 01:40:17.545918 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2576995 virtual)\n",
      "I0228 01:40:17.565686 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2581826 virtual)\n",
      "I0228 01:40:17.570191 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2586780 virtual)\n",
      "I0228 01:40:17.574759 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2592324 virtual)\n",
      "I0228 01:40:17.577621 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2597697 virtual)\n",
      "I0228 01:40:17.599516 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2602980 virtual)\n",
      "I0228 01:40:17.621168 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2608378 virtual)\n",
      "I0228 01:40:17.626473 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2613636 virtual)\n",
      "I0228 01:40:17.632065 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2619138 virtual)\n",
      "I0228 01:40:17.636189 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2624137 virtual)\n",
      "I0228 01:40:17.660904 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2628974 virtual)\n",
      "I0228 01:40:17.676336 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2633951 virtual)\n",
      "I0228 01:40:17.683892 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2639190 virtual)\n",
      "I0228 01:40:17.692013 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2643909 virtual)\n",
      "I0228 01:40:17.695501 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2648853 virtual)\n",
      "I0228 01:40:17.719546 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2653724 virtual)\n",
      "I0228 01:40:17.732415 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2658706 virtual)\n",
      "I0228 01:40:17.743730 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2664083 virtual)\n",
      "I0228 01:40:17.749013 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2669141 virtual)\n",
      "I0228 01:40:17.754366 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2674755 virtual)\n",
      "I0228 01:40:17.772559 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2679832 virtual)\n",
      "I0228 01:40:17.788044 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2685335 virtual)\n",
      "I0228 01:40:17.801522 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2690379 virtual)\n",
      "I0228 01:40:17.806395 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2695828 virtual)\n",
      "I0228 01:40:17.811142 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2701067 virtual)\n",
      "I0228 01:40:17.825748 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2705963 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:40:17.844650 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2711061 virtual)\n",
      "I0228 01:40:17.858097 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2716228 virtual)\n",
      "I0228 01:40:17.863795 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2721670 virtual)\n",
      "I0228 01:40:17.870690 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2727232 virtual)\n",
      "I0228 01:40:17.883697 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2732887 virtual)\n",
      "I0228 01:40:17.902678 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2738131 virtual)\n",
      "I0228 01:40:17.913632 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2743160 virtual)\n",
      "I0228 01:40:17.918430 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2748231 virtual)\n",
      "I0228 01:40:17.930190 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2753148 virtual)\n",
      "I0228 01:40:17.942921 140370323834688 text_analysis.py:506] 552 batches submitted to accumulate stats from 35328 documents (2758378 virtual)\n",
      "I0228 01:40:17.960178 140370323834688 text_analysis.py:506] 553 batches submitted to accumulate stats from 35392 documents (2763882 virtual)\n",
      "I0228 01:40:17.969315 140370323834688 text_analysis.py:506] 554 batches submitted to accumulate stats from 35456 documents (2768927 virtual)\n",
      "I0228 01:40:17.976081 140370323834688 text_analysis.py:506] 555 batches submitted to accumulate stats from 35520 documents (2773784 virtual)\n",
      "I0228 01:40:17.990173 140370323834688 text_analysis.py:506] 556 batches submitted to accumulate stats from 35584 documents (2775322 virtual)\n",
      "I0228 01:40:18.059293 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:40:18.062843 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:40:18.076959 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:40:18.086679 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:40:18.088424 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:40:18.065261 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:40:18.112854 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:40:18.079856 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:40:18.063772 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:40:18.090107 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:40:18.494335 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:40:18.514883 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2775624 virtual documents\n",
      "/home/dhamzeia/Thesis/BiomedicalTopicModelling/contextualized_topic_models/models/ctm.py:511: Warning: This is an experimental feature that we has not been fully tested. Refer to the following issue:https://github.com/MilaNLProc/contextualized-topic-models/issues/38\n",
      "  Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores\n",
      "{'epoch': 149, 'cv': 0.6988838938241145, 'umass': -4.0431869275053085, 'uci': -0.002791064115426789, 'npmi': 0.07848748293754446, 'rbo': 1.0, 'td': 1.0, 'train_loss': 642.1069097420765, 'topics': [['c0267454', 'ref', 'salivary', 'heparin', 'c0020933', 'c0605290', 'c1446219', 'c0536858', 'c0521990', 'c0851891', 'c0052432', 'c0038174', 'excretion', 'seventh', 'c0117438', 'shareholder', 'c0444584', 'somatic', 'c0751651', 'swell', 'c0007789', 'c0032821', 'c0524909', 'appreciable', 'cow', 'c1256763'], ['c0011900', 'c0012634', 'c0032285', 'c0015967', 'child', 'c1457887', 'clinical', 'c0546788', 'c0003232', 'common', 'c0035236', 'sars-cov-2', 'c0010200', 'respiratory', 'c0019993', 'c0221423', 'cause', 'c3714514', 'manifestation', 'disclosure', 'c0010076', 'presentation', 'c0809949', 'c0038410', 'c0027442', 'c0035235'], ['c0543467', 'c0025080', 'postoperative', 'c0005898', 'c0031150', 'undergo', 'c0038930', 'c0728940', 'operative', 'c0002940', 'c0009566', 'procedure', 'c0229962', 'perform', 'c0850292', 'c0582175', 'c0547070', 'conversion', 'c0014245', 'surgical', 'perioperative', 'c0019080', 'recurrence', 'preoperative', 'c0162522', 'c1522577'], ['compare', 'c0243095', 'c0199470', 'difference', 'c0032042', 'significantly', 'significant', 'c0034108', 'receive', 'decrease', 'c0021708', 'c0369768', 'c0005516', 'concentration', 'confidence', 'assess', 'measurement', 'infant', 'versus', 'determine', 'odds', 'regression', 'predictor', 'association', 'associate', 'c0038454'], ['crisis', 'policy', 'political', 'threat', 'economic', 'disaster', 'market', 'economy', 'public', 'food', 'supply', 'face', 'emergency', 'argue', 'draw', 'sector', 'international', 'c1561598', 'c0018104', 'challenge', 'governance', 'innovation', 'national', 'society', 'c0242456', 'trade'], ['c0042210', 'c1254351', 'c1514562', 'c1167622', 'c0030956', 'c0029224', 'c0003320', 'c0003316', 'c0003250', 'c0020971', 'affinity', 'active', 'c1706082', 'nanoparticles', 'bind', 'c0014442', 'potential', 'elicit', 'c0003241', 'potent', 'candidate', 'drug', 'c3687832', 'c0034861', 'c0017968', 'c0678594'], ['c1171362', 'activation', 'c0025929', 'mechanism', 'c0007634', 'c0007613', 'c0079189', 'role', 'c0017262', 'induce', 'c0024432', 'c3539881', 'c0021747', 'activate', 'c0162638', 'pathway', 'c0021368', 'c0039194', 'mouse', 'c0023810', 'c1101610', 'c0035696', 'c0013081', 'induction', 'c0014597', 'release'], ['propose', 'c0002045', 'c3161035', 'machine', 'c0025663', 'c0150098', 'automate', 'c0679083', 'accuracy', 'input', 'performance', 'prediction', 'compute', 'sensor', 'c0037585', 'image', 'representation', 'solve', 'algorithm', 'outperform', 'c0037589', 'c1704254', 'computational', 'learn', 'filter', 'optimization'], ['c0679646', 'search', 'c2603343', 'conduct', 'c0242356', 'report', 'train', 'c0025353', 'include', 'c1257890', 'c0027361', 'c0242481', 'c0086388', 'impact', 'c0038951', 'c0003467', 'evidence', 'psychological', 'c0184661', 'c0030971', 'recommendation', 'parent', 'c1706852', 'c0282122', 'physical', 'measure'], ['c1705920', 'c0042776', 'c0032098', 'c0684063', 'c0017428', 'sample', 'c0012984', 'c0003062', 'c0007452', 'genetic', 'c0017337', 'c0005595', 'c0039005', 'c0442726', 'c0017446', 'c1764827', 'specie', 'genotype', 'isolate', 'c0086418', 'c0015733', 'diversity', 'c0242781', 'host', 'c0162326', 'c0029347']]}\n",
      "Epoch: [151/250]\tSamples: [5545626/9181500]\tTrain Loss: 642.0220262892774\tTime: 0:00:04.390355\n",
      "Epoch: [152/250]\tSamples: [5582352/9181500]\tTrain Loss: 641.9736793458653\tTime: 0:00:04.506871\n",
      "Epoch: [153/250]\tSamples: [5619078/9181500]\tTrain Loss: 641.8869152005732\tTime: 0:00:04.436609\n",
      "Epoch: [154/250]\tSamples: [5655804/9181500]\tTrain Loss: 642.0393864785847\tTime: 0:00:04.518233\n",
      "Epoch: [155/250]\tSamples: [5692530/9181500]\tTrain Loss: 642.0697385707673\tTime: 0:00:04.748400\n",
      "Epoch: [156/250]\tSamples: [5729256/9181500]\tTrain Loss: 641.8044089379799\tTime: 0:00:04.756332\n",
      "Epoch: [157/250]\tSamples: [5765982/9181500]\tTrain Loss: 641.9188579361352\tTime: 0:00:04.789201\n",
      "Epoch: [158/250]\tSamples: [5802708/9181500]\tTrain Loss: 641.9891771369398\tTime: 0:00:04.754150\n",
      "Epoch: [159/250]\tSamples: [5839434/9181500]\tTrain Loss: 641.8982574291714\tTime: 0:00:04.770263\n",
      "Epoch: [160/250]\tSamples: [5876160/9181500]\tTrain Loss: 642.0157543361651\tTime: 0:00:04.723582\n",
      "Epoch: [161/250]\tSamples: [5912886/9181500]\tTrain Loss: 641.9088321497372\tTime: 0:00:04.735654\n",
      "Epoch: [162/250]\tSamples: [5949612/9181500]\tTrain Loss: 641.8981180949804\tTime: 0:00:04.790461\n",
      "Epoch: [163/250]\tSamples: [5986338/9181500]\tTrain Loss: 641.9417915994459\tTime: 0:00:04.764103\n",
      "Epoch: [164/250]\tSamples: [6023064/9181500]\tTrain Loss: 641.9519338522095\tTime: 0:00:04.780970\n",
      "Epoch: [165/250]\tSamples: [6059790/9181500]\tTrain Loss: 642.0559977732056\tTime: 0:00:04.806118\n",
      "Epoch: [166/250]\tSamples: [6096516/9181500]\tTrain Loss: 641.8423856949096\tTime: 0:00:04.747254\n",
      "Epoch: [167/250]\tSamples: [6133242/9181500]\tTrain Loss: 641.948444328439\tTime: 0:00:04.770623\n",
      "Epoch: [168/250]\tSamples: [6169968/9181500]\tTrain Loss: 641.8093662570454\tTime: 0:00:04.753345\n",
      "Epoch: [169/250]\tSamples: [6206694/9181500]\tTrain Loss: 641.8828040974038\tTime: 0:00:04.774547\n",
      "Epoch: [170/250]\tSamples: [6243420/9181500]\tTrain Loss: 641.7321653299093\tTime: 0:00:04.731696\n",
      "Epoch: [171/250]\tSamples: [6280146/9181500]\tTrain Loss: 641.9289741302143\tTime: 0:00:04.792738\n",
      "Epoch: [172/250]\tSamples: [6316872/9181500]\tTrain Loss: 641.8709723909832\tTime: 0:00:04.807178\n",
      "Epoch: [173/250]\tSamples: [6353598/9181500]\tTrain Loss: 641.9606533007951\tTime: 0:00:04.762030\n",
      "Epoch: [174/250]\tSamples: [6390324/9181500]\tTrain Loss: 641.9014232934434\tTime: 0:00:04.821053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:18.757591 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [175/250]\tSamples: [6427050/9181500]\tTrain Loss: 642.0106920378274\tTime: 0:00:04.747342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:19.589153 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:42:20.275936 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:42:21.087671 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:42:21.619089 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:42:21.626379 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:42:22.377028 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:42:23.046781 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:42:23.849823 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:42:24.371287 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:42:24.376498 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:42:25.126343 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:42:25.800240 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:42:26.597764 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:42:27.116902 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:42:27.123308 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:42:27.871794 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:42:28.542635 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:42:29.337513 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:42:29.859178 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:42:29.881433 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:42:30.401844 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (-34922 virtual)\n",
      "I0228 01:42:30.467963 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (-50697 virtual)\n",
      "I0228 01:42:30.778299 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (-206536 virtual)\n",
      "I0228 01:42:30.800698 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (-215000 virtual)\n",
      "I0228 01:42:31.088180 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (-482749 virtual)\n",
      "I0228 01:42:31.127813 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (-487828 virtual)\n",
      "I0228 01:42:31.215427 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (-512362 virtual)\n",
      "I0228 01:42:31.219870 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (-510165 virtual)\n",
      "I0228 01:42:31.223869 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (-508293 virtual)\n",
      "I0228 01:42:31.227810 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (-506801 virtual)\n",
      "I0228 01:42:31.234359 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (-508351 virtual)\n",
      "I0228 01:42:31.889286 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:42:31.892672 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:42:31.896729 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:42:31.900207 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:42:31.900436 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:42:31.893520 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:42:31.907563 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:42:31.896385 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:42:31.904493 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:42:31.902386 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:42:32.321169 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:42:32.347417 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 229310 virtual documents\n",
      "I0228 01:42:34.241457 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 1000 documents\n",
      "I0228 01:42:34.256945 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 2000 documents\n",
      "I0228 01:42:34.272758 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 3000 documents\n",
      "I0228 01:42:34.289878 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 4000 documents\n",
      "I0228 01:42:34.304730 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 5000 documents\n",
      "I0228 01:42:34.317661 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 6000 documents\n",
      "I0228 01:42:34.330891 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 7000 documents\n",
      "I0228 01:42:34.344159 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 8000 documents\n",
      "I0228 01:42:34.358490 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 9000 documents\n",
      "I0228 01:42:34.373664 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 10000 documents\n",
      "I0228 01:42:34.389594 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 11000 documents\n",
      "I0228 01:42:34.403936 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 12000 documents\n",
      "I0228 01:42:34.415390 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 13000 documents\n",
      "I0228 01:42:34.427466 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 14000 documents\n",
      "I0228 01:42:34.439720 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 15000 documents\n",
      "I0228 01:42:34.452068 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 16000 documents\n",
      "I0228 01:42:34.463463 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 17000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:34.475956 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 18000 documents\n",
      "I0228 01:42:34.487772 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 19000 documents\n",
      "I0228 01:42:34.500821 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 20000 documents\n",
      "I0228 01:42:34.515110 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 21000 documents\n",
      "I0228 01:42:34.530176 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 22000 documents\n",
      "I0228 01:42:34.546168 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 23000 documents\n",
      "I0228 01:42:34.561906 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 24000 documents\n",
      "I0228 01:42:34.576593 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 25000 documents\n",
      "I0228 01:42:34.591183 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 26000 documents\n",
      "I0228 01:42:34.605918 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 27000 documents\n",
      "I0228 01:42:34.620435 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 28000 documents\n",
      "I0228 01:42:34.635020 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 29000 documents\n",
      "I0228 01:42:34.649868 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 30000 documents\n",
      "I0228 01:42:34.664577 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 31000 documents\n",
      "I0228 01:42:34.679247 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 32000 documents\n",
      "I0228 01:42:34.695376 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 33000 documents\n",
      "I0228 01:42:34.710416 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 34000 documents\n",
      "I0228 01:42:34.725579 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 35000 documents\n",
      "I0228 01:42:34.740493 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 36000 documents\n",
      "I0228 01:42:34.892475 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:42:35.278433 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (4980 virtual)\n",
      "I0228 01:42:35.283167 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10307 virtual)\n",
      "I0228 01:42:35.286599 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16414 virtual)\n",
      "I0228 01:42:35.290215 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21227 virtual)\n",
      "I0228 01:42:35.293854 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27031 virtual)\n",
      "I0228 01:42:35.296539 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32232 virtual)\n",
      "I0228 01:42:35.300902 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37799 virtual)\n",
      "I0228 01:42:35.303254 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43526 virtual)\n",
      "I0228 01:42:35.305293 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48755 virtual)\n",
      "I0228 01:42:35.307418 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54476 virtual)\n",
      "I0228 01:42:35.338034 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60339 virtual)\n",
      "I0228 01:42:35.346443 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65654 virtual)\n",
      "I0228 01:42:35.350860 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70702 virtual)\n",
      "I0228 01:42:35.353384 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (75991 virtual)\n",
      "I0228 01:42:35.370627 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81605 virtual)\n",
      "I0228 01:42:35.396039 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87682 virtual)\n",
      "I0228 01:42:35.408221 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93571 virtual)\n",
      "I0228 01:42:35.413177 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (99157 virtual)\n",
      "I0228 01:42:35.418168 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104215 virtual)\n",
      "I0228 01:42:35.434609 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109677 virtual)\n",
      "I0228 01:42:35.456541 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (115196 virtual)\n",
      "I0228 01:42:35.464361 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120433 virtual)\n",
      "I0228 01:42:35.468401 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (126047 virtual)\n",
      "I0228 01:42:35.473637 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (131983 virtual)\n",
      "I0228 01:42:35.501777 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137553 virtual)\n",
      "I0228 01:42:35.520833 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (142928 virtual)\n",
      "I0228 01:42:35.526873 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148409 virtual)\n",
      "I0228 01:42:35.529883 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154567 virtual)\n",
      "I0228 01:42:35.532880 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160542 virtual)\n",
      "I0228 01:42:35.557285 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (165931 virtual)\n",
      "I0228 01:42:35.579231 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (171254 virtual)\n",
      "I0228 01:42:35.582209 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176788 virtual)\n",
      "I0228 01:42:35.588835 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (182043 virtual)\n",
      "I0228 01:42:35.592095 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (187413 virtual)\n",
      "I0228 01:42:35.624736 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (193231 virtual)\n",
      "I0228 01:42:35.637822 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (198451 virtual)\n",
      "I0228 01:42:35.643312 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (204204 virtual)\n",
      "I0228 01:42:35.653033 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (209778 virtual)\n",
      "I0228 01:42:35.657292 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (215112 virtual)\n",
      "I0228 01:42:35.681827 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220832 virtual)\n",
      "I0228 01:42:35.695020 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (226380 virtual)\n",
      "I0228 01:42:35.706212 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (233878 virtual)\n",
      "I0228 01:42:35.709626 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (239209 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:35.715147 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (245096 virtual)\n",
      "I0228 01:42:35.745503 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (250661 virtual)\n",
      "I0228 01:42:35.750968 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (255927 virtual)\n",
      "I0228 01:42:35.768699 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (261308 virtual)\n",
      "I0228 01:42:35.772500 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (266505 virtual)\n",
      "I0228 01:42:35.776858 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (272057 virtual)\n",
      "I0228 01:42:35.804574 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (277470 virtual)\n",
      "I0228 01:42:35.809620 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (282946 virtual)\n",
      "I0228 01:42:35.829608 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288657 virtual)\n",
      "I0228 01:42:35.834113 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (293856 virtual)\n",
      "I0228 01:42:35.838100 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (299308 virtual)\n",
      "I0228 01:42:35.863510 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304635 virtual)\n",
      "I0228 01:42:35.867763 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (310215 virtual)\n",
      "I0228 01:42:35.889188 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (315884 virtual)\n",
      "I0228 01:42:35.893876 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (321584 virtual)\n",
      "I0228 01:42:35.898688 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (326898 virtual)\n",
      "I0228 01:42:35.922768 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (333303 virtual)\n",
      "I0228 01:42:35.927412 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (339080 virtual)\n",
      "I0228 01:42:35.946322 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (344744 virtual)\n",
      "I0228 01:42:35.950741 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (350429 virtual)\n",
      "I0228 01:42:35.956476 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (356369 virtual)\n",
      "I0228 01:42:35.981805 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (362315 virtual)\n",
      "I0228 01:42:35.986088 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (367915 virtual)\n",
      "I0228 01:42:36.006522 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (373632 virtual)\n",
      "I0228 01:42:36.011621 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (379415 virtual)\n",
      "I0228 01:42:36.017242 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (385392 virtual)\n",
      "I0228 01:42:36.045223 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (390146 virtual)\n",
      "I0228 01:42:36.054888 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (395335 virtual)\n",
      "I0228 01:42:36.065854 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398984 virtual)\n",
      "I0228 01:42:36.069768 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (402435 virtual)\n",
      "I0228 01:42:36.075499 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (406965 virtual)\n",
      "I0228 01:42:36.107053 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (411064 virtual)\n",
      "I0228 01:42:36.111897 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (415460 virtual)\n",
      "I0228 01:42:36.128054 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (420008 virtual)\n",
      "I0228 01:42:36.133696 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (424304 virtual)\n",
      "I0228 01:42:36.139349 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (428498 virtual)\n",
      "I0228 01:42:36.153422 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (433158 virtual)\n",
      "I0228 01:42:36.161705 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (438091 virtual)\n",
      "I0228 01:42:36.176119 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (442750 virtual)\n",
      "I0228 01:42:36.180016 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (447651 virtual)\n",
      "I0228 01:42:36.185996 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (451744 virtual)\n",
      "I0228 01:42:36.194422 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (455278 virtual)\n",
      "I0228 01:42:36.205624 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (459300 virtual)\n",
      "I0228 01:42:36.216160 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (463125 virtual)\n",
      "I0228 01:42:36.220947 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (467367 virtual)\n",
      "I0228 01:42:36.234945 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (472205 virtual)\n",
      "I0228 01:42:36.240728 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (476889 virtual)\n",
      "I0228 01:42:36.256346 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (481346 virtual)\n",
      "I0228 01:42:36.264612 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (486015 virtual)\n",
      "I0228 01:42:36.270842 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (490744 virtual)\n",
      "I0228 01:42:36.275053 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (495501 virtual)\n",
      "I0228 01:42:36.279309 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (499809 virtual)\n",
      "I0228 01:42:36.301393 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (504139 virtual)\n",
      "I0228 01:42:36.305197 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (509497 virtual)\n",
      "I0228 01:42:36.312691 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (514134 virtual)\n",
      "I0228 01:42:36.325991 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (518953 virtual)\n",
      "I0228 01:42:36.329794 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (523097 virtual)\n",
      "I0228 01:42:36.341844 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (527912 virtual)\n",
      "I0228 01:42:36.356981 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (531673 virtual)\n",
      "I0228 01:42:36.361840 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (536365 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:36.374259 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (539956 virtual)\n",
      "I0228 01:42:36.378378 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (544483 virtual)\n",
      "I0228 01:42:36.387007 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (549041 virtual)\n",
      "I0228 01:42:36.416686 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (553296 virtual)\n",
      "I0228 01:42:36.420673 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (557966 virtual)\n",
      "I0228 01:42:36.424075 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (561825 virtual)\n",
      "I0228 01:42:36.427782 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (566442 virtual)\n",
      "I0228 01:42:36.440248 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (570529 virtual)\n",
      "I0228 01:42:36.451776 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (574738 virtual)\n",
      "I0228 01:42:36.457762 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (578572 virtual)\n",
      "I0228 01:42:36.469540 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (581593 virtual)\n",
      "I0228 01:42:36.478105 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (586042 virtual)\n",
      "I0228 01:42:36.484371 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (591329 virtual)\n",
      "I0228 01:42:36.494124 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (596986 virtual)\n",
      "I0228 01:42:36.507978 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (601923 virtual)\n",
      "I0228 01:42:36.513558 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (607194 virtual)\n",
      "I0228 01:42:36.526671 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (612977 virtual)\n",
      "I0228 01:42:36.530049 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (617983 virtual)\n",
      "I0228 01:42:36.537396 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (622536 virtual)\n",
      "I0228 01:42:36.546442 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (627363 virtual)\n",
      "I0228 01:42:36.550360 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (631222 virtual)\n",
      "I0228 01:42:36.575654 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (636219 virtual)\n",
      "I0228 01:42:36.584327 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (641455 virtual)\n",
      "I0228 01:42:36.597237 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (646867 virtual)\n",
      "I0228 01:42:36.601765 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (652460 virtual)\n",
      "I0228 01:42:36.606431 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (657498 virtual)\n",
      "I0228 01:42:36.637524 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (662072 virtual)\n",
      "I0228 01:42:36.642457 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (667307 virtual)\n",
      "I0228 01:42:36.648215 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (672679 virtual)\n",
      "I0228 01:42:36.652241 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (677837 virtual)\n",
      "I0228 01:42:36.655089 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (682763 virtual)\n",
      "I0228 01:42:36.690983 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (687765 virtual)\n",
      "I0228 01:42:36.695685 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (692951 virtual)\n",
      "I0228 01:42:36.708194 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (697930 virtual)\n",
      "I0228 01:42:36.711673 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (703283 virtual)\n",
      "I0228 01:42:36.714612 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (708432 virtual)\n",
      "I0228 01:42:36.735114 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (713607 virtual)\n",
      "I0228 01:42:36.748814 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (719113 virtual)\n",
      "I0228 01:42:36.755442 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (723999 virtual)\n",
      "I0228 01:42:36.760083 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (729559 virtual)\n",
      "I0228 01:42:36.764947 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (735007 virtual)\n",
      "I0228 01:42:36.787200 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (740677 virtual)\n",
      "I0228 01:42:36.802935 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (745536 virtual)\n",
      "I0228 01:42:36.809896 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (750897 virtual)\n",
      "I0228 01:42:36.814473 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (755938 virtual)\n",
      "I0228 01:42:36.817416 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (761538 virtual)\n",
      "I0228 01:42:36.838399 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (766495 virtual)\n",
      "I0228 01:42:36.857738 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (772597 virtual)\n",
      "I0228 01:42:36.863458 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (778412 virtual)\n",
      "I0228 01:42:36.869362 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (784205 virtual)\n",
      "I0228 01:42:36.874766 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (790027 virtual)\n",
      "I0228 01:42:36.896146 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (796044 virtual)\n",
      "I0228 01:42:36.903112 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (801581 virtual)\n",
      "I0228 01:42:36.908888 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (807153 virtual)\n",
      "I0228 01:42:36.917372 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (812950 virtual)\n",
      "I0228 01:42:36.935332 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (818563 virtual)\n",
      "I0228 01:42:36.946893 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (824800 virtual)\n",
      "I0228 01:42:36.970144 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (830315 virtual)\n",
      "I0228 01:42:36.975431 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (835534 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:36.979518 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (841208 virtual)\n",
      "I0228 01:42:36.995682 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (847189 virtual)\n",
      "I0228 01:42:37.005876 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (853256 virtual)\n",
      "I0228 01:42:37.031413 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (859126 virtual)\n",
      "I0228 01:42:37.034497 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (865227 virtual)\n",
      "I0228 01:42:37.039304 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (870813 virtual)\n",
      "I0228 01:42:37.054674 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (876939 virtual)\n",
      "I0228 01:42:37.071785 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (882665 virtual)\n",
      "I0228 01:42:37.090621 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (888779 virtual)\n",
      "I0228 01:42:37.093618 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (894475 virtual)\n",
      "I0228 01:42:37.100481 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (900206 virtual)\n",
      "I0228 01:42:37.112781 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (907064 virtual)\n",
      "I0228 01:42:37.132184 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (912823 virtual)\n",
      "I0228 01:42:37.148046 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (917046 virtual)\n",
      "I0228 01:42:37.152735 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (920344 virtual)\n",
      "I0228 01:42:37.159056 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (923869 virtual)\n",
      "I0228 01:42:37.175232 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (930600 virtual)\n",
      "I0228 01:42:37.188249 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (934424 virtual)\n",
      "I0228 01:42:37.206494 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (938246 virtual)\n",
      "I0228 01:42:37.211414 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (942172 virtual)\n",
      "I0228 01:42:37.216539 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (945680 virtual)\n",
      "I0228 01:42:37.244348 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (949451 virtual)\n",
      "I0228 01:42:37.248774 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (953331 virtual)\n",
      "I0228 01:42:37.253357 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (956910 virtual)\n",
      "I0228 01:42:37.257073 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (960163 virtual)\n",
      "I0228 01:42:37.259820 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (964510 virtual)\n",
      "I0228 01:42:37.283268 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (968438 virtual)\n",
      "I0228 01:42:37.286774 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (972126 virtual)\n",
      "I0228 01:42:37.290354 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (975905 virtual)\n",
      "I0228 01:42:37.293850 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (979789 virtual)\n",
      "I0228 01:42:37.317073 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (983403 virtual)\n",
      "I0228 01:42:37.320700 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (987236 virtual)\n",
      "I0228 01:42:37.323745 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (990890 virtual)\n",
      "I0228 01:42:37.326816 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (994640 virtual)\n",
      "I0228 01:42:37.329954 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (998673 virtual)\n",
      "I0228 01:42:37.357537 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (1002711 virtual)\n",
      "I0228 01:42:37.361978 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1006309 virtual)\n",
      "I0228 01:42:37.367604 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1010275 virtual)\n",
      "I0228 01:42:37.370594 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1014148 virtual)\n",
      "I0228 01:42:37.373255 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1018510 virtual)\n",
      "I0228 01:42:37.391849 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1023786 virtual)\n",
      "I0228 01:42:37.395638 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1027612 virtual)\n",
      "I0228 01:42:37.399482 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1031829 virtual)\n",
      "I0228 01:42:37.402817 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1035498 virtual)\n",
      "I0228 01:42:37.405652 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1040387 virtual)\n",
      "I0228 01:42:37.430129 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1044053 virtual)\n",
      "I0228 01:42:37.433985 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1048058 virtual)\n",
      "I0228 01:42:37.438385 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1052232 virtual)\n",
      "I0228 01:42:37.442878 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1056440 virtual)\n",
      "I0228 01:42:37.446351 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1060311 virtual)\n",
      "I0228 01:42:37.469581 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1064161 virtual)\n",
      "I0228 01:42:37.473533 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1068262 virtual)\n",
      "I0228 01:42:37.477753 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1072912 virtual)\n",
      "I0228 01:42:37.480746 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1077243 virtual)\n",
      "I0228 01:42:37.490886 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1081498 virtual)\n",
      "I0228 01:42:37.505220 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1086180 virtual)\n",
      "I0228 01:42:37.511604 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1090335 virtual)\n",
      "I0228 01:42:37.515791 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1094050 virtual)\n",
      "I0228 01:42:37.519736 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1097759 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:37.528393 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1102096 virtual)\n",
      "I0228 01:42:37.541172 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1105777 virtual)\n",
      "I0228 01:42:37.552167 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1110490 virtual)\n",
      "I0228 01:42:37.556579 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1114582 virtual)\n",
      "I0228 01:42:37.561789 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1119382 virtual)\n",
      "I0228 01:42:37.571914 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1123482 virtual)\n",
      "I0228 01:42:37.580829 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1128043 virtual)\n",
      "I0228 01:42:37.591376 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1132999 virtual)\n",
      "I0228 01:42:37.595558 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1136743 virtual)\n",
      "I0228 01:42:37.600553 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1142546 virtual)\n",
      "I0228 01:42:37.613420 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1146615 virtual)\n",
      "I0228 01:42:37.616048 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1150692 virtual)\n",
      "I0228 01:42:37.634975 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1154507 virtual)\n",
      "I0228 01:42:37.640031 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1159156 virtual)\n",
      "I0228 01:42:37.644966 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1162910 virtual)\n",
      "I0228 01:42:37.654138 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1167838 virtual)\n",
      "I0228 01:42:37.659396 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1172459 virtual)\n",
      "I0228 01:42:37.678068 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1176254 virtual)\n",
      "I0228 01:42:37.683259 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1181032 virtual)\n",
      "I0228 01:42:37.694777 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1186118 virtual)\n",
      "I0228 01:42:37.698622 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1189859 virtual)\n",
      "I0228 01:42:37.711822 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1194427 virtual)\n",
      "I0228 01:42:37.719176 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1199082 virtual)\n",
      "I0228 01:42:37.729005 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1203290 virtual)\n",
      "I0228 01:42:37.732624 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1207322 virtual)\n",
      "I0228 01:42:37.748977 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1211930 virtual)\n",
      "I0228 01:42:37.754307 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1216084 virtual)\n",
      "I0228 01:42:37.759013 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1220527 virtual)\n",
      "I0228 01:42:37.773416 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1224394 virtual)\n",
      "I0228 01:42:37.778090 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1228204 virtual)\n",
      "I0228 01:42:37.783504 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1231065 virtual)\n",
      "I0228 01:42:37.797616 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1235357 virtual)\n",
      "I0228 01:42:37.801664 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1239531 virtual)\n",
      "I0228 01:42:37.807387 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1244046 virtual)\n",
      "I0228 01:42:37.815472 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1249352 virtual)\n",
      "I0228 01:42:37.829960 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1253399 virtual)\n",
      "I0228 01:42:37.836489 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1257562 virtual)\n",
      "I0228 01:42:37.840666 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1261774 virtual)\n",
      "I0228 01:42:37.844942 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1265915 virtual)\n",
      "I0228 01:42:37.849504 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1269831 virtual)\n",
      "I0228 01:42:37.854223 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1273689 virtual)\n",
      "I0228 01:42:37.876105 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1277718 virtual)\n",
      "I0228 01:42:37.881217 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1282590 virtual)\n",
      "I0228 01:42:37.888376 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1286297 virtual)\n",
      "I0228 01:42:37.893972 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1290651 virtual)\n",
      "I0228 01:42:37.900529 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1295260 virtual)\n",
      "I0228 01:42:37.915585 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1298637 virtual)\n",
      "I0228 01:42:37.919896 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1302955 virtual)\n",
      "I0228 01:42:37.928367 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1307883 virtual)\n",
      "I0228 01:42:37.935650 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1312149 virtual)\n",
      "I0228 01:42:37.938889 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1315814 virtual)\n",
      "I0228 01:42:37.952113 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1320339 virtual)\n",
      "I0228 01:42:37.963320 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1324065 virtual)\n",
      "I0228 01:42:37.968155 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1327407 virtual)\n",
      "I0228 01:42:37.973906 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1331461 virtual)\n",
      "I0228 01:42:37.979676 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1335819 virtual)\n",
      "I0228 01:42:37.984750 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1340116 virtual)\n",
      "I0228 01:42:38.002295 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1344529 virtual)\n",
      "I0228 01:42:38.010021 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1348714 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:38.014061 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1353779 virtual)\n",
      "I0228 01:42:38.016997 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1356990 virtual)\n",
      "I0228 01:42:38.026392 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1360571 virtual)\n",
      "I0228 01:42:38.037297 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1365840 virtual)\n",
      "I0228 01:42:38.042916 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1371100 virtual)\n",
      "I0228 01:42:38.048775 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1375851 virtual)\n",
      "I0228 01:42:38.058160 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1380112 virtual)\n",
      "I0228 01:42:38.070341 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1384630 virtual)\n",
      "I0228 01:42:38.077646 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1389012 virtual)\n",
      "I0228 01:42:38.082490 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1393645 virtual)\n",
      "I0228 01:42:38.092586 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1398648 virtual)\n",
      "I0228 01:42:38.100207 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1403375 virtual)\n",
      "I0228 01:42:38.107604 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1407449 virtual)\n",
      "I0228 01:42:38.130586 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1412146 virtual)\n",
      "I0228 01:42:38.134630 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1417158 virtual)\n",
      "I0228 01:42:38.138404 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1421076 virtual)\n",
      "I0228 01:42:38.141358 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1426024 virtual)\n",
      "I0228 01:42:38.154239 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1430432 virtual)\n",
      "I0228 01:42:38.175900 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1435032 virtual)\n",
      "I0228 01:42:38.180243 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1440659 virtual)\n",
      "I0228 01:42:38.185832 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1445759 virtual)\n",
      "I0228 01:42:38.189021 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1451534 virtual)\n",
      "I0228 01:42:38.195039 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1456359 virtual)\n",
      "I0228 01:42:38.219938 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1461714 virtual)\n",
      "I0228 01:42:38.224731 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1468028 virtual)\n",
      "I0228 01:42:38.230675 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1475651 virtual)\n",
      "I0228 01:42:38.236442 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1481945 virtual)\n",
      "I0228 01:42:38.240339 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1486837 virtual)\n",
      "I0228 01:42:38.264491 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1492113 virtual)\n",
      "I0228 01:42:38.279221 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1497254 virtual)\n",
      "I0228 01:42:38.284883 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1502753 virtual)\n",
      "I0228 01:42:38.290678 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1507786 virtual)\n",
      "I0228 01:42:38.293624 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1512831 virtual)\n",
      "I0228 01:42:38.315112 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1517974 virtual)\n",
      "I0228 01:42:38.335263 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1528172 virtual)\n",
      "I0228 01:42:38.339550 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1533737 virtual)\n",
      "I0228 01:42:38.344386 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1539614 virtual)\n",
      "I0228 01:42:38.351158 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1544731 virtual)\n",
      "I0228 01:42:38.365631 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1549064 virtual)\n",
      "I0228 01:42:38.388262 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1553985 virtual)\n",
      "I0228 01:42:38.394056 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1558683 virtual)\n",
      "I0228 01:42:38.400227 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1563673 virtual)\n",
      "I0228 01:42:38.403231 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1568343 virtual)\n",
      "I0228 01:42:38.414502 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1573924 virtual)\n",
      "I0228 01:42:38.448229 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1580138 virtual)\n",
      "I0228 01:42:38.452666 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1585665 virtual)\n",
      "I0228 01:42:38.456913 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1591233 virtual)\n",
      "I0228 01:42:38.460098 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1596805 virtual)\n",
      "I0228 01:42:38.483226 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1602800 virtual)\n",
      "I0228 01:42:38.495719 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1609023 virtual)\n",
      "I0228 01:42:38.500282 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1613801 virtual)\n",
      "I0228 01:42:38.504317 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1619018 virtual)\n",
      "I0228 01:42:38.507365 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1624021 virtual)\n",
      "I0228 01:42:38.536992 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1629206 virtual)\n",
      "I0228 01:42:38.551586 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1634562 virtual)\n",
      "I0228 01:42:38.565804 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1640129 virtual)\n",
      "I0228 01:42:38.570465 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1645127 virtual)\n",
      "I0228 01:42:38.575471 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1650887 virtual)\n",
      "I0228 01:42:38.593958 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1655895 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:38.602093 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1661976 virtual)\n",
      "I0228 01:42:38.609027 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1670038 virtual)\n",
      "I0228 01:42:38.620106 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1678635 virtual)\n",
      "I0228 01:42:38.626457 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1686907 virtual)\n",
      "I0228 01:42:38.642978 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1694799 virtual)\n",
      "I0228 01:42:38.652801 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1701002 virtual)\n",
      "I0228 01:42:38.662046 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1705821 virtual)\n",
      "I0228 01:42:38.665664 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1712449 virtual)\n",
      "I0228 01:42:38.680983 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1717940 virtual)\n",
      "I0228 01:42:38.690890 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1723953 virtual)\n",
      "I0228 01:42:38.719387 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1730071 virtual)\n",
      "I0228 01:42:38.748247 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1735798 virtual)\n",
      "I0228 01:42:38.754227 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1740904 virtual)\n",
      "I0228 01:42:38.774567 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1746502 virtual)\n",
      "I0228 01:42:38.779479 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1751743 virtual)\n",
      "I0228 01:42:38.787959 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1756813 virtual)\n",
      "I0228 01:42:38.796454 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1761788 virtual)\n",
      "I0228 01:42:38.822180 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1766886 virtual)\n",
      "I0228 01:42:38.827465 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1772316 virtual)\n",
      "I0228 01:42:38.836414 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1777715 virtual)\n",
      "I0228 01:42:38.849180 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1782475 virtual)\n",
      "I0228 01:42:38.855231 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1787502 virtual)\n",
      "I0228 01:42:38.872422 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1792439 virtual)\n",
      "I0228 01:42:38.886888 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1797586 virtual)\n",
      "I0228 01:42:38.894464 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1802744 virtual)\n",
      "I0228 01:42:38.903809 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1808354 virtual)\n",
      "I0228 01:42:38.907189 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1813238 virtual)\n",
      "I0228 01:42:38.928357 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1818478 virtual)\n",
      "I0228 01:42:38.943470 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1823538 virtual)\n",
      "I0228 01:42:38.952200 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1828337 virtual)\n",
      "I0228 01:42:38.956335 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1833607 virtual)\n",
      "I0228 01:42:38.960718 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1838889 virtual)\n",
      "I0228 01:42:38.983408 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1843769 virtual)\n",
      "I0228 01:42:38.995980 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1849609 virtual)\n",
      "I0228 01:42:39.004953 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1854117 virtual)\n",
      "I0228 01:42:39.009747 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1859198 virtual)\n",
      "I0228 01:42:39.015400 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1863907 virtual)\n",
      "I0228 01:42:39.039389 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1868993 virtual)\n",
      "I0228 01:42:39.048503 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1873929 virtual)\n",
      "I0228 01:42:39.056047 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1878964 virtual)\n",
      "I0228 01:42:39.063913 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1883914 virtual)\n",
      "I0228 01:42:39.071902 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1889380 virtual)\n",
      "I0228 01:42:39.093104 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1894841 virtual)\n",
      "I0228 01:42:39.105710 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1899985 virtual)\n",
      "I0228 01:42:39.111398 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1904665 virtual)\n",
      "I0228 01:42:39.117560 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1910225 virtual)\n",
      "I0228 01:42:39.121627 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1915300 virtual)\n",
      "I0228 01:42:39.148512 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1919920 virtual)\n",
      "I0228 01:42:39.157848 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1924583 virtual)\n",
      "I0228 01:42:39.165632 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1929659 virtual)\n",
      "I0228 01:42:39.170152 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1934997 virtual)\n",
      "I0228 01:42:39.185668 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1939836 virtual)\n",
      "I0228 01:42:39.206768 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1945025 virtual)\n",
      "I0228 01:42:39.211503 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1949877 virtual)\n",
      "I0228 01:42:39.215242 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1955018 virtual)\n",
      "I0228 01:42:39.227026 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1960360 virtual)\n",
      "I0228 01:42:39.237989 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1965340 virtual)\n",
      "I0228 01:42:39.255572 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1970596 virtual)\n",
      "I0228 01:42:39.261691 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1975441 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:39.267815 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1980839 virtual)\n",
      "I0228 01:42:39.283675 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1986183 virtual)\n",
      "I0228 01:42:39.289144 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1991104 virtual)\n",
      "I0228 01:42:39.320443 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (1996461 virtual)\n",
      "I0228 01:42:39.324735 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (2001731 virtual)\n",
      "I0228 01:42:39.327684 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (2006971 virtual)\n",
      "I0228 01:42:39.339568 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (2012234 virtual)\n",
      "I0228 01:42:39.342923 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (2017468 virtual)\n",
      "I0228 01:42:39.365815 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (2022808 virtual)\n",
      "I0228 01:42:39.371284 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2027822 virtual)\n",
      "I0228 01:42:39.384264 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2032469 virtual)\n",
      "I0228 01:42:39.392668 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2037544 virtual)\n",
      "I0228 01:42:39.397597 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2042272 virtual)\n",
      "I0228 01:42:39.422147 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2046886 virtual)\n",
      "I0228 01:42:39.429680 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2051550 virtual)\n",
      "I0228 01:42:39.438691 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2056562 virtual)\n",
      "I0228 01:42:39.450966 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2061848 virtual)\n",
      "I0228 01:42:39.454752 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2066781 virtual)\n",
      "I0228 01:42:39.477923 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2072066 virtual)\n",
      "I0228 01:42:39.484690 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2077299 virtual)\n",
      "I0228 01:42:39.488835 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2082404 virtual)\n",
      "I0228 01:42:39.504260 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2087827 virtual)\n",
      "I0228 01:42:39.508265 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2093151 virtual)\n",
      "I0228 01:42:39.529289 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2098586 virtual)\n",
      "I0228 01:42:39.534165 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2103373 virtual)\n",
      "I0228 01:42:39.541124 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2108212 virtual)\n",
      "I0228 01:42:39.560976 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2113047 virtual)\n",
      "I0228 01:42:39.565256 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2118058 virtual)\n",
      "I0228 01:42:39.582589 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2122588 virtual)\n",
      "I0228 01:42:39.589888 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2127622 virtual)\n",
      "I0228 01:42:39.593904 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2133046 virtual)\n",
      "I0228 01:42:39.616324 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2138165 virtual)\n",
      "I0228 01:42:39.620693 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2143698 virtual)\n",
      "I0228 01:42:39.639655 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2148734 virtual)\n",
      "I0228 01:42:39.643756 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2153886 virtual)\n",
      "I0228 01:42:39.647753 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2158796 virtual)\n",
      "I0228 01:42:39.666110 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2163804 virtual)\n",
      "I0228 01:42:39.676529 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2168988 virtual)\n",
      "I0228 01:42:39.688829 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2173965 virtual)\n",
      "I0228 01:42:39.696952 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2178939 virtual)\n",
      "I0228 01:42:39.705891 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2183653 virtual)\n",
      "I0228 01:42:39.721859 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2189085 virtual)\n",
      "I0228 01:42:39.736697 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2194116 virtual)\n",
      "I0228 01:42:39.739978 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2199326 virtual)\n",
      "I0228 01:42:39.754086 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2205133 virtual)\n",
      "I0228 01:42:39.758715 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2209961 virtual)\n",
      "I0228 01:42:39.773406 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2215231 virtual)\n",
      "I0228 01:42:39.792106 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2220294 virtual)\n",
      "I0228 01:42:39.796463 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2225029 virtual)\n",
      "I0228 01:42:39.807132 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2230152 virtual)\n",
      "I0228 01:42:39.811139 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2235430 virtual)\n",
      "I0228 01:42:39.830383 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2240516 virtual)\n",
      "I0228 01:42:39.846885 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2245837 virtual)\n",
      "I0228 01:42:39.852461 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2250569 virtual)\n",
      "I0228 01:42:39.860720 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2256042 virtual)\n",
      "I0228 01:42:39.870585 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2260687 virtual)\n",
      "I0228 01:42:39.887077 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2265767 virtual)\n",
      "I0228 01:42:39.900415 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2270733 virtual)\n",
      "I0228 01:42:39.905055 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2275992 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:39.913832 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2281086 virtual)\n",
      "I0228 01:42:39.927348 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2285609 virtual)\n",
      "I0228 01:42:39.938774 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2290809 virtual)\n",
      "I0228 01:42:39.952265 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2295995 virtual)\n",
      "I0228 01:42:39.957012 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2300980 virtual)\n",
      "I0228 01:42:39.969357 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2305971 virtual)\n",
      "I0228 01:42:39.977705 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2311312 virtual)\n",
      "I0228 01:42:39.989633 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2316544 virtual)\n",
      "I0228 01:42:40.006231 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2321944 virtual)\n",
      "I0228 01:42:40.012076 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2326853 virtual)\n",
      "I0228 01:42:40.026630 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2331791 virtual)\n",
      "I0228 01:42:40.031077 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2337063 virtual)\n",
      "I0228 01:42:40.045567 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2342791 virtual)\n",
      "I0228 01:42:40.061159 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2347731 virtual)\n",
      "I0228 01:42:40.065683 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2352828 virtual)\n",
      "I0228 01:42:40.078210 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2357554 virtual)\n",
      "I0228 01:42:40.083808 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2362626 virtual)\n",
      "I0228 01:42:40.101169 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2367824 virtual)\n",
      "I0228 01:42:40.115724 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2372770 virtual)\n",
      "I0228 01:42:40.118500 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2377766 virtual)\n",
      "I0228 01:42:40.129389 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2383485 virtual)\n",
      "I0228 01:42:40.137651 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2388356 virtual)\n",
      "I0228 01:42:40.164205 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2393455 virtual)\n",
      "I0228 01:42:40.169345 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2398415 virtual)\n",
      "I0228 01:42:40.175430 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2403594 virtual)\n",
      "I0228 01:42:40.179640 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2408259 virtual)\n",
      "I0228 01:42:40.191741 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2413413 virtual)\n",
      "I0228 01:42:40.220180 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2418776 virtual)\n",
      "I0228 01:42:40.224490 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2424156 virtual)\n",
      "I0228 01:42:40.228661 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2429242 virtual)\n",
      "I0228 01:42:40.241201 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2434112 virtual)\n",
      "I0228 01:42:40.245524 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2439385 virtual)\n",
      "I0228 01:42:40.272538 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2444466 virtual)\n",
      "I0228 01:42:40.281784 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2449225 virtual)\n",
      "I0228 01:42:40.287045 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2454222 virtual)\n",
      "I0228 01:42:40.292567 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2459374 virtual)\n",
      "I0228 01:42:40.299318 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2464663 virtual)\n",
      "I0228 01:42:40.329758 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2470042 virtual)\n",
      "I0228 01:42:40.336478 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2475251 virtual)\n",
      "I0228 01:42:40.339383 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2480336 virtual)\n",
      "I0228 01:42:40.345019 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2485477 virtual)\n",
      "I0228 01:42:40.354269 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2490095 virtual)\n",
      "I0228 01:42:40.383074 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2494999 virtual)\n",
      "I0228 01:42:40.388756 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2500139 virtual)\n",
      "I0228 01:42:40.394641 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2505727 virtual)\n",
      "I0228 01:42:40.401041 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2510988 virtual)\n",
      "I0228 01:42:40.405611 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2515929 virtual)\n",
      "I0228 01:42:40.436922 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2520886 virtual)\n",
      "I0228 01:42:40.445573 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2526358 virtual)\n",
      "I0228 01:42:40.451182 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2531205 virtual)\n",
      "I0228 01:42:40.454202 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2536063 virtual)\n",
      "I0228 01:42:40.457212 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2541335 virtual)\n",
      "I0228 01:42:40.488167 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2546309 virtual)\n",
      "I0228 01:42:40.499157 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2551193 virtual)\n",
      "I0228 01:42:40.504632 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2556476 virtual)\n",
      "I0228 01:42:40.509795 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2561586 virtual)\n",
      "I0228 01:42:40.513186 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2566753 virtual)\n",
      "I0228 01:42:40.540651 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2572101 virtual)\n",
      "I0228 01:42:40.553373 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2576922 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:40.558124 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2582163 virtual)\n",
      "I0228 01:42:40.563528 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2587612 virtual)\n",
      "I0228 01:42:40.567727 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2592963 virtual)\n",
      "I0228 01:42:40.592228 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2598167 virtual)\n",
      "I0228 01:42:40.606620 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2603533 virtual)\n",
      "I0228 01:42:40.611652 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2609023 virtual)\n",
      "I0228 01:42:40.616488 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2614537 virtual)\n",
      "I0228 01:42:40.623856 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2619188 virtual)\n",
      "I0228 01:42:40.650012 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2624284 virtual)\n",
      "I0228 01:42:40.656556 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2629381 virtual)\n",
      "I0228 01:42:40.667262 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2634605 virtual)\n",
      "I0228 01:42:40.676230 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2639353 virtual)\n",
      "I0228 01:42:40.681307 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2644382 virtual)\n",
      "I0228 01:42:40.704054 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2649124 virtual)\n",
      "I0228 01:42:40.713476 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2654167 virtual)\n",
      "I0228 01:42:40.728978 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2659308 virtual)\n",
      "I0228 01:42:40.732488 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2664907 virtual)\n",
      "I0228 01:42:40.735404 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2670254 virtual)\n",
      "I0228 01:42:40.759287 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2675486 virtual)\n",
      "I0228 01:42:40.765519 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2680660 virtual)\n",
      "I0228 01:42:40.781274 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2686013 virtual)\n",
      "I0228 01:42:40.786360 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2691488 virtual)\n",
      "I0228 01:42:40.790678 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2696430 virtual)\n",
      "I0228 01:42:40.809062 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2701628 virtual)\n",
      "I0228 01:42:40.819088 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2706568 virtual)\n",
      "I0228 01:42:40.834622 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2712059 virtual)\n",
      "I0228 01:42:40.843309 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2717475 virtual)\n",
      "I0228 01:42:40.847523 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2723233 virtual)\n",
      "I0228 01:42:40.863929 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2728777 virtual)\n",
      "I0228 01:42:40.874018 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2733715 virtual)\n",
      "I0228 01:42:40.892295 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2738642 virtual)\n",
      "I0228 01:42:40.899880 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2743801 virtual)\n",
      "I0228 01:42:40.904264 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2748713 virtual)\n",
      "I0228 01:42:40.920666 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2754239 virtual)\n",
      "I0228 01:42:40.925856 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2759561 virtual)\n",
      "I0228 01:42:40.949110 140370323834688 text_analysis.py:506] 552 batches submitted to accumulate stats from 35328 documents (2764730 virtual)\n",
      "I0228 01:42:40.956123 140370323834688 text_analysis.py:506] 553 batches submitted to accumulate stats from 35392 documents (2768834 virtual)\n",
      "I0228 01:42:41.011969 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:42:41.034814 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:42:41.037911 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:42:41.056173 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:42:41.057575 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:42:41.016115 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:42:41.059726 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:42:41.039067 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:42:41.043084 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:42:41.061760 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:42:41.464877 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:42:41.480993 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2769135 virtual documents\n",
      "I0228 01:42:41.569006 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:42:41.959084 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (4980 virtual)\n",
      "I0228 01:42:41.963372 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10307 virtual)\n",
      "I0228 01:42:41.967856 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16414 virtual)\n",
      "I0228 01:42:41.970917 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21227 virtual)\n",
      "I0228 01:42:41.977019 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27031 virtual)\n",
      "I0228 01:42:41.987440 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32232 virtual)\n",
      "I0228 01:42:41.992022 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37799 virtual)\n",
      "I0228 01:42:41.996248 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43526 virtual)\n",
      "I0228 01:42:41.999180 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48755 virtual)\n",
      "I0228 01:42:42.002134 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54476 virtual)\n",
      "I0228 01:42:42.021111 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60339 virtual)\n",
      "I0228 01:42:42.026581 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65654 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:42.030762 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70702 virtual)\n",
      "I0228 01:42:42.035647 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (75991 virtual)\n",
      "I0228 01:42:42.045756 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81605 virtual)\n",
      "I0228 01:42:42.077966 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87682 virtual)\n",
      "I0228 01:42:42.085623 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93571 virtual)\n",
      "I0228 01:42:42.090098 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (99157 virtual)\n",
      "I0228 01:42:42.093756 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104215 virtual)\n",
      "I0228 01:42:42.109336 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109677 virtual)\n",
      "I0228 01:42:42.137159 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (115196 virtual)\n",
      "I0228 01:42:42.141274 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120433 virtual)\n",
      "I0228 01:42:42.145536 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (126047 virtual)\n",
      "I0228 01:42:42.153693 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (131983 virtual)\n",
      "I0228 01:42:42.176201 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137553 virtual)\n",
      "I0228 01:42:42.200195 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (142928 virtual)\n",
      "I0228 01:42:42.204588 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148409 virtual)\n",
      "I0228 01:42:42.209156 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154567 virtual)\n",
      "I0228 01:42:42.213057 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160542 virtual)\n",
      "I0228 01:42:42.232913 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (165931 virtual)\n",
      "I0228 01:42:42.254132 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (171254 virtual)\n",
      "I0228 01:42:42.260176 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176788 virtual)\n",
      "I0228 01:42:42.264451 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (182043 virtual)\n",
      "I0228 01:42:42.273761 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (187413 virtual)\n",
      "I0228 01:42:42.291973 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (193231 virtual)\n",
      "I0228 01:42:42.311750 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (198451 virtual)\n",
      "I0228 01:42:42.318961 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (204204 virtual)\n",
      "I0228 01:42:42.326028 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (209778 virtual)\n",
      "I0228 01:42:42.339550 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (215112 virtual)\n",
      "I0228 01:42:42.349152 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220832 virtual)\n",
      "I0228 01:42:42.368654 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (226380 virtual)\n",
      "I0228 01:42:42.378211 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (233878 virtual)\n",
      "I0228 01:42:42.382645 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (239209 virtual)\n",
      "I0228 01:42:42.397439 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (245096 virtual)\n",
      "I0228 01:42:42.412822 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (250661 virtual)\n",
      "I0228 01:42:42.423879 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (255927 virtual)\n",
      "I0228 01:42:42.441667 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (261308 virtual)\n",
      "I0228 01:42:42.447953 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (266505 virtual)\n",
      "I0228 01:42:42.453723 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (272057 virtual)\n",
      "I0228 01:42:42.470718 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (277470 virtual)\n",
      "I0228 01:42:42.481868 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (282946 virtual)\n",
      "I0228 01:42:42.503626 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288657 virtual)\n",
      "I0228 01:42:42.509759 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (293856 virtual)\n",
      "I0228 01:42:42.516620 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (299308 virtual)\n",
      "I0228 01:42:42.528494 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304635 virtual)\n",
      "I0228 01:42:42.536215 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (310215 virtual)\n",
      "I0228 01:42:42.562761 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (315884 virtual)\n",
      "I0228 01:42:42.567654 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (321584 virtual)\n",
      "I0228 01:42:42.584811 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (326898 virtual)\n",
      "I0228 01:42:42.588625 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (333303 virtual)\n",
      "I0228 01:42:42.595973 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (339080 virtual)\n",
      "I0228 01:42:42.621130 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (344744 virtual)\n",
      "I0228 01:42:42.625434 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (350429 virtual)\n",
      "I0228 01:42:42.643613 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (356369 virtual)\n",
      "I0228 01:42:42.649376 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (362315 virtual)\n",
      "I0228 01:42:42.656019 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (367915 virtual)\n",
      "I0228 01:42:42.679834 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (373632 virtual)\n",
      "I0228 01:42:42.684505 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (379415 virtual)\n",
      "I0228 01:42:42.701602 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (385392 virtual)\n",
      "I0228 01:42:42.715847 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (390146 virtual)\n",
      "I0228 01:42:42.722341 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (395335 virtual)\n",
      "I0228 01:42:42.739651 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398984 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:42.743343 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (402435 virtual)\n",
      "I0228 01:42:42.765055 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (406965 virtual)\n",
      "I0228 01:42:42.777806 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (411064 virtual)\n",
      "I0228 01:42:42.781867 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (415460 virtual)\n",
      "I0228 01:42:42.802477 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (420008 virtual)\n",
      "I0228 01:42:42.807035 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (424304 virtual)\n",
      "I0228 01:42:42.824158 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (428498 virtual)\n",
      "I0228 01:42:42.829846 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (433158 virtual)\n",
      "I0228 01:42:42.833908 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (438091 virtual)\n",
      "I0228 01:42:42.846215 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (442750 virtual)\n",
      "I0228 01:42:42.850138 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (447651 virtual)\n",
      "I0228 01:42:42.871060 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (451744 virtual)\n",
      "I0228 01:42:42.874678 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (455278 virtual)\n",
      "I0228 01:42:42.878787 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (459300 virtual)\n",
      "I0228 01:42:42.888340 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (463125 virtual)\n",
      "I0228 01:42:42.891889 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (467367 virtual)\n",
      "I0228 01:42:42.916097 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (472205 virtual)\n",
      "I0228 01:42:42.920362 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (476889 virtual)\n",
      "I0228 01:42:42.927559 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (481346 virtual)\n",
      "I0228 01:42:42.935909 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (486015 virtual)\n",
      "I0228 01:42:42.942154 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (490744 virtual)\n",
      "I0228 01:42:42.950788 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (495501 virtual)\n",
      "I0228 01:42:42.960110 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (499809 virtual)\n",
      "I0228 01:42:42.966648 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (504139 virtual)\n",
      "I0228 01:42:42.973456 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (509497 virtual)\n",
      "I0228 01:42:42.986058 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (514134 virtual)\n",
      "I0228 01:42:43.001494 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (518953 virtual)\n",
      "I0228 01:42:43.011700 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (523097 virtual)\n",
      "I0228 01:42:43.016322 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (527912 virtual)\n",
      "I0228 01:42:43.024801 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (531673 virtual)\n",
      "I0228 01:42:43.034567 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (536365 virtual)\n",
      "I0228 01:42:43.050066 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (539956 virtual)\n",
      "I0228 01:42:43.058273 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (544483 virtual)\n",
      "I0228 01:42:43.062104 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (549041 virtual)\n",
      "I0228 01:42:43.083896 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (553296 virtual)\n",
      "I0228 01:42:43.087786 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (557966 virtual)\n",
      "I0228 01:42:43.102055 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (561825 virtual)\n",
      "I0228 01:42:43.106484 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (566442 virtual)\n",
      "I0228 01:42:43.113991 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (570529 virtual)\n",
      "I0228 01:42:43.123298 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (574738 virtual)\n",
      "I0228 01:42:43.136946 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (578572 virtual)\n",
      "I0228 01:42:43.141149 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (581593 virtual)\n",
      "I0228 01:42:43.158061 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (586042 virtual)\n",
      "I0228 01:42:43.164718 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (591329 virtual)\n",
      "I0228 01:42:43.168012 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (596986 virtual)\n",
      "I0228 01:42:43.181359 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (601923 virtual)\n",
      "I0228 01:42:43.187502 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (607194 virtual)\n",
      "I0228 01:42:43.207783 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (612977 virtual)\n",
      "I0228 01:42:43.210530 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (617983 virtual)\n",
      "I0228 01:42:43.220000 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (622536 virtual)\n",
      "I0228 01:42:43.223277 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (627363 virtual)\n",
      "I0228 01:42:43.225094 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (631222 virtual)\n",
      "I0228 01:42:43.257788 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (636219 virtual)\n",
      "I0228 01:42:43.266870 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (641455 virtual)\n",
      "I0228 01:42:43.279490 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (646867 virtual)\n",
      "I0228 01:42:43.283210 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (652460 virtual)\n",
      "I0228 01:42:43.287411 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (657498 virtual)\n",
      "I0228 01:42:43.321726 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (662072 virtual)\n",
      "I0228 01:42:43.326183 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (667307 virtual)\n",
      "I0228 01:42:43.330761 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (672679 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:43.333529 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (677837 virtual)\n",
      "I0228 01:42:43.336227 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (682763 virtual)\n",
      "I0228 01:42:43.375928 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (687765 virtual)\n",
      "I0228 01:42:43.379379 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (692951 virtual)\n",
      "I0228 01:42:43.382491 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (697930 virtual)\n",
      "I0228 01:42:43.387541 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (703283 virtual)\n",
      "I0228 01:42:43.389903 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (708432 virtual)\n",
      "I0228 01:42:43.419833 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (713607 virtual)\n",
      "I0228 01:42:43.432500 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (719113 virtual)\n",
      "I0228 01:42:43.438493 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (723999 virtual)\n",
      "I0228 01:42:43.443212 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (729559 virtual)\n",
      "I0228 01:42:43.446615 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (735007 virtual)\n",
      "I0228 01:42:43.471827 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (740677 virtual)\n",
      "I0228 01:42:43.488103 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (745536 virtual)\n",
      "I0228 01:42:43.493109 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (750897 virtual)\n",
      "I0228 01:42:43.497890 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (755938 virtual)\n",
      "I0228 01:42:43.501958 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (761538 virtual)\n",
      "I0228 01:42:43.523217 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (766495 virtual)\n",
      "I0228 01:42:43.540823 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (772597 virtual)\n",
      "I0228 01:42:43.547144 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (778412 virtual)\n",
      "I0228 01:42:43.551422 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (784205 virtual)\n",
      "I0228 01:42:43.559355 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (790027 virtual)\n",
      "I0228 01:42:43.580952 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (796044 virtual)\n",
      "I0228 01:42:43.586666 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (801581 virtual)\n",
      "I0228 01:42:43.595400 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (807153 virtual)\n",
      "I0228 01:42:43.600915 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (812950 virtual)\n",
      "I0228 01:42:43.620455 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (818563 virtual)\n",
      "I0228 01:42:43.632193 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (824800 virtual)\n",
      "I0228 01:42:43.653002 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (830315 virtual)\n",
      "I0228 01:42:43.661983 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (835534 virtual)\n",
      "I0228 01:42:43.666351 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (841208 virtual)\n",
      "I0228 01:42:43.680164 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (847189 virtual)\n",
      "I0228 01:42:43.691707 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (853256 virtual)\n",
      "I0228 01:42:43.719369 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (859126 virtual)\n",
      "I0228 01:42:43.724176 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (865227 virtual)\n",
      "I0228 01:42:43.729039 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (870813 virtual)\n",
      "I0228 01:42:43.740023 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (876939 virtual)\n",
      "I0228 01:42:43.758102 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (882665 virtual)\n",
      "I0228 01:42:43.776925 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (888779 virtual)\n",
      "I0228 01:42:43.780702 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (894475 virtual)\n",
      "I0228 01:42:43.788087 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (900206 virtual)\n",
      "I0228 01:42:43.798979 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (907064 virtual)\n",
      "I0228 01:42:43.819384 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (912823 virtual)\n",
      "I0228 01:42:43.839034 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (917046 virtual)\n",
      "I0228 01:42:43.844347 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (920344 virtual)\n",
      "I0228 01:42:43.848384 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (923869 virtual)\n",
      "I0228 01:42:43.858773 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (930600 virtual)\n",
      "I0228 01:42:43.875620 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (934424 virtual)\n",
      "I0228 01:42:43.897651 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (938246 virtual)\n",
      "I0228 01:42:43.903474 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (942172 virtual)\n",
      "I0228 01:42:43.906728 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (945680 virtual)\n",
      "I0228 01:42:43.930268 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (949451 virtual)\n",
      "I0228 01:42:43.934022 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (953331 virtual)\n",
      "I0228 01:42:43.937680 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (956910 virtual)\n",
      "I0228 01:42:43.942170 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (960163 virtual)\n",
      "I0228 01:42:43.945048 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (964510 virtual)\n",
      "I0228 01:42:43.971793 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (968438 virtual)\n",
      "I0228 01:42:43.976216 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (972126 virtual)\n",
      "I0228 01:42:43.980801 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (975905 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:43.983705 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (979789 virtual)\n",
      "I0228 01:42:44.003216 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (983403 virtual)\n",
      "I0228 01:42:44.008165 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (987236 virtual)\n",
      "I0228 01:42:44.012390 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (990890 virtual)\n",
      "I0228 01:42:44.016879 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (994640 virtual)\n",
      "I0228 01:42:44.019726 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (998673 virtual)\n",
      "I0228 01:42:44.045384 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (1002711 virtual)\n",
      "I0228 01:42:44.049026 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1006309 virtual)\n",
      "I0228 01:42:44.052998 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1010275 virtual)\n",
      "I0228 01:42:44.056815 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1014148 virtual)\n",
      "I0228 01:42:44.059537 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1018510 virtual)\n",
      "I0228 01:42:44.080077 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1023786 virtual)\n",
      "I0228 01:42:44.083790 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1027612 virtual)\n",
      "I0228 01:42:44.088244 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1031829 virtual)\n",
      "I0228 01:42:44.092539 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1035498 virtual)\n",
      "I0228 01:42:44.095585 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1040387 virtual)\n",
      "I0228 01:42:44.116968 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1044053 virtual)\n",
      "I0228 01:42:44.120657 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1048058 virtual)\n",
      "I0228 01:42:44.124429 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1052232 virtual)\n",
      "I0228 01:42:44.127935 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1056440 virtual)\n",
      "I0228 01:42:44.134432 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1060311 virtual)\n",
      "I0228 01:42:44.158067 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1064161 virtual)\n",
      "I0228 01:42:44.162772 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1068262 virtual)\n",
      "I0228 01:42:44.168909 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1072912 virtual)\n",
      "I0228 01:42:44.175217 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1077243 virtual)\n",
      "I0228 01:42:44.184281 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1081498 virtual)\n",
      "I0228 01:42:44.193491 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1086180 virtual)\n",
      "I0228 01:42:44.197789 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1090335 virtual)\n",
      "I0228 01:42:44.201889 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1094050 virtual)\n",
      "I0228 01:42:44.205508 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1097759 virtual)\n",
      "I0228 01:42:44.222730 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1102096 virtual)\n",
      "I0228 01:42:44.229758 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1105777 virtual)\n",
      "I0228 01:42:44.237793 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1110490 virtual)\n",
      "I0228 01:42:44.246012 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1114582 virtual)\n",
      "I0228 01:42:44.249318 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1119382 virtual)\n",
      "I0228 01:42:44.267729 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1123482 virtual)\n",
      "I0228 01:42:44.272295 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1128043 virtual)\n",
      "I0228 01:42:44.276615 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1132999 virtual)\n",
      "I0228 01:42:44.282267 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1136743 virtual)\n",
      "I0228 01:42:44.286695 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1142546 virtual)\n",
      "I0228 01:42:44.304512 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1146615 virtual)\n",
      "I0228 01:42:44.310105 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1150692 virtual)\n",
      "I0228 01:42:44.319902 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1154507 virtual)\n",
      "I0228 01:42:44.324790 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1159156 virtual)\n",
      "I0228 01:42:44.335568 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1162910 virtual)\n",
      "I0228 01:42:44.352483 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1167838 virtual)\n",
      "I0228 01:42:44.355976 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1172459 virtual)\n",
      "I0228 01:42:44.364584 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1176254 virtual)\n",
      "I0228 01:42:44.367821 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1181032 virtual)\n",
      "I0228 01:42:44.386219 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1186118 virtual)\n",
      "I0228 01:42:44.393992 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1189859 virtual)\n",
      "I0228 01:42:44.399175 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1194427 virtual)\n",
      "I0228 01:42:44.402541 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1199082 virtual)\n",
      "I0228 01:42:44.412832 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1203290 virtual)\n",
      "I0228 01:42:44.421903 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1207322 virtual)\n",
      "I0228 01:42:44.437493 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1211930 virtual)\n",
      "I0228 01:42:44.444372 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1216084 virtual)\n",
      "I0228 01:42:44.448823 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1220527 virtual)\n",
      "I0228 01:42:44.456619 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1224394 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:44.468780 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1228204 virtual)\n",
      "I0228 01:42:44.472665 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1231065 virtual)\n",
      "I0228 01:42:44.487562 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1235357 virtual)\n",
      "I0228 01:42:44.491261 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1239531 virtual)\n",
      "I0228 01:42:44.495600 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1244046 virtual)\n",
      "I0228 01:42:44.506050 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1249352 virtual)\n",
      "I0228 01:42:44.519822 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1253399 virtual)\n",
      "I0228 01:42:44.526599 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1257562 virtual)\n",
      "I0228 01:42:44.530890 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1261774 virtual)\n",
      "I0228 01:42:44.533714 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1265915 virtual)\n",
      "I0228 01:42:44.539942 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1269831 virtual)\n",
      "I0228 01:42:44.544259 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1273689 virtual)\n",
      "I0228 01:42:44.561755 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1277718 virtual)\n",
      "I0228 01:42:44.567023 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1282590 virtual)\n",
      "I0228 01:42:44.574331 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1286297 virtual)\n",
      "I0228 01:42:44.584469 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1290651 virtual)\n",
      "I0228 01:42:44.590499 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1295260 virtual)\n",
      "I0228 01:42:44.602517 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1298637 virtual)\n",
      "I0228 01:42:44.607128 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1302955 virtual)\n",
      "I0228 01:42:44.614930 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1307883 virtual)\n",
      "I0228 01:42:44.626118 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1312149 virtual)\n",
      "I0228 01:42:44.630030 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1315814 virtual)\n",
      "I0228 01:42:44.638505 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1320339 virtual)\n",
      "I0228 01:42:44.650308 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1324065 virtual)\n",
      "I0228 01:42:44.654456 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1327407 virtual)\n",
      "I0228 01:42:44.674419 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1331461 virtual)\n",
      "I0228 01:42:44.678231 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1335819 virtual)\n",
      "I0228 01:42:44.684063 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1340116 virtual)\n",
      "I0228 01:42:44.690030 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1344529 virtual)\n",
      "I0228 01:42:44.697888 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1348714 virtual)\n",
      "I0228 01:42:44.700686 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1353779 virtual)\n",
      "I0228 01:42:44.704061 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1356990 virtual)\n",
      "I0228 01:42:44.721008 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1360571 virtual)\n",
      "I0228 01:42:44.727247 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1365840 virtual)\n",
      "I0228 01:42:44.732124 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1371100 virtual)\n",
      "I0228 01:42:44.745218 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1375851 virtual)\n",
      "I0228 01:42:44.748665 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1380112 virtual)\n",
      "I0228 01:42:44.760557 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1384630 virtual)\n",
      "I0228 01:42:44.766610 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1389012 virtual)\n",
      "I0228 01:42:44.769564 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1393645 virtual)\n",
      "I0228 01:42:44.783566 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1398648 virtual)\n",
      "I0228 01:42:44.787559 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1403375 virtual)\n",
      "I0228 01:42:44.798616 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1407449 virtual)\n",
      "I0228 01:42:44.819947 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1412146 virtual)\n",
      "I0228 01:42:44.824081 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1417158 virtual)\n",
      "I0228 01:42:44.827986 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1421076 virtual)\n",
      "I0228 01:42:44.832120 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1426024 virtual)\n",
      "I0228 01:42:44.844655 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1430432 virtual)\n",
      "I0228 01:42:44.864327 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1435032 virtual)\n",
      "I0228 01:42:44.869479 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1440659 virtual)\n",
      "I0228 01:42:44.875769 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1445759 virtual)\n",
      "I0228 01:42:44.879380 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1451534 virtual)\n",
      "I0228 01:42:44.885767 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1456359 virtual)\n",
      "I0228 01:42:44.909922 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1461714 virtual)\n",
      "I0228 01:42:44.914523 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1468028 virtual)\n",
      "I0228 01:42:44.919517 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1475651 virtual)\n",
      "I0228 01:42:44.923294 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1481945 virtual)\n",
      "I0228 01:42:44.929628 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1486837 virtual)\n",
      "I0228 01:42:44.954689 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1492113 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:44.962941 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1497254 virtual)\n",
      "I0228 01:42:44.968508 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1502753 virtual)\n",
      "I0228 01:42:44.975934 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1507786 virtual)\n",
      "I0228 01:42:44.979493 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1512831 virtual)\n",
      "I0228 01:42:45.003537 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1517974 virtual)\n",
      "I0228 01:42:45.021497 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1528172 virtual)\n",
      "I0228 01:42:45.026526 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1533737 virtual)\n",
      "I0228 01:42:45.034815 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1539614 virtual)\n",
      "I0228 01:42:45.038416 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1544731 virtual)\n",
      "I0228 01:42:45.053877 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1549064 virtual)\n",
      "I0228 01:42:45.075056 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1553985 virtual)\n",
      "I0228 01:42:45.080680 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1558683 virtual)\n",
      "I0228 01:42:45.086730 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1563673 virtual)\n",
      "I0228 01:42:45.090229 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1568343 virtual)\n",
      "I0228 01:42:45.102309 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1573924 virtual)\n",
      "I0228 01:42:45.132664 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1580138 virtual)\n",
      "I0228 01:42:45.137189 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1585665 virtual)\n",
      "I0228 01:42:45.141581 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1591233 virtual)\n",
      "I0228 01:42:45.147197 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1596805 virtual)\n",
      "I0228 01:42:45.176814 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1602800 virtual)\n",
      "I0228 01:42:45.181427 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1609023 virtual)\n",
      "I0228 01:42:45.185958 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1613801 virtual)\n",
      "I0228 01:42:45.189347 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1619018 virtual)\n",
      "I0228 01:42:45.192145 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1624021 virtual)\n",
      "I0228 01:42:45.228325 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1629206 virtual)\n",
      "I0228 01:42:45.238931 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1634562 virtual)\n",
      "I0228 01:42:45.243169 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1640129 virtual)\n",
      "I0228 01:42:45.247335 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1645127 virtual)\n",
      "I0228 01:42:45.251694 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1650887 virtual)\n",
      "I0228 01:42:45.286396 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1655895 virtual)\n",
      "I0228 01:42:45.292103 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1661976 virtual)\n",
      "I0228 01:42:45.299080 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1670038 virtual)\n",
      "I0228 01:42:45.302700 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1678635 virtual)\n",
      "I0228 01:42:45.306123 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1686907 virtual)\n",
      "I0228 01:42:45.336918 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1694799 virtual)\n",
      "I0228 01:42:45.341683 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1701002 virtual)\n",
      "I0228 01:42:45.357805 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1705821 virtual)\n",
      "I0228 01:42:45.361674 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1712449 virtual)\n",
      "I0228 01:42:45.364698 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1717940 virtual)\n",
      "I0228 01:42:45.386221 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1723953 virtual)\n",
      "I0228 01:42:45.401872 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1730071 virtual)\n",
      "I0228 01:42:45.438931 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1735798 virtual)\n",
      "I0228 01:42:45.442719 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1740904 virtual)\n",
      "I0228 01:42:45.447096 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1746502 virtual)\n",
      "I0228 01:42:45.469474 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1751743 virtual)\n",
      "I0228 01:42:45.474690 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1756813 virtual)\n",
      "I0228 01:42:45.487430 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1761788 virtual)\n",
      "I0228 01:42:45.497187 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1766886 virtual)\n",
      "I0228 01:42:45.507599 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1772316 virtual)\n",
      "I0228 01:42:45.527042 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1777715 virtual)\n",
      "I0228 01:42:45.533433 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1782475 virtual)\n",
      "I0228 01:42:45.547071 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1787502 virtual)\n",
      "I0228 01:42:45.551436 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1792439 virtual)\n",
      "I0228 01:42:45.567656 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1797586 virtual)\n",
      "I0228 01:42:45.583165 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1802744 virtual)\n",
      "I0228 01:42:45.588899 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1808354 virtual)\n",
      "I0228 01:42:45.599559 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1813238 virtual)\n",
      "I0228 01:42:45.603693 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1818478 virtual)\n",
      "I0228 01:42:45.624229 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1823538 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:45.637827 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1828337 virtual)\n",
      "I0228 01:42:45.642612 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1833607 virtual)\n",
      "I0228 01:42:45.654329 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1838889 virtual)\n",
      "I0228 01:42:45.658640 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1843769 virtual)\n",
      "I0228 01:42:45.676873 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1849609 virtual)\n",
      "I0228 01:42:45.691485 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1854117 virtual)\n",
      "I0228 01:42:45.699383 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1859198 virtual)\n",
      "I0228 01:42:45.703020 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1863907 virtual)\n",
      "I0228 01:42:45.714238 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1868993 virtual)\n",
      "I0228 01:42:45.728826 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1873929 virtual)\n",
      "I0228 01:42:45.750323 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1878964 virtual)\n",
      "I0228 01:42:45.754664 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1883914 virtual)\n",
      "I0228 01:42:45.760505 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1889380 virtual)\n",
      "I0228 01:42:45.768641 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1894841 virtual)\n",
      "I0228 01:42:45.790113 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1899985 virtual)\n",
      "I0228 01:42:45.794375 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1904665 virtual)\n",
      "I0228 01:42:45.807300 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1910225 virtual)\n",
      "I0228 01:42:45.811353 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1915300 virtual)\n",
      "I0228 01:42:45.824752 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1919920 virtual)\n",
      "I0228 01:42:45.841458 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1924583 virtual)\n",
      "I0228 01:42:45.849694 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1929659 virtual)\n",
      "I0228 01:42:45.858447 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1934997 virtual)\n",
      "I0228 01:42:45.867185 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1939836 virtual)\n",
      "I0228 01:42:45.882827 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1945025 virtual)\n",
      "I0228 01:42:45.894562 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1949877 virtual)\n",
      "I0228 01:42:45.899066 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1955018 virtual)\n",
      "I0228 01:42:45.915492 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1960360 virtual)\n",
      "I0228 01:42:45.920018 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1965340 virtual)\n",
      "I0228 01:42:45.931897 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1970596 virtual)\n",
      "I0228 01:42:45.943976 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1975441 virtual)\n",
      "I0228 01:42:45.953836 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1980839 virtual)\n",
      "I0228 01:42:45.971295 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1986183 virtual)\n",
      "I0228 01:42:45.975668 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1991104 virtual)\n",
      "I0228 01:42:45.988273 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (1996461 virtual)\n",
      "I0228 01:42:45.994843 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (2001731 virtual)\n",
      "I0228 01:42:46.010705 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (2006971 virtual)\n",
      "I0228 01:42:46.024649 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (2012234 virtual)\n",
      "I0228 01:42:46.029656 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (2017468 virtual)\n",
      "I0228 01:42:46.045877 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (2022808 virtual)\n",
      "I0228 01:42:46.050913 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2027822 virtual)\n",
      "I0228 01:42:46.072003 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2032469 virtual)\n",
      "I0228 01:42:46.077227 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2037544 virtual)\n",
      "I0228 01:42:46.083346 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2042272 virtual)\n",
      "I0228 01:42:46.102471 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2046886 virtual)\n",
      "I0228 01:42:46.106515 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2051550 virtual)\n",
      "I0228 01:42:46.127866 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2056562 virtual)\n",
      "I0228 01:42:46.135470 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2061848 virtual)\n",
      "I0228 01:42:46.139212 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2066781 virtual)\n",
      "I0228 01:42:46.158046 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2072066 virtual)\n",
      "I0228 01:42:46.162548 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2077299 virtual)\n",
      "I0228 01:42:46.177345 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2082404 virtual)\n",
      "I0228 01:42:46.190626 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2087827 virtual)\n",
      "I0228 01:42:46.193992 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2093151 virtual)\n",
      "I0228 01:42:46.215069 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2098586 virtual)\n",
      "I0228 01:42:46.219006 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2103373 virtual)\n",
      "I0228 01:42:46.232177 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2108212 virtual)\n",
      "I0228 01:42:46.244495 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2113047 virtual)\n",
      "I0228 01:42:46.248297 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2118058 virtual)\n",
      "I0228 01:42:46.267380 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2122588 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:46.270661 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2127622 virtual)\n",
      "I0228 01:42:46.286185 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2133046 virtual)\n",
      "I0228 01:42:46.301938 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2138165 virtual)\n",
      "I0228 01:42:46.306052 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2143698 virtual)\n",
      "I0228 01:42:46.313065 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2148734 virtual)\n",
      "I0228 01:42:46.324031 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2153886 virtual)\n",
      "I0228 01:42:46.340487 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2158796 virtual)\n",
      "I0228 01:42:46.351621 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2163804 virtual)\n",
      "I0228 01:42:46.366279 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2168988 virtual)\n",
      "I0228 01:42:46.370229 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2173965 virtual)\n",
      "I0228 01:42:46.379504 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2178939 virtual)\n",
      "I0228 01:42:46.401569 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2183653 virtual)\n",
      "I0228 01:42:46.406946 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2189085 virtual)\n",
      "I0228 01:42:46.413295 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2194116 virtual)\n",
      "I0228 01:42:46.422059 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2199326 virtual)\n",
      "I0228 01:42:46.437963 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2205133 virtual)\n",
      "I0228 01:42:46.453969 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2209961 virtual)\n",
      "I0228 01:42:46.458627 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2215231 virtual)\n",
      "I0228 01:42:46.470824 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2220294 virtual)\n",
      "I0228 01:42:46.474625 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2225029 virtual)\n",
      "I0228 01:42:46.491418 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2230152 virtual)\n",
      "I0228 01:42:46.505709 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2235430 virtual)\n",
      "I0228 01:42:46.515056 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2240516 virtual)\n",
      "I0228 01:42:46.526177 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2245837 virtual)\n",
      "I0228 01:42:46.530874 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2250569 virtual)\n",
      "I0228 01:42:46.555881 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2256042 virtual)\n",
      "I0228 01:42:46.559875 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2260687 virtual)\n",
      "I0228 01:42:46.572129 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2265767 virtual)\n",
      "I0228 01:42:46.580069 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2270733 virtual)\n",
      "I0228 01:42:46.583989 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2275992 virtual)\n",
      "I0228 01:42:46.609323 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2281086 virtual)\n",
      "I0228 01:42:46.618076 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2285609 virtual)\n",
      "I0228 01:42:46.624036 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2290809 virtual)\n",
      "I0228 01:42:46.633044 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2295995 virtual)\n",
      "I0228 01:42:46.636805 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2300980 virtual)\n",
      "I0228 01:42:46.665409 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2305971 virtual)\n",
      "I0228 01:42:46.669991 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2311312 virtual)\n",
      "I0228 01:42:46.678924 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2316544 virtual)\n",
      "I0228 01:42:46.686269 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2321944 virtual)\n",
      "I0228 01:42:46.689631 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2326853 virtual)\n",
      "I0228 01:42:46.722450 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2331791 virtual)\n",
      "I0228 01:42:46.727206 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2337063 virtual)\n",
      "I0228 01:42:46.732164 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2342791 virtual)\n",
      "I0228 01:42:46.740623 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2347731 virtual)\n",
      "I0228 01:42:46.743433 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2352828 virtual)\n",
      "I0228 01:42:46.779246 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2357554 virtual)\n",
      "I0228 01:42:46.784181 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2362626 virtual)\n",
      "I0228 01:42:46.788074 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2367824 virtual)\n",
      "I0228 01:42:46.795245 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2372770 virtual)\n",
      "I0228 01:42:46.798474 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2377766 virtual)\n",
      "I0228 01:42:46.831897 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2383485 virtual)\n",
      "I0228 01:42:46.835962 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2388356 virtual)\n",
      "I0228 01:42:46.847829 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2393455 virtual)\n",
      "I0228 01:42:46.851830 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2398415 virtual)\n",
      "I0228 01:42:46.855971 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2403594 virtual)\n",
      "I0228 01:42:46.883574 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2408259 virtual)\n",
      "I0228 01:42:46.889581 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2413413 virtual)\n",
      "I0228 01:42:46.903869 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2418776 virtual)\n",
      "I0228 01:42:46.909536 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2424156 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:46.915368 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2429242 virtual)\n",
      "I0228 01:42:46.939150 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2434112 virtual)\n",
      "I0228 01:42:46.947718 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2439385 virtual)\n",
      "I0228 01:42:46.955951 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2444466 virtual)\n",
      "I0228 01:42:46.959543 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2449225 virtual)\n",
      "I0228 01:42:46.962953 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2454222 virtual)\n",
      "I0228 01:42:46.990767 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2459374 virtual)\n",
      "I0228 01:42:47.005091 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2464663 virtual)\n",
      "I0228 01:42:47.013168 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2470042 virtual)\n",
      "I0228 01:42:47.017472 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2475251 virtual)\n",
      "I0228 01:42:47.021793 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2480336 virtual)\n",
      "I0228 01:42:47.043000 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2485477 virtual)\n",
      "I0228 01:42:47.061742 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2490095 virtual)\n",
      "I0228 01:42:47.066608 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2494999 virtual)\n",
      "I0228 01:42:47.071465 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2500139 virtual)\n",
      "I0228 01:42:47.076682 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2505727 virtual)\n",
      "I0228 01:42:47.099310 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2510988 virtual)\n",
      "I0228 01:42:47.114590 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2515929 virtual)\n",
      "I0228 01:42:47.119719 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2520886 virtual)\n",
      "I0228 01:42:47.124281 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2526358 virtual)\n",
      "I0228 01:42:47.127443 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2531205 virtual)\n",
      "I0228 01:42:47.151333 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2536063 virtual)\n",
      "I0228 01:42:47.162397 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2541335 virtual)\n",
      "I0228 01:42:47.170596 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2546309 virtual)\n",
      "I0228 01:42:47.178432 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2551193 virtual)\n",
      "I0228 01:42:47.186177 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2556476 virtual)\n",
      "I0228 01:42:47.206462 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2561586 virtual)\n",
      "I0228 01:42:47.214982 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2566753 virtual)\n",
      "I0228 01:42:47.222824 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2572101 virtual)\n",
      "I0228 01:42:47.234992 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2576922 virtual)\n",
      "I0228 01:42:47.240128 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2582163 virtual)\n",
      "I0228 01:42:47.258001 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2587612 virtual)\n",
      "I0228 01:42:47.272153 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2592963 virtual)\n",
      "I0228 01:42:47.276867 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2598167 virtual)\n",
      "I0228 01:42:47.287429 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2603533 virtual)\n",
      "I0228 01:42:47.294597 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2609023 virtual)\n",
      "I0228 01:42:47.311562 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2614537 virtual)\n",
      "I0228 01:42:47.330807 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2619188 virtual)\n",
      "I0228 01:42:47.336124 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2624284 virtual)\n",
      "I0228 01:42:47.341715 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2629381 virtual)\n",
      "I0228 01:42:47.350860 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2634605 virtual)\n",
      "I0228 01:42:47.372854 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2639353 virtual)\n",
      "I0228 01:42:47.386600 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2644382 virtual)\n",
      "I0228 01:42:47.389582 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2649124 virtual)\n",
      "I0228 01:42:47.393514 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2654167 virtual)\n",
      "I0228 01:42:47.413916 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2659308 virtual)\n",
      "I0228 01:42:47.430540 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2664907 virtual)\n",
      "I0228 01:42:47.436316 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2670254 virtual)\n",
      "I0228 01:42:47.445240 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2675486 virtual)\n",
      "I0228 01:42:47.448300 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2680660 virtual)\n",
      "I0228 01:42:47.471808 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2686013 virtual)\n",
      "I0228 01:42:47.480570 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2691488 virtual)\n",
      "I0228 01:42:47.489552 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2696430 virtual)\n",
      "I0228 01:42:47.494731 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2701628 virtual)\n",
      "I0228 01:42:47.501322 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2706568 virtual)\n",
      "I0228 01:42:47.525102 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2712059 virtual)\n",
      "I0228 01:42:47.538145 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2717475 virtual)\n",
      "I0228 01:42:47.546357 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2723233 virtual)\n",
      "I0228 01:42:47.550549 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2728777 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:42:47.557429 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2733715 virtual)\n",
      "I0228 01:42:47.582920 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2738642 virtual)\n",
      "I0228 01:42:47.595577 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2743801 virtual)\n",
      "I0228 01:42:47.601802 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2748713 virtual)\n",
      "I0228 01:42:47.607712 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2754239 virtual)\n",
      "I0228 01:42:47.610638 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2759561 virtual)\n",
      "I0228 01:42:47.639495 140370323834688 text_analysis.py:506] 552 batches submitted to accumulate stats from 35328 documents (2764730 virtual)\n",
      "I0228 01:42:47.652565 140370323834688 text_analysis.py:506] 553 batches submitted to accumulate stats from 35392 documents (2768834 virtual)\n",
      "I0228 01:42:47.711586 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:42:47.721240 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:42:47.724101 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:42:47.725186 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:42:47.746666 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:42:47.754860 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:42:47.715955 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:42:47.731481 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:42:47.759255 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:42:47.764657 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:42:48.160335 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:42:48.176295 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2769135 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores\n",
      "{'epoch': 174, 'cv': 0.6852554332684024, 'umass': -4.228792628581067, 'uci': -0.1449379641091472, 'npmi': 0.07224426189401636, 'rbo': 1.0, 'td': 1.0, 'train_loss': 642.0106920378274, 'topics': [['c0267454', 'ref', 'c0605290', 'salivary', 'c1446219', 'heparin', 'c0536858', 'c0521990', 'c0020933', 'c0851891', 'c0052432', 'c0444584', 'c0751651', 'c1960870', 'c0007789', 'c0038174', 'c0024348', 'somatic', 'c0032821', 'shareholder', 'excretion', 'swell', 'c0271737', 'colitis', 'goat', 'candida'], ['c0012634', 'c0011900', 'c0015967', 'c0032285', 'child', 'common', 'c1457887', 'c0546788', 'c0003232', 'c0035236', 'clinical', 'c0010200', 'c0221423', 'sars-cov-2', 'presentation', 'c0809949', 'c0019993', 'manifestation', 'cause', 'disclosure', 'c0035235', 'c0027442', 'c0231221', 'c0039082', 'c3714514', 'c0038410'], ['c0543467', 'c0025080', 'postoperative', 'c0031150', 'c0728940', 'operative', 'c0038930', 'c0002940', 'c0850292', 'c0005898', 'procedure', 'undergo', 'c0229962', 'c0009566', 'recurrence', 'surgical', 'perform', 'c0582175', 'c0014245', 'conversion', 'perioperative', 'c0162522', 'c0547070', 'c0187996', 'c1522577', 'c0019080'], ['c0199470', 'c0243095', 'compare', 'difference', 'c0032042', 'significantly', 'c0034108', 'significant', 'receive', 'decrease', 'curve', 'measurement', 'c0235195', 'c0005516', 'c0369768', 'infant', 'c0021708', 'c0018810', 'predict', 'concentration', 'predictor', 'regression', 'confidence', 'assess', 'c0918012', 'determine'], ['crisis', 'policy', 'disaster', 'economic', 'political', 'threat', 'market', 'economy', 'emergency', 'public', 'food', 'sector', 'national', 'supply', 'argue', 'face', 'c1561598', 'international', 'draw', 'financial', 'society', 'governance', 'management', 'c0242456', 'inequality', 'trade'], ['c0042210', 'c1254351', 'c0030956', 'c1167622', 'c1514562', 'c0029224', 'c0003320', 'c0020971', 'c0003316', 'c1706082', 'c0003250', 'affinity', 'active', 'nanoparticles', 'drug', 'potential', 'promise', 'potency', 'candidate', 'elicit', 'therapeutic', 'c0243077', 'c3687832', 'synthetic', 'potent', 'bind'], ['activation', 'c1171362', 'c0079189', 'mechanism', 'c0007613', 'c0025929', 'c0007634', 'role', 'c3539881', 'c0024432', 'c0017262', 'induce', 'c0162638', 'activate', 'c1101610', 'induction', 'c0021368', 'pathway', 'c0023810', 'c0021747', 'c0035696', 'c0039194', 'suppress', 'innate', 'c0007994', 'c0013081'], ['c3161035', 'propose', 'c0002045', 'c0025663', 'machine', 'automate', 'c0150098', 'c0679083', 'accuracy', 'performance', 'sensor', 'prediction', 'image', 'compute', 'c0037585', 'input', 'solve', 'representation', 'algorithm', 'c0037589', 'computational', 'filter', 'c1704254', 'outperform', 'equation', 'c1710191'], ['c0679646', 'search', 'c2603343', 'train', 'conduct', 'report', 'c0025353', 'c0242356', 'c0086388', 'include', 'evidence', 'c0038951', 'c1257890', 'c0027361', 'c0030971', 'recommendation', 'impact', 'c0242481', 'c0282122', 'c0003467', 'c1706852', 'c0282574', 'psychological', 'c0184661', 'c1955832', 'parent'], ['c1705920', 'c0042776', 'c0684063', 'c0032098', 'c0017428', 'sample', 'genetic', 'c0005595', 'c0039005', 'c0012984', 'c0003062', 'c0007452', 'c1764827', 'c0017446', 'c0017337', 'diversity', 'c0026882', 'genotype', 'c0004793', 'c0242781', 'c0162326', 'specie', 'c0442726', 'c0015733', 'c1519068', 'c0086418']]}\n",
      "Epoch: [176/250]\tSamples: [6463776/9181500]\tTrain Loss: 641.839324384462\tTime: 0:00:04.402809\n",
      "Epoch: [177/250]\tSamples: [6500502/9181500]\tTrain Loss: 642.1410860791741\tTime: 0:00:04.579267\n",
      "Epoch: [178/250]\tSamples: [6537228/9181500]\tTrain Loss: 642.0016175529597\tTime: 0:00:04.488482\n",
      "Epoch: [179/250]\tSamples: [6573954/9181500]\tTrain Loss: 641.81611747723\tTime: 0:00:04.603717\n",
      "Epoch: [180/250]\tSamples: [6610680/9181500]\tTrain Loss: 641.7472768142801\tTime: 0:00:04.716602\n",
      "Epoch: [181/250]\tSamples: [6647406/9181500]\tTrain Loss: 642.0567008258795\tTime: 0:00:04.803235\n",
      "Epoch: [182/250]\tSamples: [6684132/9181500]\tTrain Loss: 641.8225898163087\tTime: 0:00:04.764419\n",
      "Epoch: [183/250]\tSamples: [6720858/9181500]\tTrain Loss: 641.8382265161262\tTime: 0:00:04.756815\n",
      "Epoch: [184/250]\tSamples: [6757584/9181500]\tTrain Loss: 641.734312565519\tTime: 0:00:04.790728\n",
      "Epoch: [185/250]\tSamples: [6794310/9181500]\tTrain Loss: 641.8766772220293\tTime: 0:00:04.740634\n",
      "Epoch: [186/250]\tSamples: [6831036/9181500]\tTrain Loss: 641.7452948651841\tTime: 0:00:04.813272\n",
      "Epoch: [187/250]\tSamples: [6867762/9181500]\tTrain Loss: 641.8686868848023\tTime: 0:00:04.825667\n",
      "Epoch: [188/250]\tSamples: [6904488/9181500]\tTrain Loss: 641.9112112544587\tTime: 0:00:04.790506\n",
      "Epoch: [189/250]\tSamples: [6941214/9181500]\tTrain Loss: 642.1040197807752\tTime: 0:00:04.740645\n",
      "Epoch: [190/250]\tSamples: [6977940/9181500]\tTrain Loss: 642.0507383861229\tTime: 0:00:04.738715\n",
      "Epoch: [191/250]\tSamples: [7014666/9181500]\tTrain Loss: 641.9363953244977\tTime: 0:00:04.745954\n",
      "Epoch: [192/250]\tSamples: [7051392/9181500]\tTrain Loss: 641.8634730206461\tTime: 0:00:04.742040\n",
      "Epoch: [193/250]\tSamples: [7088118/9181500]\tTrain Loss: 642.0231962710342\tTime: 0:00:04.735390\n",
      "Epoch: [194/250]\tSamples: [7124844/9181500]\tTrain Loss: 641.9388788766813\tTime: 0:00:04.793435\n",
      "Epoch: [195/250]\tSamples: [7161570/9181500]\tTrain Loss: 642.0323077698702\tTime: 0:00:04.791109\n",
      "Epoch: [196/250]\tSamples: [7198296/9181500]\tTrain Loss: 641.6772846765575\tTime: 0:00:04.773792\n",
      "Epoch: [197/250]\tSamples: [7235022/9181500]\tTrain Loss: 641.8882717412391\tTime: 0:00:04.753275\n",
      "Epoch: [198/250]\tSamples: [7271748/9181500]\tTrain Loss: 641.9508628934542\tTime: 0:00:04.810019\n",
      "Epoch: [199/250]\tSamples: [7308474/9181500]\tTrain Loss: 641.7834415885544\tTime: 0:00:04.754950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:44:46.578902 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [200/250]\tSamples: [7345200/9181500]\tTrain Loss: 641.8717040550291\tTime: 0:00:04.817824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:44:47.401487 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:44:48.085269 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:44:48.904190 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:44:49.441246 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:44:49.447748 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:44:50.202729 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:44:50.872859 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:44:51.672607 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:44:52.189724 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:44:52.194809 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:44:52.942055 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:44:53.615209 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:44:54.406133 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:44:54.925306 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:44:54.931727 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:44:55.684108 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:44:56.358300 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:44:57.151750 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:44:57.679454 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:44:57.701548 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:44:58.240341 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (-33775 virtual)\n",
      "I0228 01:44:58.308112 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (-50409 virtual)\n",
      "I0228 01:44:58.609140 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (-204994 virtual)\n",
      "I0228 01:44:58.879018 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (-476239 virtual)\n",
      "I0228 01:44:58.884133 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (-475779 virtual)\n",
      "I0228 01:44:58.910475 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (-480491 virtual)\n",
      "I0228 01:44:59.007257 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (-504169 virtual)\n",
      "I0228 01:44:59.009798 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (-502003 virtual)\n",
      "I0228 01:44:59.014934 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (-500285 virtual)\n",
      "I0228 01:44:59.018892 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (-498788 virtual)\n",
      "I0228 01:44:59.025145 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (-500736 virtual)\n",
      "I0228 01:44:59.645171 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:44:59.645772 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:44:59.648160 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:44:59.649251 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:44:59.656087 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:44:59.658024 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:44:59.651288 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:44:59.657930 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:44:59.648336 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:44:59.650397 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:45:00.072209 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:45:00.091172 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 229090 virtual documents\n",
      "I0228 01:45:01.940969 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 1000 documents\n",
      "I0228 01:45:01.954571 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 2000 documents\n",
      "I0228 01:45:01.968257 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 3000 documents\n",
      "I0228 01:45:01.982228 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 4000 documents\n",
      "I0228 01:45:01.994962 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 5000 documents\n",
      "I0228 01:45:02.006504 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 6000 documents\n",
      "I0228 01:45:02.018357 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 7000 documents\n",
      "I0228 01:45:02.030121 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 8000 documents\n",
      "I0228 01:45:02.042845 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 9000 documents\n",
      "I0228 01:45:02.056077 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 10000 documents\n",
      "I0228 01:45:02.069920 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 11000 documents\n",
      "I0228 01:45:02.081579 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 12000 documents\n",
      "I0228 01:45:02.091799 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 13000 documents\n",
      "I0228 01:45:02.103174 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 14000 documents\n",
      "I0228 01:45:02.113931 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 15000 documents\n",
      "I0228 01:45:02.125173 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 16000 documents\n",
      "I0228 01:45:02.135534 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 17000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:02.146278 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 18000 documents\n",
      "I0228 01:45:02.156473 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 19000 documents\n",
      "I0228 01:45:02.167675 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 20000 documents\n",
      "I0228 01:45:02.180381 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 21000 documents\n",
      "I0228 01:45:02.201920 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 22000 documents\n",
      "I0228 01:45:02.218993 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 23000 documents\n",
      "I0228 01:45:02.235903 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 24000 documents\n",
      "I0228 01:45:02.251544 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 25000 documents\n",
      "I0228 01:45:02.269016 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 26000 documents\n",
      "I0228 01:45:02.285074 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 27000 documents\n",
      "I0228 01:45:02.301148 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 28000 documents\n",
      "I0228 01:45:02.317535 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 29000 documents\n",
      "I0228 01:45:02.334516 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 30000 documents\n",
      "I0228 01:45:02.350985 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 31000 documents\n",
      "I0228 01:45:02.367157 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 32000 documents\n",
      "I0228 01:45:02.383368 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 33000 documents\n",
      "I0228 01:45:02.400018 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 34000 documents\n",
      "I0228 01:45:02.416712 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 35000 documents\n",
      "I0228 01:45:02.433034 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 36000 documents\n",
      "I0228 01:45:02.589931 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:45:02.961900 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (5080 virtual)\n",
      "I0228 01:45:02.967161 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10414 virtual)\n",
      "I0228 01:45:02.971349 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16449 virtual)\n",
      "I0228 01:45:02.974462 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21306 virtual)\n",
      "I0228 01:45:02.977905 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27023 virtual)\n",
      "I0228 01:45:02.984824 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32224 virtual)\n",
      "I0228 01:45:02.988626 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37895 virtual)\n",
      "I0228 01:45:02.991160 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43621 virtual)\n",
      "I0228 01:45:02.992980 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48845 virtual)\n",
      "I0228 01:45:02.994789 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54503 virtual)\n",
      "I0228 01:45:03.025235 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60366 virtual)\n",
      "I0228 01:45:03.027164 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65737 virtual)\n",
      "I0228 01:45:03.044350 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70907 virtual)\n",
      "I0228 01:45:03.046656 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (76109 virtual)\n",
      "I0228 01:45:03.062806 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81738 virtual)\n",
      "I0228 01:45:03.082109 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87859 virtual)\n",
      "I0228 01:45:03.088013 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93775 virtual)\n",
      "I0228 01:45:03.102322 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (99405 virtual)\n",
      "I0228 01:45:03.106201 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104476 virtual)\n",
      "I0228 01:45:03.125412 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109888 virtual)\n",
      "I0228 01:45:03.141638 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (115369 virtual)\n",
      "I0228 01:45:03.146811 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120625 virtual)\n",
      "I0228 01:45:03.159192 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (126240 virtual)\n",
      "I0228 01:45:03.164261 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (132209 virtual)\n",
      "I0228 01:45:03.188915 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137837 virtual)\n",
      "I0228 01:45:03.204761 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (143025 virtual)\n",
      "I0228 01:45:03.210728 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148615 virtual)\n",
      "I0228 01:45:03.217298 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154689 virtual)\n",
      "I0228 01:45:03.220474 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160627 virtual)\n",
      "I0228 01:45:03.248123 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (166143 virtual)\n",
      "I0228 01:45:03.260699 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (171487 virtual)\n",
      "I0228 01:45:03.266345 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176923 virtual)\n",
      "I0228 01:45:03.274211 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (182229 virtual)\n",
      "I0228 01:45:03.278250 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (187561 virtual)\n",
      "I0228 01:45:03.307574 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (193421 virtual)\n",
      "I0228 01:45:03.315156 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (198625 virtual)\n",
      "I0228 01:45:03.329511 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (204318 virtual)\n",
      "I0228 01:45:03.338153 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (209995 virtual)\n",
      "I0228 01:45:03.341113 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (215361 virtual)\n",
      "I0228 01:45:03.363797 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220917 virtual)\n",
      "I0228 01:45:03.373020 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (228625 virtual)\n",
      "I0228 01:45:03.383984 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (234048 virtual)\n",
      "I0228 01:45:03.394085 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (239489 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:03.398383 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (245251 virtual)\n",
      "I0228 01:45:03.423900 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (250762 virtual)\n",
      "I0228 01:45:03.427764 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (256041 virtual)\n",
      "I0228 01:45:03.446339 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (261420 virtual)\n",
      "I0228 01:45:03.457003 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (266680 virtual)\n",
      "I0228 01:45:03.460827 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (272256 virtual)\n",
      "I0228 01:45:03.480039 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (277666 virtual)\n",
      "I0228 01:45:03.503506 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (283051 virtual)\n",
      "I0228 01:45:03.506707 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288857 virtual)\n",
      "I0228 01:45:03.514547 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (294057 virtual)\n",
      "I0228 01:45:03.517424 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (299477 virtual)\n",
      "I0228 01:45:03.536181 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304828 virtual)\n",
      "I0228 01:45:03.551983 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (310452 virtual)\n",
      "I0228 01:45:03.568493 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (316106 virtual)\n",
      "I0228 01:45:03.573468 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (321778 virtual)\n",
      "I0228 01:45:03.577918 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (327111 virtual)\n",
      "I0228 01:45:03.592626 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (333591 virtual)\n",
      "I0228 01:45:03.611224 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (339290 virtual)\n",
      "I0228 01:45:03.623500 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (345053 virtual)\n",
      "I0228 01:45:03.636875 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (350650 virtual)\n",
      "I0228 01:45:03.638957 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (356731 virtual)\n",
      "I0228 01:45:03.648862 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (362532 virtual)\n",
      "I0228 01:45:03.671283 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (368103 virtual)\n",
      "I0228 01:45:03.681707 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (374074 virtual)\n",
      "I0228 01:45:03.690300 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (379711 virtual)\n",
      "I0228 01:45:03.703555 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (385645 virtual)\n",
      "I0228 01:45:03.717386 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (390277 virtual)\n",
      "I0228 01:45:03.728011 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (395407 virtual)\n",
      "I0228 01:45:03.740128 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (399135 virtual)\n",
      "I0228 01:45:03.747966 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (402520 virtual)\n",
      "I0228 01:45:03.767678 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (407029 virtual)\n",
      "I0228 01:45:03.777536 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (411385 virtual)\n",
      "I0228 01:45:03.781919 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (415669 virtual)\n",
      "I0228 01:45:03.803281 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (420069 virtual)\n",
      "I0228 01:45:03.808673 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (424569 virtual)\n",
      "I0228 01:45:03.822021 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (428850 virtual)\n",
      "I0228 01:45:03.830125 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (433515 virtual)\n",
      "I0228 01:45:03.834030 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (438330 virtual)\n",
      "I0228 01:45:03.847693 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (443074 virtual)\n",
      "I0228 01:45:03.852131 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (447643 virtual)\n",
      "I0228 01:45:03.869596 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (451945 virtual)\n",
      "I0228 01:45:03.872426 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (455575 virtual)\n",
      "I0228 01:45:03.876245 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (459304 virtual)\n",
      "I0228 01:45:03.891636 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (463390 virtual)\n",
      "I0228 01:45:03.894867 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (467683 virtual)\n",
      "I0228 01:45:03.915621 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (472539 virtual)\n",
      "I0228 01:45:03.919870 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (477187 virtual)\n",
      "I0228 01:45:03.923935 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (481753 virtual)\n",
      "I0228 01:45:03.936767 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (486331 virtual)\n",
      "I0228 01:45:03.940282 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (491124 virtual)\n",
      "I0228 01:45:03.952082 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (495829 virtual)\n",
      "I0228 01:45:03.957771 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (500137 virtual)\n",
      "I0228 01:45:03.961649 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (504446 virtual)\n",
      "I0228 01:45:03.977322 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (509797 virtual)\n",
      "I0228 01:45:03.982917 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (514851 virtual)\n",
      "I0228 01:45:04.001943 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (519254 virtual)\n",
      "I0228 01:45:04.007009 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (523599 virtual)\n",
      "I0228 01:45:04.011740 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (528238 virtual)\n",
      "I0228 01:45:04.025554 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (532162 virtual)\n",
      "I0228 01:45:04.030045 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (536491 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:04.049179 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (540697 virtual)\n",
      "I0228 01:45:04.054149 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (545094 virtual)\n",
      "I0228 01:45:04.058738 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (549444 virtual)\n",
      "I0228 01:45:04.082688 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (553545 virtual)\n",
      "I0228 01:45:04.087083 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (558314 virtual)\n",
      "I0228 01:45:04.101267 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (562133 virtual)\n",
      "I0228 01:45:04.115905 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (567041 virtual)\n",
      "I0228 01:45:04.120647 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (570920 virtual)\n",
      "I0228 01:45:04.125413 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (574953 virtual)\n",
      "I0228 01:45:04.130725 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (578584 virtual)\n",
      "I0228 01:45:04.144814 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (581849 virtual)\n",
      "I0228 01:45:04.148688 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (586763 virtual)\n",
      "I0228 01:45:04.160847 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (592286 virtual)\n",
      "I0228 01:45:04.167634 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (597645 virtual)\n",
      "I0228 01:45:04.181182 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (602263 virtual)\n",
      "I0228 01:45:04.184802 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (608268 virtual)\n",
      "I0228 01:45:04.197963 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (613636 virtual)\n",
      "I0228 01:45:04.207976 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (618872 virtual)\n",
      "I0228 01:45:04.212552 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (623186 virtual)\n",
      "I0228 01:45:04.222319 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (627812 virtual)\n",
      "I0228 01:45:04.225495 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (631814 virtual)\n",
      "I0228 01:45:04.248007 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (637236 virtual)\n",
      "I0228 01:45:04.266895 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (642534 virtual)\n",
      "I0228 01:45:04.272507 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (647613 virtual)\n",
      "I0228 01:45:04.276556 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (653231 virtual)\n",
      "I0228 01:45:04.287021 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (658186 virtual)\n",
      "I0228 01:45:04.307499 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (662891 virtual)\n",
      "I0228 01:45:04.315314 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (668093 virtual)\n",
      "I0228 01:45:04.320637 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (673573 virtual)\n",
      "I0228 01:45:04.334171 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (678608 virtual)\n",
      "I0228 01:45:04.337526 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (683593 virtual)\n",
      "I0228 01:45:04.362620 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (688732 virtual)\n",
      "I0228 01:45:04.367138 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (693759 virtual)\n",
      "I0228 01:45:04.371214 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (698826 virtual)\n",
      "I0228 01:45:04.379183 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (704099 virtual)\n",
      "I0228 01:45:04.383483 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (709397 virtual)\n",
      "I0228 01:45:04.408875 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (714689 virtual)\n",
      "I0228 01:45:04.427546 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (719688 virtual)\n",
      "I0228 01:45:04.432415 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (725141 virtual)\n",
      "I0228 01:45:04.444236 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (730530 virtual)\n",
      "I0228 01:45:04.447708 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (735998 virtual)\n",
      "I0228 01:45:04.459076 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (741508 virtual)\n",
      "I0228 01:45:04.477763 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (746154 virtual)\n",
      "I0228 01:45:04.486345 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (751803 virtual)\n",
      "I0228 01:45:04.491970 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (756875 virtual)\n",
      "I0228 01:45:04.500221 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (762298 virtual)\n",
      "I0228 01:45:04.508079 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (767869 virtual)\n",
      "I0228 01:45:04.524765 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (773693 virtual)\n",
      "I0228 01:45:04.538795 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (779553 virtual)\n",
      "I0228 01:45:04.559908 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (785522 virtual)\n",
      "I0228 01:45:04.581923 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (791389 virtual)\n",
      "I0228 01:45:04.587851 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (796928 virtual)\n",
      "I0228 01:45:04.590781 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (802649 virtual)\n",
      "I0228 01:45:04.593656 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (808203 virtual)\n",
      "I0228 01:45:04.597207 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (814032 virtual)\n",
      "I0228 01:45:04.618057 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (819867 virtual)\n",
      "I0228 01:45:04.622701 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (825834 virtual)\n",
      "I0228 01:45:04.639366 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (831179 virtual)\n",
      "I0228 01:45:04.658032 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (836497 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:04.660781 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (842022 virtual)\n",
      "I0228 01:45:04.675427 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (848311 virtual)\n",
      "I0228 01:45:04.677686 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (854136 virtual)\n",
      "I0228 01:45:04.692915 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (860202 virtual)\n",
      "I0228 01:45:04.716243 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (866136 virtual)\n",
      "I0228 01:45:04.720496 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (871998 virtual)\n",
      "I0228 01:45:04.734789 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (877852 virtual)\n",
      "I0228 01:45:04.739261 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (883859 virtual)\n",
      "I0228 01:45:04.748830 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (889990 virtual)\n",
      "I0228 01:45:04.773572 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (895289 virtual)\n",
      "I0228 01:45:04.778307 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (901075 virtual)\n",
      "I0228 01:45:04.807365 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (908606 virtual)\n",
      "I0228 01:45:04.810096 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (913386 virtual)\n",
      "I0228 01:45:04.812696 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (917260 virtual)\n",
      "I0228 01:45:04.830133 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (920027 virtual)\n",
      "I0228 01:45:04.836662 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (925952 virtual)\n",
      "I0228 01:45:04.855039 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (930816 virtual)\n",
      "I0228 01:45:04.861734 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (934656 virtual)\n",
      "I0228 01:45:04.871863 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (938569 virtual)\n",
      "I0228 01:45:04.883097 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (942520 virtual)\n",
      "I0228 01:45:04.894453 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (946177 virtual)\n",
      "I0228 01:45:04.916972 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (949760 virtual)\n",
      "I0228 01:45:04.927489 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (953500 virtual)\n",
      "I0228 01:45:04.944463 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (957357 virtual)\n",
      "I0228 01:45:04.947157 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (960867 virtual)\n",
      "I0228 01:45:04.950420 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (965157 virtual)\n",
      "I0228 01:45:04.952930 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (968883 virtual)\n",
      "I0228 01:45:04.956063 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (972655 virtual)\n",
      "I0228 01:45:04.961311 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (976390 virtual)\n",
      "I0228 01:45:04.967873 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (980407 virtual)\n",
      "I0228 01:45:04.983303 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (983961 virtual)\n",
      "I0228 01:45:04.987285 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (988135 virtual)\n",
      "I0228 01:45:04.991935 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (991379 virtual)\n",
      "I0228 01:45:05.001311 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (995232 virtual)\n",
      "I0228 01:45:05.003903 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (999563 virtual)\n",
      "I0228 01:45:05.021270 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (1003229 virtual)\n",
      "I0228 01:45:05.025256 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1006799 virtual)\n",
      "I0228 01:45:05.029408 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1010816 virtual)\n",
      "I0228 01:45:05.045138 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1015187 virtual)\n",
      "I0228 01:45:05.049794 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1020444 virtual)\n",
      "I0228 01:45:05.055721 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1024302 virtual)\n",
      "I0228 01:45:05.058268 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1028319 virtual)\n",
      "I0228 01:45:05.062925 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1032208 virtual)\n",
      "I0228 01:45:05.075105 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1036835 virtual)\n",
      "I0228 01:45:05.082747 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1040934 virtual)\n",
      "I0228 01:45:05.091824 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1044697 virtual)\n",
      "I0228 01:45:05.096293 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1049280 virtual)\n",
      "I0228 01:45:05.101156 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1053152 virtual)\n",
      "I0228 01:45:05.116444 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1057190 virtual)\n",
      "I0228 01:45:05.128796 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1060998 virtual)\n",
      "I0228 01:45:05.133973 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1065019 virtual)\n",
      "I0228 01:45:05.139117 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1069734 virtual)\n",
      "I0228 01:45:05.141694 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1073783 virtual)\n",
      "I0228 01:45:05.163053 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1078268 virtual)\n",
      "I0228 01:45:05.165722 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1083205 virtual)\n",
      "I0228 01:45:05.168292 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1087288 virtual)\n",
      "I0228 01:45:05.171870 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1091144 virtual)\n",
      "I0228 01:45:05.174356 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1094665 virtual)\n",
      "I0228 01:45:05.201530 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1099219 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:05.205874 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1102959 virtual)\n",
      "I0228 01:45:05.211286 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1107621 virtual)\n",
      "I0228 01:45:05.215057 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1111739 virtual)\n",
      "I0228 01:45:05.217759 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1116563 virtual)\n",
      "I0228 01:45:05.241761 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1120726 virtual)\n",
      "I0228 01:45:05.247996 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1125272 virtual)\n",
      "I0228 01:45:05.252390 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1130303 virtual)\n",
      "I0228 01:45:05.256637 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1134309 virtual)\n",
      "I0228 01:45:05.259532 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1140133 virtual)\n",
      "I0228 01:45:05.284420 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1144163 virtual)\n",
      "I0228 01:45:05.288208 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1148135 virtual)\n",
      "I0228 01:45:05.291459 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1152053 virtual)\n",
      "I0228 01:45:05.294919 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1156629 virtual)\n",
      "I0228 01:45:05.301381 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1160355 virtual)\n",
      "I0228 01:45:05.326321 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1165296 virtual)\n",
      "I0228 01:45:05.342206 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1169874 virtual)\n",
      "I0228 01:45:05.345474 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1173649 virtual)\n",
      "I0228 01:45:05.348231 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1178551 virtual)\n",
      "I0228 01:45:05.362088 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1183460 virtual)\n",
      "I0228 01:45:05.368148 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1187340 virtual)\n",
      "I0228 01:45:05.372263 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1191856 virtual)\n",
      "I0228 01:45:05.375992 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1196556 virtual)\n",
      "I0228 01:45:05.383746 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1200661 virtual)\n",
      "I0228 01:45:05.397166 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1204803 virtual)\n",
      "I0228 01:45:05.405640 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1209331 virtual)\n",
      "I0228 01:45:05.415992 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1213542 virtual)\n",
      "I0228 01:45:05.420192 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1218225 virtual)\n",
      "I0228 01:45:05.430139 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1221942 virtual)\n",
      "I0228 01:45:05.440215 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1225681 virtual)\n",
      "I0228 01:45:05.444133 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1228488 virtual)\n",
      "I0228 01:45:05.457330 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1232891 virtual)\n",
      "I0228 01:45:05.460229 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1237080 virtual)\n",
      "I0228 01:45:05.463668 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1241796 virtual)\n",
      "I0228 01:45:05.476847 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1246992 virtual)\n",
      "I0228 01:45:05.486997 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1251365 virtual)\n",
      "I0228 01:45:05.493272 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1255424 virtual)\n",
      "I0228 01:45:05.498488 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1259531 virtual)\n",
      "I0228 01:45:05.508433 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1263662 virtual)\n",
      "I0228 01:45:05.511289 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1267436 virtual)\n",
      "I0228 01:45:05.514773 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1271449 virtual)\n",
      "I0228 01:45:05.532653 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1275531 virtual)\n",
      "I0228 01:45:05.536168 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1280150 virtual)\n",
      "I0228 01:45:05.552119 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1283983 virtual)\n",
      "I0228 01:45:05.555775 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1288758 virtual)\n",
      "I0228 01:45:05.559998 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1292927 virtual)\n",
      "I0228 01:45:05.570176 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1296630 virtual)\n",
      "I0228 01:45:05.573065 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1301231 virtual)\n",
      "I0228 01:45:05.591510 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1306194 virtual)\n",
      "I0228 01:45:05.595220 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1309652 virtual)\n",
      "I0228 01:45:05.600106 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1313892 virtual)\n",
      "I0228 01:45:05.606736 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1318064 virtual)\n",
      "I0228 01:45:05.615922 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1321582 virtual)\n",
      "I0228 01:45:05.628462 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1325491 virtual)\n",
      "I0228 01:45:05.632919 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1329466 virtual)\n",
      "I0228 01:45:05.637227 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1333752 virtual)\n",
      "I0228 01:45:05.640035 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1338537 virtual)\n",
      "I0228 01:45:05.655836 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1342641 virtual)\n",
      "I0228 01:45:05.664132 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1347248 virtual)\n",
      "I0228 01:45:05.669764 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1352570 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:05.675379 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1354399 virtual)\n",
      "I0228 01:45:05.677446 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1359721 virtual)\n",
      "I0228 01:45:05.688177 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1364987 virtual)\n",
      "I0228 01:45:05.699786 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1369845 virtual)\n",
      "I0228 01:45:05.711151 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1374263 virtual)\n",
      "I0228 01:45:05.716462 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1378463 virtual)\n",
      "I0228 01:45:05.722605 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1383574 virtual)\n",
      "I0228 01:45:05.725652 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1387419 virtual)\n",
      "I0228 01:45:05.736240 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1392661 virtual)\n",
      "I0228 01:45:05.740093 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1397921 virtual)\n",
      "I0228 01:45:05.764482 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1401409 virtual)\n",
      "I0228 01:45:05.774825 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1406413 virtual)\n",
      "I0228 01:45:05.779476 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1411399 virtual)\n",
      "I0228 01:45:05.783121 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1415442 virtual)\n",
      "I0228 01:45:05.786345 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1420363 virtual)\n",
      "I0228 01:45:05.804070 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1424948 virtual)\n",
      "I0228 01:45:05.815403 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1429447 virtual)\n",
      "I0228 01:45:05.825329 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1434665 virtual)\n",
      "I0228 01:45:05.830789 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1439861 virtual)\n",
      "I0228 01:45:05.835037 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1445882 virtual)\n",
      "I0228 01:45:05.838479 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1450500 virtual)\n",
      "I0228 01:45:05.867502 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1456485 virtual)\n",
      "I0228 01:45:05.871889 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1461703 virtual)\n",
      "I0228 01:45:05.877933 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1469361 virtual)\n",
      "I0228 01:45:05.883573 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1476221 virtual)\n",
      "I0228 01:45:05.886359 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1481441 virtual)\n",
      "I0228 01:45:05.908253 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1486437 virtual)\n",
      "I0228 01:45:05.919438 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1491757 virtual)\n",
      "I0228 01:45:05.923996 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1497248 virtual)\n",
      "I0228 01:45:05.929446 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1502128 virtual)\n",
      "I0228 01:45:05.933667 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1507271 virtual)\n",
      "I0228 01:45:05.966579 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1512481 virtual)\n",
      "I0228 01:45:05.972688 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1522709 virtual)\n",
      "I0228 01:45:05.980123 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1528136 virtual)\n",
      "I0228 01:45:05.985552 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1533763 virtual)\n",
      "I0228 01:45:05.988911 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1539103 virtual)\n",
      "I0228 01:45:06.011721 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1543688 virtual)\n",
      "I0228 01:45:06.021057 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1548446 virtual)\n",
      "I0228 01:45:06.031641 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1553325 virtual)\n",
      "I0228 01:45:06.036642 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1558443 virtual)\n",
      "I0228 01:45:06.041420 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1563103 virtual)\n",
      "I0228 01:45:06.059152 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1568684 virtual)\n",
      "I0228 01:45:06.084320 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1574898 virtual)\n",
      "I0228 01:45:06.088376 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1580425 virtual)\n",
      "I0228 01:45:06.092661 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1585993 virtual)\n",
      "I0228 01:45:06.106352 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1591543 virtual)\n",
      "I0228 01:45:06.114352 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1597531 virtual)\n",
      "I0228 01:45:06.130301 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1603744 virtual)\n",
      "I0228 01:45:06.134827 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1608422 virtual)\n",
      "I0228 01:45:06.138747 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1613665 virtual)\n",
      "I0228 01:45:06.148240 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1618647 virtual)\n",
      "I0228 01:45:06.167564 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1624042 virtual)\n",
      "I0228 01:45:06.188369 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1629266 virtual)\n",
      "I0228 01:45:06.193193 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1634940 virtual)\n",
      "I0228 01:45:06.200324 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1640007 virtual)\n",
      "I0228 01:45:06.203711 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1645821 virtual)\n",
      "I0228 01:45:06.223469 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1650769 virtual)\n",
      "I0228 01:45:06.233890 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1657166 virtual)\n",
      "I0228 01:45:06.241286 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1665431 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:06.245628 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1673997 virtual)\n",
      "I0228 01:45:06.253290 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1682115 virtual)\n",
      "I0228 01:45:06.274119 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1690012 virtual)\n",
      "I0228 01:45:06.282790 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1695835 virtual)\n",
      "I0228 01:45:06.294785 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1700765 virtual)\n",
      "I0228 01:45:06.297922 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1707264 virtual)\n",
      "I0228 01:45:06.308911 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1713026 virtual)\n",
      "I0228 01:45:06.322751 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1719018 virtual)\n",
      "I0228 01:45:06.350031 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1725005 virtual)\n",
      "I0228 01:45:06.378864 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1730702 virtual)\n",
      "I0228 01:45:06.383088 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1735688 virtual)\n",
      "I0228 01:45:06.388374 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1741386 virtual)\n",
      "I0228 01:45:06.402586 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1746441 virtual)\n",
      "I0228 01:45:06.411602 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1751660 virtual)\n",
      "I0228 01:45:06.427894 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1756328 virtual)\n",
      "I0228 01:45:06.442153 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1761791 virtual)\n",
      "I0228 01:45:06.447359 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1767189 virtual)\n",
      "I0228 01:45:06.458834 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1772412 virtual)\n",
      "I0228 01:45:06.471171 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1777370 virtual)\n",
      "I0228 01:45:06.483679 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1782091 virtual)\n",
      "I0228 01:45:06.489825 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1787359 virtual)\n",
      "I0228 01:45:06.507625 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1792307 virtual)\n",
      "I0228 01:45:06.520734 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1797401 virtual)\n",
      "I0228 01:45:06.525738 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1803060 virtual)\n",
      "I0228 01:45:06.531295 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1808000 virtual)\n",
      "I0228 01:45:06.546001 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1813200 virtual)\n",
      "I0228 01:45:06.561946 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1818223 virtual)\n",
      "I0228 01:45:06.583372 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1823169 virtual)\n",
      "I0228 01:45:06.586582 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1828467 virtual)\n",
      "I0228 01:45:06.589491 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1833584 virtual)\n",
      "I0228 01:45:06.603603 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1838692 virtual)\n",
      "I0228 01:45:06.611569 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1844545 virtual)\n",
      "I0228 01:45:06.625412 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1848850 virtual)\n",
      "I0228 01:45:06.636586 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1853907 virtual)\n",
      "I0228 01:45:06.640997 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1858683 virtual)\n",
      "I0228 01:45:06.657051 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1864020 virtual)\n",
      "I0228 01:45:06.663841 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1868658 virtual)\n",
      "I0228 01:45:06.676364 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1873905 virtual)\n",
      "I0228 01:45:06.699776 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1879052 virtual)\n",
      "I0228 01:45:06.702689 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1884478 virtual)\n",
      "I0228 01:45:06.712216 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1889829 virtual)\n",
      "I0228 01:45:06.720912 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1894821 virtual)\n",
      "I0228 01:45:06.724347 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1899737 virtual)\n",
      "I0228 01:45:06.742479 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1905048 virtual)\n",
      "I0228 01:45:06.748442 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1910043 virtual)\n",
      "I0228 01:45:06.769005 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1914718 virtual)\n",
      "I0228 01:45:06.773405 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1919440 virtual)\n",
      "I0228 01:45:06.778670 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1924671 virtual)\n",
      "I0228 01:45:06.794898 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1929771 virtual)\n",
      "I0228 01:45:06.813247 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1934598 virtual)\n",
      "I0228 01:45:06.820432 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1940006 virtual)\n",
      "I0228 01:45:06.824720 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1944798 virtual)\n",
      "I0228 01:45:06.827852 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1950073 virtual)\n",
      "I0228 01:45:06.848473 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1955439 virtual)\n",
      "I0228 01:45:06.867306 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1960419 virtual)\n",
      "I0228 01:45:06.871849 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1965274 virtual)\n",
      "I0228 01:45:06.876888 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1970389 virtual)\n",
      "I0228 01:45:06.882597 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1975732 virtual)\n",
      "I0228 01:45:06.900286 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1980975 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:06.916461 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1986170 virtual)\n",
      "I0228 01:45:06.921535 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1991509 virtual)\n",
      "I0228 01:45:06.928616 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1996870 virtual)\n",
      "I0228 01:45:06.939192 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (2002135 virtual)\n",
      "I0228 01:45:06.953851 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (2007372 virtual)\n",
      "I0228 01:45:06.971129 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (2012657 virtual)\n",
      "I0228 01:45:06.976278 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (2017848 virtual)\n",
      "I0228 01:45:06.982214 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (2022691 virtual)\n",
      "I0228 01:45:06.997267 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (2027659 virtual)\n",
      "I0228 01:45:07.007615 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2032563 virtual)\n",
      "I0228 01:45:07.025668 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2037255 virtual)\n",
      "I0228 01:45:07.029900 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2042070 virtual)\n",
      "I0228 01:45:07.039108 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2046741 virtual)\n",
      "I0228 01:45:07.052975 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2051521 virtual)\n",
      "I0228 01:45:07.061424 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2057030 virtual)\n",
      "I0228 01:45:07.079638 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2061983 virtual)\n",
      "I0228 01:45:07.084580 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2067375 virtual)\n",
      "I0228 01:45:07.091007 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2072649 virtual)\n",
      "I0228 01:45:07.106051 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2078012 virtual)\n",
      "I0228 01:45:07.112802 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2083154 virtual)\n",
      "I0228 01:45:07.131706 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2088531 virtual)\n",
      "I0228 01:45:07.136131 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2093868 virtual)\n",
      "I0228 01:45:07.141393 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2098455 virtual)\n",
      "I0228 01:45:07.156060 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2103551 virtual)\n",
      "I0228 01:45:07.168979 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2108295 virtual)\n",
      "I0228 01:45:07.181329 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2113073 virtual)\n",
      "I0228 01:45:07.185025 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2117927 virtual)\n",
      "I0228 01:45:07.197432 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2122987 virtual)\n",
      "I0228 01:45:07.211454 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2128277 virtual)\n",
      "I0228 01:45:07.219304 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2133626 virtual)\n",
      "I0228 01:45:07.238991 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2138774 virtual)\n",
      "I0228 01:45:07.244515 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2143971 virtual)\n",
      "I0228 01:45:07.250431 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2149232 virtual)\n",
      "I0228 01:45:07.263551 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2154179 virtual)\n",
      "I0228 01:45:07.268146 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2159382 virtual)\n",
      "I0228 01:45:07.291149 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2164307 virtual)\n",
      "I0228 01:45:07.296070 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2169221 virtual)\n",
      "I0228 01:45:07.300202 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2174455 virtual)\n",
      "I0228 01:45:07.320893 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2179123 virtual)\n",
      "I0228 01:45:07.325073 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2184437 virtual)\n",
      "I0228 01:45:07.353353 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2189842 virtual)\n",
      "I0228 01:45:07.357229 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2194915 virtual)\n",
      "I0228 01:45:07.360178 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2200443 virtual)\n",
      "I0228 01:45:07.372622 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2205569 virtual)\n",
      "I0228 01:45:07.378217 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2210957 virtual)\n",
      "I0228 01:45:07.397131 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2215701 virtual)\n",
      "I0228 01:45:07.401385 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2220607 virtual)\n",
      "I0228 01:45:07.405970 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2225961 virtual)\n",
      "I0228 01:45:07.420349 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2231214 virtual)\n",
      "I0228 01:45:07.435375 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2236121 virtual)\n",
      "I0228 01:45:07.448078 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2241263 virtual)\n",
      "I0228 01:45:07.452274 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2246426 virtual)\n",
      "I0228 01:45:07.465790 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2251259 virtual)\n",
      "I0228 01:45:07.477376 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2256293 virtual)\n",
      "I0228 01:45:07.491674 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2261223 virtual)\n",
      "I0228 01:45:07.499377 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2266170 virtual)\n",
      "I0228 01:45:07.504000 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2271371 virtual)\n",
      "I0228 01:45:07.520172 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2276257 virtual)\n",
      "I0228 01:45:07.529536 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2281260 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:07.541234 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2286765 virtual)\n",
      "I0228 01:45:07.550889 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2291734 virtual)\n",
      "I0228 01:45:07.555662 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2296667 virtual)\n",
      "I0228 01:45:07.572479 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2301894 virtual)\n",
      "I0228 01:45:07.580677 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2307202 virtual)\n",
      "I0228 01:45:07.590871 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2312413 virtual)\n",
      "I0228 01:45:07.602771 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2317598 virtual)\n",
      "I0228 01:45:07.609255 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2322610 virtual)\n",
      "I0228 01:45:07.625538 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2327863 virtual)\n",
      "I0228 01:45:07.633453 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2333386 virtual)\n",
      "I0228 01:45:07.648615 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2338432 virtual)\n",
      "I0228 01:45:07.653438 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2343293 virtual)\n",
      "I0228 01:45:07.658435 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2348095 virtual)\n",
      "I0228 01:45:07.678798 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2353050 virtual)\n",
      "I0228 01:45:07.686963 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2358668 virtual)\n",
      "I0228 01:45:07.700271 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2363386 virtual)\n",
      "I0228 01:45:07.708113 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2368366 virtual)\n",
      "I0228 01:45:07.712795 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2373887 virtual)\n",
      "I0228 01:45:07.729231 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2379026 virtual)\n",
      "I0228 01:45:07.746257 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2384128 virtual)\n",
      "I0228 01:45:07.752590 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2389026 virtual)\n",
      "I0228 01:45:07.757544 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2394100 virtual)\n",
      "I0228 01:45:07.761135 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2399067 virtual)\n",
      "I0228 01:45:07.781662 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2404122 virtual)\n",
      "I0228 01:45:07.803782 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2409421 virtual)\n",
      "I0228 01:45:07.808867 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2414879 virtual)\n",
      "I0228 01:45:07.812917 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2419941 virtual)\n",
      "I0228 01:45:07.819288 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2424807 virtual)\n",
      "I0228 01:45:07.835176 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2430035 virtual)\n",
      "I0228 01:45:07.852848 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2435370 virtual)\n",
      "I0228 01:45:07.857770 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2440157 virtual)\n",
      "I0228 01:45:07.862949 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2445014 virtual)\n",
      "I0228 01:45:07.871001 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2450253 virtual)\n",
      "I0228 01:45:07.887430 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2455022 virtual)\n",
      "I0228 01:45:07.909735 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2460539 virtual)\n",
      "I0228 01:45:07.914496 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2466109 virtual)\n",
      "I0228 01:45:07.919208 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2471042 virtual)\n",
      "I0228 01:45:07.922975 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2476536 virtual)\n",
      "I0228 01:45:07.941350 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2481185 virtual)\n",
      "I0228 01:45:07.962878 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2485843 virtual)\n",
      "I0228 01:45:07.968221 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2491075 virtual)\n",
      "I0228 01:45:07.973578 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2496606 virtual)\n",
      "I0228 01:45:07.976984 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2501816 virtual)\n",
      "I0228 01:45:07.988936 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2506944 virtual)\n",
      "I0228 01:45:08.016560 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2511731 virtual)\n",
      "I0228 01:45:08.020844 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2517064 virtual)\n",
      "I0228 01:45:08.024822 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2522115 virtual)\n",
      "I0228 01:45:08.031418 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2526988 virtual)\n",
      "I0228 01:45:08.034613 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2532076 virtual)\n",
      "I0228 01:45:08.064991 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2537114 virtual)\n",
      "I0228 01:45:08.071514 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2542142 virtual)\n",
      "I0228 01:45:08.084093 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2547388 virtual)\n",
      "I0228 01:45:08.088805 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2552206 virtual)\n",
      "I0228 01:45:08.093953 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2557711 virtual)\n",
      "I0228 01:45:08.119386 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2563072 virtual)\n",
      "I0228 01:45:08.127825 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2567882 virtual)\n",
      "I0228 01:45:08.133390 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2572944 virtual)\n",
      "I0228 01:45:08.136482 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2578502 virtual)\n",
      "I0228 01:45:08.139893 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2583889 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:08.162142 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2589126 virtual)\n",
      "I0228 01:45:08.180784 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2594627 virtual)\n",
      "I0228 01:45:08.185182 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2599795 virtual)\n",
      "I0228 01:45:08.190217 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2605354 virtual)\n",
      "I0228 01:45:08.196510 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2610365 virtual)\n",
      "I0228 01:45:08.222403 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2615308 virtual)\n",
      "I0228 01:45:08.228327 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2620404 virtual)\n",
      "I0228 01:45:08.240779 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2625624 virtual)\n",
      "I0228 01:45:08.251519 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2630213 virtual)\n",
      "I0228 01:45:08.255211 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2635449 virtual)\n",
      "I0228 01:45:08.275026 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2640224 virtual)\n",
      "I0228 01:45:08.285012 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2645344 virtual)\n",
      "I0228 01:45:08.298187 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2650650 virtual)\n",
      "I0228 01:45:08.307795 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2655937 virtual)\n",
      "I0228 01:45:08.312312 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2661380 virtual)\n",
      "I0228 01:45:08.325955 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2666639 virtual)\n",
      "I0228 01:45:08.336521 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2672024 virtual)\n",
      "I0228 01:45:08.359536 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2677211 virtual)\n",
      "I0228 01:45:08.363682 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2682670 virtual)\n",
      "I0228 01:45:08.369443 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2687610 virtual)\n",
      "I0228 01:45:08.373702 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2692675 virtual)\n",
      "I0228 01:45:08.388267 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2697872 virtual)\n",
      "I0228 01:45:08.408329 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2703094 virtual)\n",
      "I0228 01:45:08.413740 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2708654 virtual)\n",
      "I0228 01:45:08.425808 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2714158 virtual)\n",
      "I0228 01:45:08.430160 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2719809 virtual)\n",
      "I0228 01:45:08.441246 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2724917 virtual)\n",
      "I0228 01:45:08.463225 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2729842 virtual)\n",
      "I0228 01:45:08.468812 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2734933 virtual)\n",
      "I0228 01:45:08.480877 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2739995 virtual)\n",
      "I0228 01:45:08.485070 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2745291 virtual)\n",
      "I0228 01:45:08.494010 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2750809 virtual)\n",
      "I0228 01:45:08.516358 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2755841 virtual)\n",
      "I0228 01:45:08.522626 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2760699 virtual)\n",
      "I0228 01:45:08.534011 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2760789 virtual)\n",
      "I0228 01:45:08.586917 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:45:08.596249 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:45:08.598672 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:45:08.606210 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:45:08.620911 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:45:08.627859 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:45:08.635879 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:45:08.591275 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:45:08.623807 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:45:08.615369 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:45:09.029399 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:45:09.045217 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2761086 virtual documents\n",
      "I0228 01:45:09.132438 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:45:09.504137 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (5080 virtual)\n",
      "I0228 01:45:09.508352 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10414 virtual)\n",
      "I0228 01:45:09.512185 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16449 virtual)\n",
      "I0228 01:45:09.514796 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21306 virtual)\n",
      "I0228 01:45:09.517508 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27023 virtual)\n",
      "I0228 01:45:09.519569 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32224 virtual)\n",
      "I0228 01:45:09.522456 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37895 virtual)\n",
      "I0228 01:45:09.524752 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43621 virtual)\n",
      "I0228 01:45:09.526928 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48845 virtual)\n",
      "I0228 01:45:09.529221 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54503 virtual)\n",
      "I0228 01:45:09.567063 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60366 virtual)\n",
      "I0228 01:45:09.570142 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65737 virtual)\n",
      "I0228 01:45:09.581940 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70907 virtual)\n",
      "I0228 01:45:09.587634 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (76109 virtual)\n",
      "I0228 01:45:09.591141 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81738 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:09.624587 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87859 virtual)\n",
      "I0228 01:45:09.629858 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93775 virtual)\n",
      "I0228 01:45:09.640890 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (99405 virtual)\n",
      "I0228 01:45:09.645882 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104476 virtual)\n",
      "I0228 01:45:09.650422 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109888 virtual)\n",
      "I0228 01:45:09.683115 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (115369 virtual)\n",
      "I0228 01:45:09.691989 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120625 virtual)\n",
      "I0228 01:45:09.696627 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (126240 virtual)\n",
      "I0228 01:45:09.701973 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (132209 virtual)\n",
      "I0228 01:45:09.710906 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137837 virtual)\n",
      "I0228 01:45:09.751519 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (143025 virtual)\n",
      "I0228 01:45:09.754473 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148615 virtual)\n",
      "I0228 01:45:09.757845 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154689 virtual)\n",
      "I0228 01:45:09.769191 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160627 virtual)\n",
      "I0228 01:45:09.772152 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (166143 virtual)\n",
      "I0228 01:45:09.801884 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (171487 virtual)\n",
      "I0228 01:45:09.808629 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176923 virtual)\n",
      "I0228 01:45:09.812678 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (182229 virtual)\n",
      "I0228 01:45:09.816840 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (187561 virtual)\n",
      "I0228 01:45:09.826802 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (193421 virtual)\n",
      "I0228 01:45:09.855647 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (198625 virtual)\n",
      "I0228 01:45:09.867959 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (204318 virtual)\n",
      "I0228 01:45:09.874356 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (209995 virtual)\n",
      "I0228 01:45:09.878530 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (215361 virtual)\n",
      "I0228 01:45:09.882850 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220917 virtual)\n",
      "I0228 01:45:09.912384 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (228625 virtual)\n",
      "I0228 01:45:09.926084 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (234048 virtual)\n",
      "I0228 01:45:09.930468 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (239489 virtual)\n",
      "I0228 01:45:09.935720 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (245251 virtual)\n",
      "I0228 01:45:09.941472 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (250762 virtual)\n",
      "I0228 01:45:09.975366 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (256041 virtual)\n",
      "I0228 01:45:09.987203 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (261420 virtual)\n",
      "I0228 01:45:09.990748 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (266680 virtual)\n",
      "I0228 01:45:09.992679 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (272256 virtual)\n",
      "I0228 01:45:09.997752 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (277666 virtual)\n",
      "I0228 01:45:10.041588 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (283051 virtual)\n",
      "I0228 01:45:10.045627 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288857 virtual)\n",
      "I0228 01:45:10.051386 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (294057 virtual)\n",
      "I0228 01:45:10.056340 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (299477 virtual)\n",
      "I0228 01:45:10.059426 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304828 virtual)\n",
      "I0228 01:45:10.094719 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (310452 virtual)\n",
      "I0228 01:45:10.103631 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (316106 virtual)\n",
      "I0228 01:45:10.108693 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (321778 virtual)\n",
      "I0228 01:45:10.113795 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (327111 virtual)\n",
      "I0228 01:45:10.116977 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (333591 virtual)\n",
      "I0228 01:45:10.152859 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (339290 virtual)\n",
      "I0228 01:45:10.158014 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (345053 virtual)\n",
      "I0228 01:45:10.162966 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (350650 virtual)\n",
      "I0228 01:45:10.167587 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (356731 virtual)\n",
      "I0228 01:45:10.171280 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (362532 virtual)\n",
      "I0228 01:45:10.212399 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (368103 virtual)\n",
      "I0228 01:45:10.218281 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (374074 virtual)\n",
      "I0228 01:45:10.223968 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (379711 virtual)\n",
      "I0228 01:45:10.227113 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (385645 virtual)\n",
      "I0228 01:45:10.234333 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (390277 virtual)\n",
      "I0228 01:45:10.268040 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (395407 virtual)\n",
      "I0228 01:45:10.274744 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (399135 virtual)\n",
      "I0228 01:45:10.280250 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (402520 virtual)\n",
      "I0228 01:45:10.284601 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (407029 virtual)\n",
      "I0228 01:45:10.294826 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (411385 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:10.320630 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (415669 virtual)\n",
      "I0228 01:45:10.337556 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (420069 virtual)\n",
      "I0228 01:45:10.340264 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (424569 virtual)\n",
      "I0228 01:45:10.345148 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (428850 virtual)\n",
      "I0228 01:45:10.348513 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (433515 virtual)\n",
      "I0228 01:45:10.369515 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (438330 virtual)\n",
      "I0228 01:45:10.380451 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (443074 virtual)\n",
      "I0228 01:45:10.383224 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (447643 virtual)\n",
      "I0228 01:45:10.389697 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (451945 virtual)\n",
      "I0228 01:45:10.392480 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (455575 virtual)\n",
      "I0228 01:45:10.410234 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (459304 virtual)\n",
      "I0228 01:45:10.425410 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (463390 virtual)\n",
      "I0228 01:45:10.429677 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (467683 virtual)\n",
      "I0228 01:45:10.436423 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (472539 virtual)\n",
      "I0228 01:45:10.440072 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (477187 virtual)\n",
      "I0228 01:45:10.455542 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (481753 virtual)\n",
      "I0228 01:45:10.470022 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (486331 virtual)\n",
      "I0228 01:45:10.475108 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (491124 virtual)\n",
      "I0228 01:45:10.479867 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (495829 virtual)\n",
      "I0228 01:45:10.489413 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (500137 virtual)\n",
      "I0228 01:45:10.492605 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (504446 virtual)\n",
      "I0228 01:45:10.510756 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (509797 virtual)\n",
      "I0228 01:45:10.514569 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (514851 virtual)\n",
      "I0228 01:45:10.525675 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (519254 virtual)\n",
      "I0228 01:45:10.532239 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (523599 virtual)\n",
      "I0228 01:45:10.536346 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (528238 virtual)\n",
      "I0228 01:45:10.558805 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (532162 virtual)\n",
      "I0228 01:45:10.563179 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (536491 virtual)\n",
      "I0228 01:45:10.573381 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (540697 virtual)\n",
      "I0228 01:45:10.578072 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (545094 virtual)\n",
      "I0228 01:45:10.581197 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (549444 virtual)\n",
      "I0228 01:45:10.613464 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (553545 virtual)\n",
      "I0228 01:45:10.618365 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (558314 virtual)\n",
      "I0228 01:45:10.621879 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (562133 virtual)\n",
      "I0228 01:45:10.626871 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (567041 virtual)\n",
      "I0228 01:45:10.629985 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (570920 virtual)\n",
      "I0228 01:45:10.662208 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (574953 virtual)\n",
      "I0228 01:45:10.666929 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (578584 virtual)\n",
      "I0228 01:45:10.669447 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (581849 virtual)\n",
      "I0228 01:45:10.672134 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (586763 virtual)\n",
      "I0228 01:45:10.676516 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (592286 virtual)\n",
      "I0228 01:45:10.696504 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (597645 virtual)\n",
      "I0228 01:45:10.706399 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (602263 virtual)\n",
      "I0228 01:45:10.714548 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (608268 virtual)\n",
      "I0228 01:45:10.719254 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (613636 virtual)\n",
      "I0228 01:45:10.724170 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (618872 virtual)\n",
      "I0228 01:45:10.737270 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (623186 virtual)\n",
      "I0228 01:45:10.747860 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (627812 virtual)\n",
      "I0228 01:45:10.753119 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (631814 virtual)\n",
      "I0228 01:45:10.768093 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (637236 virtual)\n",
      "I0228 01:45:10.784420 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (642534 virtual)\n",
      "I0228 01:45:10.793042 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (647613 virtual)\n",
      "I0228 01:45:10.799201 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (653231 virtual)\n",
      "I0228 01:45:10.814940 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (658186 virtual)\n",
      "I0228 01:45:10.828222 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (662891 virtual)\n",
      "I0228 01:45:10.838602 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (668093 virtual)\n",
      "I0228 01:45:10.842861 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (673573 virtual)\n",
      "I0228 01:45:10.847185 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (678608 virtual)\n",
      "I0228 01:45:10.858048 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (683593 virtual)\n",
      "I0228 01:45:10.883539 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (688732 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:10.888658 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (693759 virtual)\n",
      "I0228 01:45:10.892475 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (698826 virtual)\n",
      "I0228 01:45:10.905161 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (704099 virtual)\n",
      "I0228 01:45:10.909256 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (709397 virtual)\n",
      "I0228 01:45:10.928414 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (714689 virtual)\n",
      "I0228 01:45:10.943119 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (719688 virtual)\n",
      "I0228 01:45:10.947607 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (725141 virtual)\n",
      "I0228 01:45:10.955235 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (730530 virtual)\n",
      "I0228 01:45:10.959428 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (735998 virtual)\n",
      "I0228 01:45:10.978357 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (741508 virtual)\n",
      "I0228 01:45:10.995097 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (746154 virtual)\n",
      "I0228 01:45:10.999997 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (751803 virtual)\n",
      "I0228 01:45:11.008416 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (756875 virtual)\n",
      "I0228 01:45:11.012979 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (762298 virtual)\n",
      "I0228 01:45:11.026539 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (767869 virtual)\n",
      "I0228 01:45:11.041742 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (773693 virtual)\n",
      "I0228 01:45:11.047835 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (779553 virtual)\n",
      "I0228 01:45:11.062055 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (785522 virtual)\n",
      "I0228 01:45:11.067269 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (791389 virtual)\n",
      "I0228 01:45:11.077934 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (796928 virtual)\n",
      "I0228 01:45:11.083234 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (802649 virtual)\n",
      "I0228 01:45:11.100940 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (808203 virtual)\n",
      "I0228 01:45:11.111393 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (814032 virtual)\n",
      "I0228 01:45:11.124614 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (819867 virtual)\n",
      "I0228 01:45:11.134966 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (825834 virtual)\n",
      "I0228 01:45:11.144882 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (831179 virtual)\n",
      "I0228 01:45:11.165162 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (836497 virtual)\n",
      "I0228 01:45:11.175402 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (842022 virtual)\n",
      "I0228 01:45:11.181291 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (848311 virtual)\n",
      "I0228 01:45:11.198853 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (854136 virtual)\n",
      "I0228 01:45:11.202753 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (860202 virtual)\n",
      "I0228 01:45:11.222350 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (866136 virtual)\n",
      "I0228 01:45:11.232434 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (871998 virtual)\n",
      "I0228 01:45:11.239354 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (877852 virtual)\n",
      "I0228 01:45:11.259852 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (883859 virtual)\n",
      "I0228 01:45:11.264397 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (889990 virtual)\n",
      "I0228 01:45:11.278723 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (895289 virtual)\n",
      "I0228 01:45:11.297374 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (901075 virtual)\n",
      "I0228 01:45:11.300612 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (908606 virtual)\n",
      "I0228 01:45:11.317465 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (913386 virtual)\n",
      "I0228 01:45:11.323582 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (917260 virtual)\n",
      "I0228 01:45:11.334079 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (920027 virtual)\n",
      "I0228 01:45:11.345319 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (925952 virtual)\n",
      "I0228 01:45:11.353758 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (930816 virtual)\n",
      "I0228 01:45:11.379442 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (934656 virtual)\n",
      "I0228 01:45:11.384399 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (938569 virtual)\n",
      "I0228 01:45:11.386307 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (942520 virtual)\n",
      "I0228 01:45:11.402814 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (946177 virtual)\n",
      "I0228 01:45:11.415084 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (949760 virtual)\n",
      "I0228 01:45:11.425035 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (953500 virtual)\n",
      "I0228 01:45:11.429668 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (957357 virtual)\n",
      "I0228 01:45:11.433938 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (960867 virtual)\n",
      "I0228 01:45:11.463238 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (965157 virtual)\n",
      "I0228 01:45:11.467227 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (968883 virtual)\n",
      "I0228 01:45:11.471989 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (972655 virtual)\n",
      "I0228 01:45:11.476637 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (976390 virtual)\n",
      "I0228 01:45:11.479429 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (980407 virtual)\n",
      "I0228 01:45:11.496099 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (983961 virtual)\n",
      "I0228 01:45:11.499691 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (988135 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:11.502186 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (991379 virtual)\n",
      "I0228 01:45:11.504696 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (995232 virtual)\n",
      "I0228 01:45:11.507284 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (999563 virtual)\n",
      "I0228 01:45:11.533487 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (1003229 virtual)\n",
      "I0228 01:45:11.537873 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1006799 virtual)\n",
      "I0228 01:45:11.542707 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1010816 virtual)\n",
      "I0228 01:45:11.547556 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1015187 virtual)\n",
      "I0228 01:45:11.550414 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1020444 virtual)\n",
      "I0228 01:45:11.566025 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1024302 virtual)\n",
      "I0228 01:45:11.569080 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1028319 virtual)\n",
      "I0228 01:45:11.573090 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1032208 virtual)\n",
      "I0228 01:45:11.577526 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1036835 virtual)\n",
      "I0228 01:45:11.583152 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1040934 virtual)\n",
      "I0228 01:45:11.601162 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1044697 virtual)\n",
      "I0228 01:45:11.605139 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1049280 virtual)\n",
      "I0228 01:45:11.610180 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1053152 virtual)\n",
      "I0228 01:45:11.613604 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1057190 virtual)\n",
      "I0228 01:45:11.629242 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1060998 virtual)\n",
      "I0228 01:45:11.637516 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1065019 virtual)\n",
      "I0228 01:45:11.641846 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1069734 virtual)\n",
      "I0228 01:45:11.647814 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1073783 virtual)\n",
      "I0228 01:45:11.658533 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1078268 virtual)\n",
      "I0228 01:45:11.668907 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1083205 virtual)\n",
      "I0228 01:45:11.677903 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1087288 virtual)\n",
      "I0228 01:45:11.681556 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1091144 virtual)\n",
      "I0228 01:45:11.684264 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1094665 virtual)\n",
      "I0228 01:45:11.695580 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1099219 virtual)\n",
      "I0228 01:45:11.705173 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1102959 virtual)\n",
      "I0228 01:45:11.708877 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1107621 virtual)\n",
      "I0228 01:45:11.723208 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1111739 virtual)\n",
      "I0228 01:45:11.727768 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1116563 virtual)\n",
      "I0228 01:45:11.740846 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1120726 virtual)\n",
      "I0228 01:45:11.744532 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1125272 virtual)\n",
      "I0228 01:45:11.748914 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1130303 virtual)\n",
      "I0228 01:45:11.757132 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1134309 virtual)\n",
      "I0228 01:45:11.764521 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1140133 virtual)\n",
      "I0228 01:45:11.778723 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1144163 virtual)\n",
      "I0228 01:45:11.783577 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1148135 virtual)\n",
      "I0228 01:45:11.789497 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1152053 virtual)\n",
      "I0228 01:45:11.799486 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1156629 virtual)\n",
      "I0228 01:45:11.812816 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1160355 virtual)\n",
      "I0228 01:45:11.819923 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1165296 virtual)\n",
      "I0228 01:45:11.827438 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1169874 virtual)\n",
      "I0228 01:45:11.837968 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1173649 virtual)\n",
      "I0228 01:45:11.842780 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1178551 virtual)\n",
      "I0228 01:45:11.861331 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1183460 virtual)\n",
      "I0228 01:45:11.865908 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1187340 virtual)\n",
      "I0228 01:45:11.872937 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1191856 virtual)\n",
      "I0228 01:45:11.876487 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1196556 virtual)\n",
      "I0228 01:45:11.885701 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1200661 virtual)\n",
      "I0228 01:45:11.895928 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1204803 virtual)\n",
      "I0228 01:45:11.908598 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1209331 virtual)\n",
      "I0228 01:45:11.913690 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1213542 virtual)\n",
      "I0228 01:45:11.919201 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1218225 virtual)\n",
      "I0228 01:45:11.930847 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1221942 virtual)\n",
      "I0228 01:45:11.938414 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1225681 virtual)\n",
      "I0228 01:45:11.943413 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1228488 virtual)\n",
      "I0228 01:45:11.955270 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1232891 virtual)\n",
      "I0228 01:45:11.959361 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1237080 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:11.963726 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1241796 virtual)\n",
      "I0228 01:45:11.974420 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1246992 virtual)\n",
      "I0228 01:45:11.989691 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1251365 virtual)\n",
      "I0228 01:45:11.993804 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1255424 virtual)\n",
      "I0228 01:45:11.998368 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1259531 virtual)\n",
      "I0228 01:45:12.002985 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1263662 virtual)\n",
      "I0228 01:45:12.006387 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1267436 virtual)\n",
      "I0228 01:45:12.013894 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1271449 virtual)\n",
      "I0228 01:45:12.030655 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1275531 virtual)\n",
      "I0228 01:45:12.035193 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1280150 virtual)\n",
      "I0228 01:45:12.043885 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1283983 virtual)\n",
      "I0228 01:45:12.053244 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1288758 virtual)\n",
      "I0228 01:45:12.056921 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1292927 virtual)\n",
      "I0228 01:45:12.068014 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1296630 virtual)\n",
      "I0228 01:45:12.070771 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1301231 virtual)\n",
      "I0228 01:45:12.082943 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1306194 virtual)\n",
      "I0228 01:45:12.091278 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1309652 virtual)\n",
      "I0228 01:45:12.095366 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1313892 virtual)\n",
      "I0228 01:45:12.104276 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1318064 virtual)\n",
      "I0228 01:45:12.113238 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1321582 virtual)\n",
      "I0228 01:45:12.119491 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1325491 virtual)\n",
      "I0228 01:45:12.132139 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1329466 virtual)\n",
      "I0228 01:45:12.136574 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1333752 virtual)\n",
      "I0228 01:45:12.140706 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1338537 virtual)\n",
      "I0228 01:45:12.152509 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1342641 virtual)\n",
      "I0228 01:45:12.160342 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1347248 virtual)\n",
      "I0228 01:45:12.164345 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1352570 virtual)\n",
      "I0228 01:45:12.170678 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1354399 virtual)\n",
      "I0228 01:45:12.177961 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1359721 virtual)\n",
      "I0228 01:45:12.184596 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1364987 virtual)\n",
      "I0228 01:45:12.195873 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1369845 virtual)\n",
      "I0228 01:45:12.200404 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1374263 virtual)\n",
      "I0228 01:45:12.213375 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1378463 virtual)\n",
      "I0228 01:45:12.218990 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1383574 virtual)\n",
      "I0228 01:45:12.222300 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1387419 virtual)\n",
      "I0228 01:45:12.232836 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1392661 virtual)\n",
      "I0228 01:45:12.236592 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1397921 virtual)\n",
      "I0228 01:45:12.254082 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1401409 virtual)\n",
      "I0228 01:45:12.270247 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1406413 virtual)\n",
      "I0228 01:45:12.276270 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1411399 virtual)\n",
      "I0228 01:45:12.281633 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1415442 virtual)\n",
      "I0228 01:45:12.284478 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1420363 virtual)\n",
      "I0228 01:45:12.295358 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1424948 virtual)\n",
      "I0228 01:45:12.306462 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1429447 virtual)\n",
      "I0228 01:45:12.320048 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1434665 virtual)\n",
      "I0228 01:45:12.325352 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1439861 virtual)\n",
      "I0228 01:45:12.329535 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1445882 virtual)\n",
      "I0228 01:45:12.333127 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1450500 virtual)\n",
      "I0228 01:45:12.354884 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1456485 virtual)\n",
      "I0228 01:45:12.361796 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1461703 virtual)\n",
      "I0228 01:45:12.367502 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1469361 virtual)\n",
      "I0228 01:45:12.371782 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1476221 virtual)\n",
      "I0228 01:45:12.374492 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1481441 virtual)\n",
      "I0228 01:45:12.395102 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1486437 virtual)\n",
      "I0228 01:45:12.411231 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1491757 virtual)\n",
      "I0228 01:45:12.415787 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1497248 virtual)\n",
      "I0228 01:45:12.420151 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1502128 virtual)\n",
      "I0228 01:45:12.427609 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1507271 virtual)\n",
      "I0228 01:45:12.448163 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1512481 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:12.457262 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1522709 virtual)\n",
      "I0228 01:45:12.473212 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1528136 virtual)\n",
      "I0228 01:45:12.477903 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1533763 virtual)\n",
      "I0228 01:45:12.482634 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1539103 virtual)\n",
      "I0228 01:45:12.498359 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1543688 virtual)\n",
      "I0228 01:45:12.510335 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1548446 virtual)\n",
      "I0228 01:45:12.524411 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1553325 virtual)\n",
      "I0228 01:45:12.529951 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1558443 virtual)\n",
      "I0228 01:45:12.534265 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1563103 virtual)\n",
      "I0228 01:45:12.539108 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1568684 virtual)\n",
      "I0228 01:45:12.575812 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1574898 virtual)\n",
      "I0228 01:45:12.580235 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1580425 virtual)\n",
      "I0228 01:45:12.584193 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1585993 virtual)\n",
      "I0228 01:45:12.587600 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1591543 virtual)\n",
      "I0228 01:45:12.602251 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1597531 virtual)\n",
      "I0228 01:45:12.621955 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1603744 virtual)\n",
      "I0228 01:45:12.626066 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1608422 virtual)\n",
      "I0228 01:45:12.630586 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1613665 virtual)\n",
      "I0228 01:45:12.634911 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1618647 virtual)\n",
      "I0228 01:45:12.654882 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1624042 virtual)\n",
      "I0228 01:45:12.678907 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1629266 virtual)\n",
      "I0228 01:45:12.684599 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1634940 virtual)\n",
      "I0228 01:45:12.687579 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1640007 virtual)\n",
      "I0228 01:45:12.690564 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1645821 virtual)\n",
      "I0228 01:45:12.709474 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1650769 virtual)\n",
      "I0228 01:45:12.726077 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1657166 virtual)\n",
      "I0228 01:45:12.733437 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1665431 virtual)\n",
      "I0228 01:45:12.738715 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1673997 virtual)\n",
      "I0228 01:45:12.742099 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1682115 virtual)\n",
      "I0228 01:45:12.759499 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1690012 virtual)\n",
      "I0228 01:45:12.775229 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1695835 virtual)\n",
      "I0228 01:45:12.785230 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1700765 virtual)\n",
      "I0228 01:45:12.789418 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1707264 virtual)\n",
      "I0228 01:45:12.792889 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1713026 virtual)\n",
      "I0228 01:45:12.807157 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1719018 virtual)\n",
      "I0228 01:45:12.843069 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1725005 virtual)\n",
      "I0228 01:45:12.868604 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1730702 virtual)\n",
      "I0228 01:45:12.872666 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1735688 virtual)\n",
      "I0228 01:45:12.876800 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1741386 virtual)\n",
      "I0228 01:45:12.885917 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1746441 virtual)\n",
      "I0228 01:45:12.905198 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1751660 virtual)\n",
      "I0228 01:45:12.917330 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1756328 virtual)\n",
      "I0228 01:45:12.924569 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1761791 virtual)\n",
      "I0228 01:45:12.933321 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1767189 virtual)\n",
      "I0228 01:45:12.940635 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1772412 virtual)\n",
      "I0228 01:45:12.964648 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1777370 virtual)\n",
      "I0228 01:45:12.971460 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1782091 virtual)\n",
      "I0228 01:45:12.976970 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1787359 virtual)\n",
      "I0228 01:45:12.992505 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1792307 virtual)\n",
      "I0228 01:45:12.996475 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1797401 virtual)\n",
      "I0228 01:45:13.018940 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1803060 virtual)\n",
      "I0228 01:45:13.023756 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1808000 virtual)\n",
      "I0228 01:45:13.029075 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1813200 virtual)\n",
      "I0228 01:45:13.046312 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1818223 virtual)\n",
      "I0228 01:45:13.050432 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1823169 virtual)\n",
      "I0228 01:45:13.067439 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1828467 virtual)\n",
      "I0228 01:45:13.072749 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1833584 virtual)\n",
      "I0228 01:45:13.085416 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1838692 virtual)\n",
      "I0228 01:45:13.095038 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1844545 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:13.098797 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1848850 virtual)\n",
      "I0228 01:45:13.116881 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1853907 virtual)\n",
      "I0228 01:45:13.124476 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1858683 virtual)\n",
      "I0228 01:45:13.138032 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1864020 virtual)\n",
      "I0228 01:45:13.146596 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1868658 virtual)\n",
      "I0228 01:45:13.151315 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1873905 virtual)\n",
      "I0228 01:45:13.175694 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1879052 virtual)\n",
      "I0228 01:45:13.180029 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1884478 virtual)\n",
      "I0228 01:45:13.191893 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1889829 virtual)\n",
      "I0228 01:45:13.194964 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1894821 virtual)\n",
      "I0228 01:45:13.206067 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1899737 virtual)\n",
      "I0228 01:45:13.222630 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1905048 virtual)\n",
      "I0228 01:45:13.227877 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1910043 virtual)\n",
      "I0228 01:45:13.239938 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1914718 virtual)\n",
      "I0228 01:45:13.248575 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1919440 virtual)\n",
      "I0228 01:45:13.260051 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1924671 virtual)\n",
      "I0228 01:45:13.274465 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1929771 virtual)\n",
      "I0228 01:45:13.281486 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1934598 virtual)\n",
      "I0228 01:45:13.294471 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1940006 virtual)\n",
      "I0228 01:45:13.297880 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1944798 virtual)\n",
      "I0228 01:45:13.307891 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1950073 virtual)\n",
      "I0228 01:45:13.327845 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1955439 virtual)\n",
      "I0228 01:45:13.333270 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1960419 virtual)\n",
      "I0228 01:45:13.340664 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1965274 virtual)\n",
      "I0228 01:45:13.345257 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1970389 virtual)\n",
      "I0228 01:45:13.362611 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1975732 virtual)\n",
      "I0228 01:45:13.378888 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1980975 virtual)\n",
      "I0228 01:45:13.394837 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1986170 virtual)\n",
      "I0228 01:45:13.398198 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1991509 virtual)\n",
      "I0228 01:45:13.401122 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1996870 virtual)\n",
      "I0228 01:45:13.418481 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (2002135 virtual)\n",
      "I0228 01:45:13.432122 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (2007372 virtual)\n",
      "I0228 01:45:13.438196 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (2012657 virtual)\n",
      "I0228 01:45:13.444228 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (2017848 virtual)\n",
      "I0228 01:45:13.449119 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (2022691 virtual)\n",
      "I0228 01:45:13.476354 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (2027659 virtual)\n",
      "I0228 01:45:13.485697 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2032563 virtual)\n",
      "I0228 01:45:13.492360 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2037255 virtual)\n",
      "I0228 01:45:13.503061 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2042070 virtual)\n",
      "I0228 01:45:13.506387 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2046741 virtual)\n",
      "I0228 01:45:13.531456 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2051521 virtual)\n",
      "I0228 01:45:13.539565 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2057030 virtual)\n",
      "I0228 01:45:13.546567 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2061983 virtual)\n",
      "I0228 01:45:13.553188 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2067375 virtual)\n",
      "I0228 01:45:13.556566 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2072649 virtual)\n",
      "I0228 01:45:13.583788 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2078012 virtual)\n",
      "I0228 01:45:13.590998 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2083154 virtual)\n",
      "I0228 01:45:13.598506 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2088531 virtual)\n",
      "I0228 01:45:13.601699 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2093868 virtual)\n",
      "I0228 01:45:13.604772 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2098455 virtual)\n",
      "I0228 01:45:13.633357 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2103551 virtual)\n",
      "I0228 01:45:13.647098 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2108295 virtual)\n",
      "I0228 01:45:13.651654 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2113073 virtual)\n",
      "I0228 01:45:13.655602 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2117927 virtual)\n",
      "I0228 01:45:13.660110 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2122987 virtual)\n",
      "I0228 01:45:13.687679 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2128277 virtual)\n",
      "I0228 01:45:13.697563 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2133626 virtual)\n",
      "I0228 01:45:13.706665 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2138774 virtual)\n",
      "I0228 01:45:13.710586 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2143971 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:13.714333 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2149232 virtual)\n",
      "I0228 01:45:13.739299 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2154179 virtual)\n",
      "I0228 01:45:13.746255 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2159382 virtual)\n",
      "I0228 01:45:13.757910 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2164307 virtual)\n",
      "I0228 01:45:13.762573 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2169221 virtual)\n",
      "I0228 01:45:13.767574 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2174455 virtual)\n",
      "I0228 01:45:13.794885 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2179123 virtual)\n",
      "I0228 01:45:13.801229 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2184437 virtual)\n",
      "I0228 01:45:13.810037 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2189842 virtual)\n",
      "I0228 01:45:13.813213 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2194915 virtual)\n",
      "I0228 01:45:13.816245 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2200443 virtual)\n",
      "I0228 01:45:13.845877 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2205569 virtual)\n",
      "I0228 01:45:13.855850 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2210957 virtual)\n",
      "I0228 01:45:13.860953 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2215701 virtual)\n",
      "I0228 01:45:13.865260 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2220607 virtual)\n",
      "I0228 01:45:13.869446 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2225961 virtual)\n",
      "I0228 01:45:13.892688 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2231214 virtual)\n",
      "I0228 01:45:13.912773 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2236121 virtual)\n",
      "I0228 01:45:13.918164 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2241263 virtual)\n",
      "I0228 01:45:13.921434 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2246426 virtual)\n",
      "I0228 01:45:13.925949 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2251259 virtual)\n",
      "I0228 01:45:13.946282 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2256293 virtual)\n",
      "I0228 01:45:13.964822 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2261223 virtual)\n",
      "I0228 01:45:13.969417 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2266170 virtual)\n",
      "I0228 01:45:13.973497 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2271371 virtual)\n",
      "I0228 01:45:13.979696 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2276257 virtual)\n",
      "I0228 01:45:13.999435 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2281260 virtual)\n",
      "I0228 01:45:14.013524 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2286765 virtual)\n",
      "I0228 01:45:14.019337 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2291734 virtual)\n",
      "I0228 01:45:14.023979 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2296667 virtual)\n",
      "I0228 01:45:14.031437 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2301894 virtual)\n",
      "I0228 01:45:14.049689 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2307202 virtual)\n",
      "I0228 01:45:14.062451 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2312413 virtual)\n",
      "I0228 01:45:14.070049 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2317598 virtual)\n",
      "I0228 01:45:14.075461 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2322610 virtual)\n",
      "I0228 01:45:14.084184 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2327863 virtual)\n",
      "I0228 01:45:14.101225 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2333386 virtual)\n",
      "I0228 01:45:14.119728 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2338432 virtual)\n",
      "I0228 01:45:14.124155 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2343293 virtual)\n",
      "I0228 01:45:14.128463 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2348095 virtual)\n",
      "I0228 01:45:14.136960 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2353050 virtual)\n",
      "I0228 01:45:14.153621 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2358668 virtual)\n",
      "I0228 01:45:14.171105 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2363386 virtual)\n",
      "I0228 01:45:14.180318 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2368366 virtual)\n",
      "I0228 01:45:14.184333 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2373887 virtual)\n",
      "I0228 01:45:14.187196 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2379026 virtual)\n",
      "I0228 01:45:14.211970 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2384128 virtual)\n",
      "I0228 01:45:14.228137 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2389026 virtual)\n",
      "I0228 01:45:14.233793 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2394100 virtual)\n",
      "I0228 01:45:14.236648 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2399067 virtual)\n",
      "I0228 01:45:14.248069 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2404122 virtual)\n",
      "I0228 01:45:14.268484 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2409421 virtual)\n",
      "I0228 01:45:14.272960 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2414879 virtual)\n",
      "I0228 01:45:14.277168 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2419941 virtual)\n",
      "I0228 01:45:14.289567 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2424807 virtual)\n",
      "I0228 01:45:14.293868 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2430035 virtual)\n",
      "I0228 01:45:14.317202 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2435370 virtual)\n",
      "I0228 01:45:14.323893 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2440157 virtual)\n",
      "I0228 01:45:14.328776 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2445014 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:14.341831 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2450253 virtual)\n",
      "I0228 01:45:14.356958 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2455022 virtual)\n",
      "I0228 01:45:14.373308 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2460539 virtual)\n",
      "I0228 01:45:14.378177 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2466109 virtual)\n",
      "I0228 01:45:14.381051 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2471042 virtual)\n",
      "I0228 01:45:14.401165 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2476536 virtual)\n",
      "I0228 01:45:14.404514 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2481185 virtual)\n",
      "I0228 01:45:14.426848 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2485843 virtual)\n",
      "I0228 01:45:14.430923 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2491075 virtual)\n",
      "I0228 01:45:14.435266 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2496606 virtual)\n",
      "I0228 01:45:14.446073 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2501816 virtual)\n",
      "I0228 01:45:14.449814 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2506944 virtual)\n",
      "I0228 01:45:14.479998 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2511731 virtual)\n",
      "I0228 01:45:14.484287 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2517064 virtual)\n",
      "I0228 01:45:14.489241 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2522115 virtual)\n",
      "I0228 01:45:14.494127 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2526988 virtual)\n",
      "I0228 01:45:14.505834 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2532076 virtual)\n",
      "I0228 01:45:14.527973 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2537114 virtual)\n",
      "I0228 01:45:14.532776 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2542142 virtual)\n",
      "I0228 01:45:14.542149 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2547388 virtual)\n",
      "I0228 01:45:14.548048 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2552206 virtual)\n",
      "I0228 01:45:14.553070 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2557711 virtual)\n",
      "I0228 01:45:14.575977 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2563072 virtual)\n",
      "I0228 01:45:14.588029 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2567882 virtual)\n",
      "I0228 01:45:14.592530 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2572944 virtual)\n",
      "I0228 01:45:14.597181 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2578502 virtual)\n",
      "I0228 01:45:14.605892 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2583889 virtual)\n",
      "I0228 01:45:14.624172 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2589126 virtual)\n",
      "I0228 01:45:14.639990 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2594627 virtual)\n",
      "I0228 01:45:14.646753 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2599795 virtual)\n",
      "I0228 01:45:14.650663 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2605354 virtual)\n",
      "I0228 01:45:14.661837 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2610365 virtual)\n",
      "I0228 01:45:14.682320 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2615308 virtual)\n",
      "I0228 01:45:14.687079 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2620404 virtual)\n",
      "I0228 01:45:14.701369 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2625624 virtual)\n",
      "I0228 01:45:14.706202 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2630213 virtual)\n",
      "I0228 01:45:14.716923 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2635449 virtual)\n",
      "I0228 01:45:14.735458 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2640224 virtual)\n",
      "I0228 01:45:14.747613 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2645344 virtual)\n",
      "I0228 01:45:14.758444 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2650650 virtual)\n",
      "I0228 01:45:14.763266 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2655937 virtual)\n",
      "I0228 01:45:14.768884 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2661380 virtual)\n",
      "I0228 01:45:14.786263 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2666639 virtual)\n",
      "I0228 01:45:14.800356 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2672024 virtual)\n",
      "I0228 01:45:14.811641 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2677211 virtual)\n",
      "I0228 01:45:14.816536 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2682670 virtual)\n",
      "I0228 01:45:14.822532 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2687610 virtual)\n",
      "I0228 01:45:14.833421 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2692675 virtual)\n",
      "I0228 01:45:14.852363 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2697872 virtual)\n",
      "I0228 01:45:14.863998 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2703094 virtual)\n",
      "I0228 01:45:14.868575 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2708654 virtual)\n",
      "I0228 01:45:14.878489 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2714158 virtual)\n",
      "I0228 01:45:14.889051 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2719809 virtual)\n",
      "I0228 01:45:14.905871 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2724917 virtual)\n",
      "I0228 01:45:14.917227 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2729842 virtual)\n",
      "I0228 01:45:14.921574 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2734933 virtual)\n",
      "I0228 01:45:14.930879 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2739995 virtual)\n",
      "I0228 01:45:14.943152 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2745291 virtual)\n",
      "I0228 01:45:14.959305 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2750809 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:45:14.968834 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2755841 virtual)\n",
      "I0228 01:45:14.978986 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2760699 virtual)\n",
      "I0228 01:45:14.986770 140370323834688 text_analysis.py:506] 551 batches submitted to accumulate stats from 35264 documents (2760789 virtual)\n",
      "I0228 01:45:15.040120 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:45:15.052173 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:45:15.055089 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:45:15.075936 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:45:15.081545 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:45:15.085334 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:45:15.044980 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:45:15.080511 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:45:15.087652 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:45:15.091500 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:45:15.476355 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:45:15.492069 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2761086 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores\n",
      "{'epoch': 199, 'cv': 0.6966576105907082, 'umass': -4.152842844580276, 'uci': -0.008847425980885237, 'npmi': 0.07780804249392087, 'rbo': 1.0, 'td': 1.0, 'train_loss': 641.8717040550291, 'topics': [['c0267454', 'ref', 'heparin', 'c1446219', 'salivary', 'c0605290', 'c0851891', 'c0521990', 'c0020933', 'c0052432', 'c0032821', 'c0751651', 'c0038174', 'c0536858', 'postpartum', 'excretion', 'c0444584', 'c0007789', 'c0036323', 'somatic', 'c0271737', 'c0524909', 'colitis', 'swell', 'c2699572', 'c1960870'], ['c0011900', 'c0015967', 'c0012634', 'c0032285', 'child', 'c0035236', 'common', 'c1457887', 'c0546788', 'c0003232', 'clinical', 'c0010200', 'c0019993', 'disclosure', 'c0809949', 'presentation', 'c0038410', 'c0221423', 'grant', 'hospitalize', 'manifestation', 'sars-cov-2', 'c0039082', 'c0027442', 'cause', 'c3714514'], ['c0543467', 'c0025080', 'c0031150', 'c0038930', 'postoperative', 'operative', 'c0728940', 'c0002940', 'procedure', 'c0005898', 'c0850292', 'undergo', 'c0229962', 'c0009566', 'conversion', 'recurrence', 'perform', 'c0582175', 'surgical', 'perioperative', 'c0547070', 'c0162522', 'c0014245', 'c0019080', 'c4039858', 'c0187996'], ['compare', 'c0199470', 'c0243095', 'c0032042', 'c0034108', 'significantly', 'difference', 'measurement', 'significant', 'c0235195', 'infant', 'c0369768', 'c0018810', 'c0918012', 'receive', 'decrease', 'c0021708', 'concentration', 'curve', 'confidence', 'regression', 'c0005823', 'c0005516', 'trial', 'association', 'c0376519'], ['policy', 'crisis', 'economic', 'disaster', 'threat', 'political', 'market', 'economy', 'emergency', 'inequality', 'food', 'public', 'sector', 'argue', 'draw', 'society', 'c1561598', 'trade', 'international', 'financial', 'national', 'face', 'supply', 'c0242456', 'innovation', 'c0018104'], ['c0042210', 'c1254351', 'c1167622', 'c0030956', 'c0029224', 'c0003316', 'c0003320', 'c1514562', 'c0003250', 'c0020971', 'affinity', 'c1706082', 'nanoparticles', 'potency', 'drug', 'active', 'promise', 'candidate', 'c0243077', 'elicit', 'potent', 'molecule', 'potential', 'c0003241', 'c0005479', 'c3687832'], ['activation', 'c1171362', 'c0007613', 'mechanism', 'c0024432', 'c0079189', 'c0007634', 'c0025929', 'c0162638', 'role', 'c0021747', 'c0017262', 'induction', 'c3539881', 'pathway', 'activate', 'induce', 'c0004391', 'c0023810', 'c1101610', 'c0014597', 'suppress', 'c1327622', 'c0035696', 'c0021368', 'c0037080'], ['propose', 'c3161035', 'c0002045', 'c0150098', 'machine', 'automate', 'c0025663', 'accuracy', 'c0679083', 'sensor', 'performance', 'prediction', 'c0037585', 'compute', 'outperform', 'image', 'input', 'algorithm', 'c1710191', 'representation', 'solve', 'computational', 'automatically', 'c0037589', 'equation', 'noise'], ['c0679646', 'search', 'c2603343', 'conduct', 'train', 'c0038951', 'c0025353', 'c0242356', 'report', 'c0086388', 'c0282122', 'c0027361', 'evidence', 'c1257890', 'include', 'c0242481', 'c0030971', 'c0003467', 'psychological', 'impact', 'c0184661', 'c1955832', 'parent', 'c0011570', 'recommendation', 'physical'], ['c1705920', 'c0042776', 'c0032098', 'c0684063', 'c0005595', 'genetic', 'c0017428', 'sample', 'c0003062', 'c1764827', 'c0012984', 'c0039005', 'genotype', 'c0007452', 'c0004793', 'c0017337', 'isolate', 'c0015733', 'diversity', 'c0442726', 'c1519068', 'c0086418', 'c0242781', 'c0017446', 'specie', 'c0162326']]}\n",
      "Epoch: [201/250]\tSamples: [7381926/9181500]\tTrain Loss: 641.8793595647497\tTime: 0:00:04.512990\n",
      "Epoch: [202/250]\tSamples: [7418652/9181500]\tTrain Loss: 641.7201050515984\tTime: 0:00:04.525963\n",
      "Epoch: [203/250]\tSamples: [7455378/9181500]\tTrain Loss: 641.8373488170846\tTime: 0:00:04.460411\n",
      "Epoch: [204/250]\tSamples: [7492104/9181500]\tTrain Loss: 641.9879377008114\tTime: 0:00:04.528623\n",
      "Epoch: [205/250]\tSamples: [7528830/9181500]\tTrain Loss: 641.8142628434216\tTime: 0:00:04.775014\n",
      "Epoch: [206/250]\tSamples: [7565556/9181500]\tTrain Loss: 641.7536263052742\tTime: 0:00:04.798634\n",
      "Epoch: [207/250]\tSamples: [7602282/9181500]\tTrain Loss: 641.7493193897035\tTime: 0:00:04.764930\n",
      "Epoch: [208/250]\tSamples: [7639008/9181500]\tTrain Loss: 641.8550274541537\tTime: 0:00:04.786101\n",
      "Epoch: [209/250]\tSamples: [7675734/9181500]\tTrain Loss: 641.7580062915238\tTime: 0:00:04.758133\n",
      "Epoch: [210/250]\tSamples: [7712460/9181500]\tTrain Loss: 641.8380033686966\tTime: 0:00:04.898149\n",
      "Epoch: [211/250]\tSamples: [7749186/9181500]\tTrain Loss: 641.7194131669321\tTime: 0:00:04.770242\n",
      "Epoch: [212/250]\tSamples: [7785912/9181500]\tTrain Loss: 641.9287070552878\tTime: 0:00:04.947784\n",
      "Epoch: [213/250]\tSamples: [7822638/9181500]\tTrain Loss: 641.8730279425679\tTime: 0:00:04.898527\n",
      "Epoch: [214/250]\tSamples: [7859364/9181500]\tTrain Loss: 641.7430660499374\tTime: 0:00:04.821194\n",
      "Epoch: [215/250]\tSamples: [7896090/9181500]\tTrain Loss: 641.8450902672834\tTime: 0:00:04.772030\n",
      "Epoch: [216/250]\tSamples: [7932816/9181500]\tTrain Loss: 641.9208084020858\tTime: 0:00:04.851708\n",
      "Epoch: [217/250]\tSamples: [7969542/9181500]\tTrain Loss: 641.6640795179164\tTime: 0:00:04.927830\n",
      "Epoch: [218/250]\tSamples: [8006268/9181500]\tTrain Loss: 641.7147334526289\tTime: 0:00:04.856097\n",
      "Epoch: [219/250]\tSamples: [8042994/9181500]\tTrain Loss: 641.9101356157763\tTime: 0:00:04.847086\n",
      "Epoch: [220/250]\tSamples: [8079720/9181500]\tTrain Loss: 641.6642391672452\tTime: 0:00:04.804302\n",
      "Epoch: [221/250]\tSamples: [8116446/9181500]\tTrain Loss: 641.7154337398914\tTime: 0:00:04.785152\n",
      "Epoch: [222/250]\tSamples: [8153172/9181500]\tTrain Loss: 641.8728776530932\tTime: 0:00:04.824443\n",
      "Epoch: [223/250]\tSamples: [8189898/9181500]\tTrain Loss: 641.7223886432637\tTime: 0:00:04.939553\n",
      "Epoch: [224/250]\tSamples: [8226624/9181500]\tTrain Loss: 641.9912174787617\tTime: 0:00:04.893160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:15.240773 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [225/250]\tSamples: [8263350/9181500]\tTrain Loss: 641.7252850926455\tTime: 0:00:04.877260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:16.054731 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:47:16.766715 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:47:17.601944 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:47:18.147047 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:47:18.153791 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:47:18.918567 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:47:19.607738 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:47:20.422883 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:47:20.952565 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:47:20.957717 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:47:21.730622 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:47:22.417525 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:47:23.230086 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:47:23.759518 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:47:23.766379 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:47:24.537942 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:47:25.223675 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:47:26.036128 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:47:26.582546 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:47:26.605204 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:47:27.136470 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (-33728 virtual)\n",
      "I0228 01:47:27.210929 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (-50275 virtual)\n",
      "I0228 01:47:27.529720 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (-204297 virtual)\n",
      "I0228 01:47:27.842049 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (-475400 virtual)\n",
      "I0228 01:47:27.849316 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (-474192 virtual)\n",
      "I0228 01:47:27.872476 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (-479802 virtual)\n",
      "I0228 01:47:27.874552 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (-479458 virtual)\n",
      "I0228 01:47:27.955749 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (-503421 virtual)\n",
      "I0228 01:47:27.958341 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (-501437 virtual)\n",
      "I0228 01:47:27.963222 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (-499527 virtual)\n",
      "I0228 01:47:27.967158 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (-497844 virtual)\n",
      "I0228 01:47:27.969083 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (-496838 virtual)\n",
      "I0228 01:47:28.605080 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:47:28.606256 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:47:28.607507 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:47:28.609936 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:47:28.609542 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:47:28.611200 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:47:28.609884 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:47:28.615266 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:47:28.617591 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:47:28.635512 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:47:29.045653 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:47:29.064856 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 229009 virtual documents\n",
      "I0228 01:47:30.980616 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 1000 documents\n",
      "I0228 01:47:30.995860 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 2000 documents\n",
      "I0228 01:47:31.014849 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 3000 documents\n",
      "I0228 01:47:31.031003 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 4000 documents\n",
      "I0228 01:47:31.045830 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 5000 documents\n",
      "I0228 01:47:31.059280 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 6000 documents\n",
      "I0228 01:47:31.072813 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 7000 documents\n",
      "I0228 01:47:31.086646 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 8000 documents\n",
      "I0228 01:47:31.101369 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 9000 documents\n",
      "I0228 01:47:31.116874 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 10000 documents\n",
      "I0228 01:47:31.134054 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 11000 documents\n",
      "I0228 01:47:31.147668 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 12000 documents\n",
      "I0228 01:47:31.166200 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 13000 documents\n",
      "I0228 01:47:31.178896 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 14000 documents\n",
      "I0228 01:47:31.195115 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 15000 documents\n",
      "I0228 01:47:31.208137 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 16000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:31.219897 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 17000 documents\n",
      "I0228 01:47:31.232875 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 18000 documents\n",
      "I0228 01:47:31.244804 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 19000 documents\n",
      "I0228 01:47:31.258309 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 20000 documents\n",
      "I0228 01:47:31.273106 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 21000 documents\n",
      "I0228 01:47:31.289353 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 22000 documents\n",
      "I0228 01:47:31.306146 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 23000 documents\n",
      "I0228 01:47:31.322221 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 24000 documents\n",
      "I0228 01:47:31.337296 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 25000 documents\n",
      "I0228 01:47:31.352037 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 26000 documents\n",
      "I0228 01:47:31.366767 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 27000 documents\n",
      "I0228 01:47:31.381250 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 28000 documents\n",
      "I0228 01:47:31.395505 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 29000 documents\n",
      "I0228 01:47:31.409864 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 30000 documents\n",
      "I0228 01:47:31.424403 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 31000 documents\n",
      "I0228 01:47:31.438770 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 32000 documents\n",
      "I0228 01:47:31.455169 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 33000 documents\n",
      "I0228 01:47:31.469759 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 34000 documents\n",
      "I0228 01:47:31.484906 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 35000 documents\n",
      "I0228 01:47:31.500377 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 36000 documents\n",
      "I0228 01:47:31.651353 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:47:32.027065 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (5080 virtual)\n",
      "I0228 01:47:32.031902 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10380 virtual)\n",
      "I0228 01:47:32.035250 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16394 virtual)\n",
      "I0228 01:47:32.041858 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21308 virtual)\n",
      "I0228 01:47:32.045417 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (26974 virtual)\n",
      "I0228 01:47:32.050489 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32183 virtual)\n",
      "I0228 01:47:32.053601 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37840 virtual)\n",
      "I0228 01:47:32.056485 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43580 virtual)\n",
      "I0228 01:47:32.059243 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48821 virtual)\n",
      "I0228 01:47:32.062092 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54526 virtual)\n",
      "I0228 01:47:32.085469 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60417 virtual)\n",
      "I0228 01:47:32.104895 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65837 virtual)\n",
      "I0228 01:47:32.107383 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70991 virtual)\n",
      "I0228 01:47:32.110626 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (76243 virtual)\n",
      "I0228 01:47:32.122762 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81772 virtual)\n",
      "I0228 01:47:32.141157 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87905 virtual)\n",
      "I0228 01:47:32.163133 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93791 virtual)\n",
      "I0228 01:47:32.166181 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (99400 virtual)\n",
      "I0228 01:47:32.168295 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104482 virtual)\n",
      "I0228 01:47:32.198825 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109923 virtual)\n",
      "I0228 01:47:32.210351 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (115414 virtual)\n",
      "I0228 01:47:32.217215 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120652 virtual)\n",
      "I0228 01:47:32.219097 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (126256 virtual)\n",
      "I0228 01:47:32.225078 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (132194 virtual)\n",
      "I0228 01:47:32.269699 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137748 virtual)\n",
      "I0228 01:47:32.272090 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (143016 virtual)\n",
      "I0228 01:47:32.275205 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148601 virtual)\n",
      "I0228 01:47:32.278145 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154669 virtual)\n",
      "I0228 01:47:32.286059 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160593 virtual)\n",
      "I0228 01:47:32.329379 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (166155 virtual)\n",
      "I0228 01:47:32.332696 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (171458 virtual)\n",
      "I0228 01:47:32.337294 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176898 virtual)\n",
      "I0228 01:47:32.344624 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (182213 virtual)\n",
      "I0228 01:47:32.353163 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (187483 virtual)\n",
      "I0228 01:47:32.384452 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (193361 virtual)\n",
      "I0228 01:47:32.387439 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (198581 virtual)\n",
      "I0228 01:47:32.394330 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (204339 virtual)\n",
      "I0228 01:47:32.415885 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (210012 virtual)\n",
      "I0228 01:47:32.417841 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (215478 virtual)\n",
      "I0228 01:47:32.437439 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220999 virtual)\n",
      "I0228 01:47:32.444005 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (228672 virtual)\n",
      "I0228 01:47:32.450621 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (234104 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:32.479557 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (239511 virtual)\n",
      "I0228 01:47:32.481411 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (245244 virtual)\n",
      "I0228 01:47:32.496713 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (250748 virtual)\n",
      "I0228 01:47:32.503916 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (256051 virtual)\n",
      "I0228 01:47:32.511040 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (261354 virtual)\n",
      "I0228 01:47:32.534871 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (266729 virtual)\n",
      "I0228 01:47:32.545826 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (272354 virtual)\n",
      "I0228 01:47:32.551917 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (277728 virtual)\n",
      "I0228 01:47:32.566829 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (283192 virtual)\n",
      "I0228 01:47:32.584233 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288866 virtual)\n",
      "I0228 01:47:32.590868 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (294138 virtual)\n",
      "I0228 01:47:32.607268 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (299551 virtual)\n",
      "I0228 01:47:32.609131 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304887 virtual)\n",
      "I0228 01:47:32.619590 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (310506 virtual)\n",
      "I0228 01:47:32.647708 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (316227 virtual)\n",
      "I0228 01:47:32.652288 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (321961 virtual)\n",
      "I0228 01:47:32.667477 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (327166 virtual)\n",
      "I0228 01:47:32.670729 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (333725 virtual)\n",
      "I0228 01:47:32.677987 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (339338 virtual)\n",
      "I0228 01:47:32.704548 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (345206 virtual)\n",
      "I0228 01:47:32.715541 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (350810 virtual)\n",
      "I0228 01:47:32.724804 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (356864 virtual)\n",
      "I0228 01:47:32.734200 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (362567 virtual)\n",
      "I0228 01:47:32.737487 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (368185 virtual)\n",
      "I0228 01:47:32.761666 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (374187 virtual)\n",
      "I0228 01:47:32.780054 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (379802 virtual)\n",
      "I0228 01:47:32.784402 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (385693 virtual)\n",
      "I0228 01:47:32.792734 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (390337 virtual)\n",
      "I0228 01:47:32.810073 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (395514 virtual)\n",
      "I0228 01:47:32.820178 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398998 virtual)\n",
      "I0228 01:47:32.837476 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (402413 virtual)\n",
      "I0228 01:47:32.849553 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (407081 virtual)\n",
      "I0228 01:47:32.856759 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (411453 virtual)\n",
      "I0228 01:47:32.870133 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (415767 virtual)\n",
      "I0228 01:47:32.881863 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (420029 virtual)\n",
      "I0228 01:47:32.896779 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (424538 virtual)\n",
      "I0228 01:47:32.907228 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (428864 virtual)\n",
      "I0228 01:47:32.910042 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (433583 virtual)\n",
      "I0228 01:47:32.921692 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (438469 virtual)\n",
      "I0228 01:47:32.926461 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (443087 virtual)\n",
      "I0228 01:47:32.936227 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (447690 virtual)\n",
      "I0228 01:47:32.950956 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (451864 virtual)\n",
      "I0228 01:47:32.961594 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (455588 virtual)\n",
      "I0228 01:47:32.965481 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (459247 virtual)\n",
      "I0228 01:47:32.973380 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (463341 virtual)\n",
      "I0228 01:47:32.980918 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (467889 virtual)\n",
      "I0228 01:47:32.996700 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (472626 virtual)\n",
      "I0228 01:47:33.006468 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (477292 virtual)\n",
      "I0228 01:47:33.020048 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (481852 virtual)\n",
      "I0228 01:47:33.026749 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (486443 virtual)\n",
      "I0228 01:47:33.039987 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (491324 virtual)\n",
      "I0228 01:47:33.041731 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (496034 virtual)\n",
      "I0228 01:47:33.045692 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (500446 virtual)\n",
      "I0228 01:47:33.063737 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (504800 virtual)\n",
      "I0228 01:47:33.072838 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (510018 virtual)\n",
      "I0228 01:47:33.085125 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (515443 virtual)\n",
      "I0228 01:47:33.088811 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (519407 virtual)\n",
      "I0228 01:47:33.093481 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (523882 virtual)\n",
      "I0228 01:47:33.120157 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (528289 virtual)\n",
      "I0228 01:47:33.126279 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (532406 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:33.131297 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (536423 virtual)\n",
      "I0228 01:47:33.135041 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (540892 virtual)\n",
      "I0228 01:47:33.138719 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (545202 virtual)\n",
      "I0228 01:47:33.171427 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (549643 virtual)\n",
      "I0228 01:47:33.175721 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (553915 virtual)\n",
      "I0228 01:47:33.187250 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (558506 virtual)\n",
      "I0228 01:47:33.190834 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (562456 virtual)\n",
      "I0228 01:47:33.196217 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (567053 virtual)\n",
      "I0228 01:47:33.217896 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (571371 virtual)\n",
      "I0228 01:47:33.225879 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (575068 virtual)\n",
      "I0228 01:47:33.229470 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (578408 virtual)\n",
      "I0228 01:47:33.239116 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (581793 virtual)\n",
      "I0228 01:47:33.262088 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (587020 virtual)\n",
      "I0228 01:47:33.264903 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (592646 virtual)\n",
      "I0228 01:47:33.277637 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (598170 virtual)\n",
      "I0228 01:47:33.279845 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (602981 virtual)\n",
      "I0228 01:47:33.281841 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (608702 virtual)\n",
      "I0228 01:47:33.309724 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (614031 virtual)\n",
      "I0228 01:47:33.317769 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (618984 virtual)\n",
      "I0228 01:47:33.319544 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (623440 virtual)\n",
      "I0228 01:47:33.321296 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (628004 virtual)\n",
      "I0228 01:47:33.328076 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (632161 virtual)\n",
      "I0228 01:47:33.364629 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (637434 virtual)\n",
      "I0228 01:47:33.371562 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (642912 virtual)\n",
      "I0228 01:47:33.375431 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (648062 virtual)\n",
      "I0228 01:47:33.377399 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (653696 virtual)\n",
      "I0228 01:47:33.395784 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (658398 virtual)\n",
      "I0228 01:47:33.420137 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (663155 virtual)\n",
      "I0228 01:47:33.423382 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (668550 virtual)\n",
      "I0228 01:47:33.425329 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (673878 virtual)\n",
      "I0228 01:47:33.427214 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (678934 virtual)\n",
      "I0228 01:47:33.449521 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (683792 virtual)\n",
      "I0228 01:47:33.472989 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (689082 virtual)\n",
      "I0228 01:47:33.476112 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (694000 virtual)\n",
      "I0228 01:47:33.477998 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (699205 virtual)\n",
      "I0228 01:47:33.480875 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (704516 virtual)\n",
      "I0228 01:47:33.501538 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (709756 virtual)\n",
      "I0228 01:47:33.519020 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (714982 virtual)\n",
      "I0228 01:47:33.530575 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (720147 virtual)\n",
      "I0228 01:47:33.532862 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (725461 virtual)\n",
      "I0228 01:47:33.534824 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (730952 virtual)\n",
      "I0228 01:47:33.558995 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (736580 virtual)\n",
      "I0228 01:47:33.569295 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (741675 virtual)\n",
      "I0228 01:47:33.582199 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (746705 virtual)\n",
      "I0228 01:47:33.584582 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (752207 virtual)\n",
      "I0228 01:47:33.586730 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (757318 virtual)\n",
      "I0228 01:47:33.617532 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (762619 virtual)\n",
      "I0228 01:47:33.622918 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (768291 virtual)\n",
      "I0228 01:47:33.630751 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (774186 virtual)\n",
      "I0228 01:47:33.636781 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (780054 virtual)\n",
      "I0228 01:47:33.639748 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (785914 virtual)\n",
      "I0228 01:47:33.676562 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (791831 virtual)\n",
      "I0228 01:47:33.678495 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (797495 virtual)\n",
      "I0228 01:47:33.682330 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (803193 virtual)\n",
      "I0228 01:47:33.688264 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (808597 virtual)\n",
      "I0228 01:47:33.691550 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (814501 virtual)\n",
      "I0228 01:47:33.731215 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (820375 virtual)\n",
      "I0228 01:47:33.735774 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (826117 virtual)\n",
      "I0228 01:47:33.750641 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (831568 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:33.753176 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (836994 virtual)\n",
      "I0228 01:47:33.758393 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (842775 virtual)\n",
      "I0228 01:47:33.788375 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (848795 virtual)\n",
      "I0228 01:47:33.790831 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (854864 virtual)\n",
      "I0228 01:47:33.806360 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (860752 virtual)\n",
      "I0228 01:47:33.808294 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (866511 virtual)\n",
      "I0228 01:47:33.828233 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (872516 virtual)\n",
      "I0228 01:47:33.847992 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (878516 virtual)\n",
      "I0228 01:47:33.850129 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (884467 virtual)\n",
      "I0228 01:47:33.863710 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (890468 virtual)\n",
      "I0228 01:47:33.865625 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (895982 virtual)\n",
      "I0228 01:47:33.893386 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (901999 virtual)\n",
      "I0228 01:47:33.907048 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (909303 virtual)\n",
      "I0228 01:47:33.908766 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (913474 virtual)\n",
      "I0228 01:47:33.919097 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (917572 virtual)\n",
      "I0228 01:47:33.922556 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (920273 virtual)\n",
      "I0228 01:47:33.959631 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (926499 virtual)\n",
      "I0228 01:47:33.962123 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (930848 virtual)\n",
      "I0228 01:47:33.967069 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (934767 virtual)\n",
      "I0228 01:47:33.976648 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (938626 virtual)\n",
      "I0228 01:47:33.979506 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (942574 virtual)\n",
      "I0228 01:47:34.008677 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (946373 virtual)\n",
      "I0228 01:47:34.010245 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (949876 virtual)\n",
      "I0228 01:47:34.017683 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (953716 virtual)\n",
      "I0228 01:47:34.029526 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (957314 virtual)\n",
      "I0228 01:47:34.036951 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (961288 virtual)\n",
      "I0228 01:47:34.051333 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (965392 virtual)\n",
      "I0228 01:47:34.059563 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (969145 virtual)\n",
      "I0228 01:47:34.072725 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (972782 virtual)\n",
      "I0228 01:47:34.074811 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (976618 virtual)\n",
      "I0228 01:47:34.076438 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (980309 virtual)\n",
      "I0228 01:47:34.086754 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (984195 virtual)\n",
      "I0228 01:47:34.093804 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (987991 virtual)\n",
      "I0228 01:47:34.110375 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (991719 virtual)\n",
      "I0228 01:47:34.114522 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (995718 virtual)\n",
      "I0228 01:47:34.119009 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (999781 virtual)\n",
      "I0228 01:47:34.125995 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (1003236 virtual)\n",
      "I0228 01:47:34.129909 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1007025 virtual)\n",
      "I0228 01:47:34.146093 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1010957 virtual)\n",
      "I0228 01:47:34.152756 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1015458 virtual)\n",
      "I0228 01:47:34.162019 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1020551 virtual)\n",
      "I0228 01:47:34.167252 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1024455 virtual)\n",
      "I0228 01:47:34.170386 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1028661 virtual)\n",
      "I0228 01:47:34.182004 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1032299 virtual)\n",
      "I0228 01:47:34.199023 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1037356 virtual)\n",
      "I0228 01:47:34.200579 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1041103 virtual)\n",
      "I0228 01:47:34.202276 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1045251 virtual)\n",
      "I0228 01:47:34.214561 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1049424 virtual)\n",
      "I0228 01:47:34.218253 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1053629 virtual)\n",
      "I0228 01:47:34.238854 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1057577 virtual)\n",
      "I0228 01:47:34.240503 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1061388 virtual)\n",
      "I0228 01:47:34.244190 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1065439 virtual)\n",
      "I0228 01:47:34.253921 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1070002 virtual)\n",
      "I0228 01:47:34.266685 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1074476 virtual)\n",
      "I0228 01:47:34.276922 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1078766 virtual)\n",
      "I0228 01:47:34.287910 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1083486 virtual)\n",
      "I0228 01:47:34.289619 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1087525 virtual)\n",
      "I0228 01:47:34.291217 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1091201 virtual)\n",
      "I0228 01:47:34.313908 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1095254 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:34.315832 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1099402 virtual)\n",
      "I0228 01:47:34.327354 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1103296 virtual)\n",
      "I0228 01:47:34.331947 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1107732 virtual)\n",
      "I0228 01:47:34.333596 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1111756 virtual)\n",
      "I0228 01:47:34.358425 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1116961 virtual)\n",
      "I0228 01:47:34.366965 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1120696 virtual)\n",
      "I0228 01:47:34.368778 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1125714 virtual)\n",
      "I0228 01:47:34.370450 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1130098 virtual)\n",
      "I0228 01:47:34.372272 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1134989 virtual)\n",
      "I0228 01:47:34.397476 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1140153 virtual)\n",
      "I0228 01:47:34.402569 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1144193 virtual)\n",
      "I0228 01:47:34.410811 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1148022 virtual)\n",
      "I0228 01:47:34.412527 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1152350 virtual)\n",
      "I0228 01:47:34.419102 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1156668 virtual)\n",
      "I0228 01:47:34.437045 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1160576 virtual)\n",
      "I0228 01:47:34.451120 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1165484 virtual)\n",
      "I0228 01:47:34.455580 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1169845 virtual)\n",
      "I0228 01:47:34.461678 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1173845 virtual)\n",
      "I0228 01:47:34.479709 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1178815 virtual)\n",
      "I0228 01:47:34.490468 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1183638 virtual)\n",
      "I0228 01:47:34.492193 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1187891 virtual)\n",
      "I0228 01:47:34.493914 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1192146 virtual)\n",
      "I0228 01:47:34.505353 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1196891 virtual)\n",
      "I0228 01:47:34.529195 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1200848 virtual)\n",
      "I0228 01:47:34.534718 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1204962 virtual)\n",
      "I0228 01:47:34.539971 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1209270 virtual)\n",
      "I0228 01:47:34.542754 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1213729 virtual)\n",
      "I0228 01:47:34.547042 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1218365 virtual)\n",
      "I0228 01:47:34.578459 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1221962 virtual)\n",
      "I0228 01:47:34.580026 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1225322 virtual)\n",
      "I0228 01:47:34.582343 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1228577 virtual)\n",
      "I0228 01:47:34.587584 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1233039 virtual)\n",
      "I0228 01:47:34.589310 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1237175 virtual)\n",
      "I0228 01:47:34.613452 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1242429 virtual)\n",
      "I0228 01:47:34.616755 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1247165 virtual)\n",
      "I0228 01:47:34.624958 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1251266 virtual)\n",
      "I0228 01:47:34.633474 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1255565 virtual)\n",
      "I0228 01:47:34.635634 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1259627 virtual)\n",
      "I0228 01:47:34.647223 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1263725 virtual)\n",
      "I0228 01:47:34.648900 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1267634 virtual)\n",
      "I0228 01:47:34.655495 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1271577 virtual)\n",
      "I0228 01:47:34.676500 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1276150 virtual)\n",
      "I0228 01:47:34.678229 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1280304 virtual)\n",
      "I0228 01:47:34.694391 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1284497 virtual)\n",
      "I0228 01:47:34.697585 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1289138 virtual)\n",
      "I0228 01:47:34.699203 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1292815 virtual)\n",
      "I0228 01:47:34.716880 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1296649 virtual)\n",
      "I0228 01:47:34.726783 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1301633 virtual)\n",
      "I0228 01:47:34.734735 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1306373 virtual)\n",
      "I0228 01:47:34.736282 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1309672 virtual)\n",
      "I0228 01:47:34.738069 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1314419 virtual)\n",
      "I0228 01:47:34.760169 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1318325 virtual)\n",
      "I0228 01:47:34.769651 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1321605 virtual)\n",
      "I0228 01:47:34.775121 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1325647 virtual)\n",
      "I0228 01:47:34.780134 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1329888 virtual)\n",
      "I0228 01:47:34.793703 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1333908 virtual)\n",
      "I0228 01:47:34.795496 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1338711 virtual)\n",
      "I0228 01:47:34.812766 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1342919 virtual)\n",
      "I0228 01:47:34.814581 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1347809 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:34.826776 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1351483 virtual)\n",
      "I0228 01:47:34.830875 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1355056 virtual)\n",
      "I0228 01:47:34.836380 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1360353 virtual)\n",
      "I0228 01:47:34.847267 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1365597 virtual)\n",
      "I0228 01:47:34.849101 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1370330 virtual)\n",
      "I0228 01:47:34.873153 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1374447 virtual)\n",
      "I0228 01:47:34.878852 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1379353 virtual)\n",
      "I0228 01:47:34.880519 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1383622 virtual)\n",
      "I0228 01:47:34.882477 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1388342 virtual)\n",
      "I0228 01:47:34.895963 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1393290 virtual)\n",
      "I0228 01:47:34.911215 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1397958 virtual)\n",
      "I0228 01:47:34.922672 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1402146 virtual)\n",
      "I0228 01:47:34.932148 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1406894 virtual)\n",
      "I0228 01:47:34.933886 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1411521 virtual)\n",
      "I0228 01:47:34.940422 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1416024 virtual)\n",
      "I0228 01:47:34.947958 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1420577 virtual)\n",
      "I0228 01:47:34.974027 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1425187 virtual)\n",
      "I0228 01:47:34.978168 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1429954 virtual)\n",
      "I0228 01:47:34.982693 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1435816 virtual)\n",
      "I0228 01:47:34.987779 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1440801 virtual)\n",
      "I0228 01:47:34.993854 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1446081 virtual)\n",
      "I0228 01:47:35.015717 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1451482 virtual)\n",
      "I0228 01:47:35.024239 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1456833 virtual)\n",
      "I0228 01:47:35.029011 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1463800 virtual)\n",
      "I0228 01:47:35.037288 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1471408 virtual)\n",
      "I0228 01:47:35.039235 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1476905 virtual)\n",
      "I0228 01:47:35.058746 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1482126 virtual)\n",
      "I0228 01:47:35.068211 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1487429 virtual)\n",
      "I0228 01:47:35.081412 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1492680 virtual)\n",
      "I0228 01:47:35.091280 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1498131 virtual)\n",
      "I0228 01:47:35.098812 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1503082 virtual)\n",
      "I0228 01:47:35.108563 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1508258 virtual)\n",
      "I0228 01:47:35.116723 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1516998 virtual)\n",
      "I0228 01:47:35.142248 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1523742 virtual)\n",
      "I0228 01:47:35.153948 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1529001 virtual)\n",
      "I0228 01:47:35.156826 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1534895 virtual)\n",
      "I0228 01:47:35.159013 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1539907 virtual)\n",
      "I0228 01:47:35.172753 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1544113 virtual)\n",
      "I0228 01:47:35.193478 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1549198 virtual)\n",
      "I0228 01:47:35.204608 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1554232 virtual)\n",
      "I0228 01:47:35.210328 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1558963 virtual)\n",
      "I0228 01:47:35.213811 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1564150 virtual)\n",
      "I0228 01:47:35.254899 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1570005 virtual)\n",
      "I0228 01:47:35.256784 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1575905 virtual)\n",
      "I0228 01:47:35.264406 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1581844 virtual)\n",
      "I0228 01:47:35.269130 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1586973 virtual)\n",
      "I0228 01:47:35.272774 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1592882 virtual)\n",
      "I0228 01:47:35.296861 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1599058 virtual)\n",
      "I0228 01:47:35.313620 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1604825 virtual)\n",
      "I0228 01:47:35.315680 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1609290 virtual)\n",
      "I0228 01:47:35.321779 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1614703 virtual)\n",
      "I0228 01:47:35.329412 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1619884 virtual)\n",
      "I0228 01:47:35.351994 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1625159 virtual)\n",
      "I0228 01:47:35.372985 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1630696 virtual)\n",
      "I0228 01:47:35.374906 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1635893 virtual)\n",
      "I0228 01:47:35.377702 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1641579 virtual)\n",
      "I0228 01:47:35.393671 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1646684 virtual)\n",
      "I0228 01:47:35.409253 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1651943 virtual)\n",
      "I0228 01:47:35.417967 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1659779 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:35.426394 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1668163 virtual)\n",
      "I0228 01:47:35.436617 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1676473 virtual)\n",
      "I0228 01:47:35.458475 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1684556 virtual)\n",
      "I0228 01:47:35.460436 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1691962 virtual)\n",
      "I0228 01:47:35.471176 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1696881 virtual)\n",
      "I0228 01:47:35.477005 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1702938 virtual)\n",
      "I0228 01:47:35.495295 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1708575 virtual)\n",
      "I0228 01:47:35.510897 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1714359 virtual)\n",
      "I0228 01:47:35.518235 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1720621 virtual)\n",
      "I0228 01:47:35.553830 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1726303 virtual)\n",
      "I0228 01:47:35.562600 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1732084 virtual)\n",
      "I0228 01:47:35.582967 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1737150 virtual)\n",
      "I0228 01:47:35.590479 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1742424 virtual)\n",
      "I0228 01:47:35.603140 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1747789 virtual)\n",
      "I0228 01:47:35.604722 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1752856 virtual)\n",
      "I0228 01:47:35.623505 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1757712 virtual)\n",
      "I0228 01:47:35.642150 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1763137 virtual)\n",
      "I0228 01:47:35.644061 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1768579 virtual)\n",
      "I0228 01:47:35.659122 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1773403 virtual)\n",
      "I0228 01:47:35.674735 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1778350 virtual)\n",
      "I0228 01:47:35.681440 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1783296 virtual)\n",
      "I0228 01:47:35.690144 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1788428 virtual)\n",
      "I0228 01:47:35.705962 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1793524 virtual)\n",
      "I0228 01:47:35.718455 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1798925 virtual)\n",
      "I0228 01:47:35.732403 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1803999 virtual)\n",
      "I0228 01:47:35.736123 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1809215 virtual)\n",
      "I0228 01:47:35.743335 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1814443 virtual)\n",
      "I0228 01:47:35.767485 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1819485 virtual)\n",
      "I0228 01:47:35.769862 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1824395 virtual)\n",
      "I0228 01:47:35.783942 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1829705 virtual)\n",
      "I0228 01:47:35.794530 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1834771 virtual)\n",
      "I0228 01:47:35.797895 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1840473 virtual)\n",
      "I0228 01:47:35.817128 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1845188 virtual)\n",
      "I0228 01:47:35.829647 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1850199 virtual)\n",
      "I0228 01:47:35.834517 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1854874 virtual)\n",
      "I0228 01:47:35.847033 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1860002 virtual)\n",
      "I0228 01:47:35.861772 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1865110 virtual)\n",
      "I0228 01:47:35.871603 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1870132 virtual)\n",
      "I0228 01:47:35.884361 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1875134 virtual)\n",
      "I0228 01:47:35.888188 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1880409 virtual)\n",
      "I0228 01:47:35.900173 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1885905 virtual)\n",
      "I0228 01:47:35.921355 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1891129 virtual)\n",
      "I0228 01:47:35.930035 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1895929 virtual)\n",
      "I0228 01:47:35.934275 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1901485 virtual)\n",
      "I0228 01:47:35.942296 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1906506 virtual)\n",
      "I0228 01:47:35.950606 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1911250 virtual)\n",
      "I0228 01:47:35.975684 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1915777 virtual)\n",
      "I0228 01:47:35.986025 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1920918 virtual)\n",
      "I0228 01:47:35.992986 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1926236 virtual)\n",
      "I0228 01:47:36.001753 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1931116 virtual)\n",
      "I0228 01:47:36.006661 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1936358 virtual)\n",
      "I0228 01:47:36.029322 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1941295 virtual)\n",
      "I0228 01:47:36.032066 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1946368 virtual)\n",
      "I0228 01:47:36.054255 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1951734 virtual)\n",
      "I0228 01:47:36.058687 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1956720 virtual)\n",
      "I0228 01:47:36.061412 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1961967 virtual)\n",
      "I0228 01:47:36.086408 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1966824 virtual)\n",
      "I0228 01:47:36.096205 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1972274 virtual)\n",
      "I0228 01:47:36.107332 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1977533 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:36.114004 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1982518 virtual)\n",
      "I0228 01:47:36.126055 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1987864 virtual)\n",
      "I0228 01:47:36.135692 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1993351 virtual)\n",
      "I0228 01:47:36.150174 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1998669 virtual)\n",
      "I0228 01:47:36.162126 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (2003688 virtual)\n",
      "I0228 01:47:36.173069 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (2009101 virtual)\n",
      "I0228 01:47:36.184794 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (2014379 virtual)\n",
      "I0228 01:47:36.191676 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (2019161 virtual)\n",
      "I0228 01:47:36.217622 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (2024102 virtual)\n",
      "I0228 01:47:36.219499 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (2029250 virtual)\n",
      "I0228 01:47:36.230697 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2033777 virtual)\n",
      "I0228 01:47:36.256631 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2038464 virtual)\n",
      "I0228 01:47:36.272210 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2043125 virtual)\n",
      "I0228 01:47:36.274920 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2048080 virtual)\n",
      "I0228 01:47:36.291642 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2053448 virtual)\n",
      "I0228 01:47:36.293533 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2058330 virtual)\n",
      "I0228 01:47:36.318864 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2063510 virtual)\n",
      "I0228 01:47:36.325396 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2068827 virtual)\n",
      "I0228 01:47:36.336698 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2074185 virtual)\n",
      "I0228 01:47:36.346747 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2079506 virtual)\n",
      "I0228 01:47:36.349223 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2084778 virtual)\n",
      "I0228 01:47:36.373268 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2090305 virtual)\n",
      "I0228 01:47:36.382563 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2094893 virtual)\n",
      "I0228 01:47:36.387991 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2099862 virtual)\n",
      "I0228 01:47:36.403410 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2104663 virtual)\n",
      "I0228 01:47:36.405261 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2109627 virtual)\n",
      "I0228 01:47:36.423928 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2114142 virtual)\n",
      "I0228 01:47:36.440961 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2119531 virtual)\n",
      "I0228 01:47:36.447578 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2124933 virtual)\n",
      "I0228 01:47:36.458594 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2130462 virtual)\n",
      "I0228 01:47:36.465592 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2135429 virtual)\n",
      "I0228 01:47:36.483525 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2140614 virtual)\n",
      "I0228 01:47:36.486492 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2145789 virtual)\n",
      "I0228 01:47:36.508140 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2150685 virtual)\n",
      "I0228 01:47:36.511070 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2155816 virtual)\n",
      "I0228 01:47:36.525564 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2161078 virtual)\n",
      "I0228 01:47:36.531003 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2165924 virtual)\n",
      "I0228 01:47:36.543912 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2171109 virtual)\n",
      "I0228 01:47:36.564470 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2175907 virtual)\n",
      "I0228 01:47:36.577140 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2181349 virtual)\n",
      "I0228 01:47:36.584031 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2186616 virtual)\n",
      "I0228 01:47:36.585872 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2191630 virtual)\n",
      "I0228 01:47:36.595947 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2197121 virtual)\n",
      "I0228 01:47:36.614284 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2202064 virtual)\n",
      "I0228 01:47:36.639709 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2207610 virtual)\n",
      "I0228 01:47:36.641531 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2212486 virtual)\n",
      "I0228 01:47:36.644279 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2217211 virtual)\n",
      "I0228 01:47:36.647480 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2222585 virtual)\n",
      "I0228 01:47:36.661751 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2227766 virtual)\n",
      "I0228 01:47:36.697228 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2232707 virtual)\n",
      "I0228 01:47:36.701790 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2237965 virtual)\n",
      "I0228 01:47:36.705427 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2243048 virtual)\n",
      "I0228 01:47:36.707222 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2247923 virtual)\n",
      "I0228 01:47:36.713928 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2252928 virtual)\n",
      "I0228 01:47:36.753958 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2257973 virtual)\n",
      "I0228 01:47:36.756386 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2262908 virtual)\n",
      "I0228 01:47:36.764383 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2268045 virtual)\n",
      "I0228 01:47:36.766391 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2273176 virtual)\n",
      "I0228 01:47:36.768148 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2277892 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:36.804536 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2283282 virtual)\n",
      "I0228 01:47:36.808811 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2288123 virtual)\n",
      "I0228 01:47:36.816355 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2293281 virtual)\n",
      "I0228 01:47:36.825527 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2298568 virtual)\n",
      "I0228 01:47:36.827753 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2303887 virtual)\n",
      "I0228 01:47:36.855251 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2308990 virtual)\n",
      "I0228 01:47:36.859800 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2314167 virtual)\n",
      "I0228 01:47:36.870765 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2319268 virtual)\n",
      "I0228 01:47:36.887237 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2324457 virtual)\n",
      "I0228 01:47:36.889181 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2330006 virtual)\n",
      "I0228 01:47:36.907293 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2335135 virtual)\n",
      "I0228 01:47:36.912153 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2339955 virtual)\n",
      "I0228 01:47:36.923619 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2344936 virtual)\n",
      "I0228 01:47:36.945992 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2349787 virtual)\n",
      "I0228 01:47:36.950690 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2355417 virtual)\n",
      "I0228 01:47:36.958284 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2360257 virtual)\n",
      "I0228 01:47:36.966079 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2365113 virtual)\n",
      "I0228 01:47:36.975510 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2370721 virtual)\n",
      "I0228 01:47:37.002478 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2375841 virtual)\n",
      "I0228 01:47:37.010057 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2380851 virtual)\n",
      "I0228 01:47:37.013749 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2385852 virtual)\n",
      "I0228 01:47:37.021796 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2390887 virtual)\n",
      "I0228 01:47:37.025583 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2395900 virtual)\n",
      "I0228 01:47:37.059979 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2400780 virtual)\n",
      "I0228 01:47:37.066196 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2406152 virtual)\n",
      "I0228 01:47:37.068161 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2411659 virtual)\n",
      "I0228 01:47:37.080213 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2416796 virtual)\n",
      "I0228 01:47:37.085432 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2421573 virtual)\n",
      "I0228 01:47:37.114936 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2426740 virtual)\n",
      "I0228 01:47:37.120775 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2432059 virtual)\n",
      "I0228 01:47:37.122617 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2436940 virtual)\n",
      "I0228 01:47:37.137168 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2441832 virtual)\n",
      "I0228 01:47:37.144323 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2446906 virtual)\n",
      "I0228 01:47:37.166625 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2451756 virtual)\n",
      "I0228 01:47:37.177837 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2457232 virtual)\n",
      "I0228 01:47:37.186384 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2462824 virtual)\n",
      "I0228 01:47:37.190916 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2467829 virtual)\n",
      "I0228 01:47:37.204298 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2473277 virtual)\n",
      "I0228 01:47:37.229573 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2477914 virtual)\n",
      "I0228 01:47:37.232199 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2482661 virtual)\n",
      "I0228 01:47:37.238881 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2487832 virtual)\n",
      "I0228 01:47:37.245558 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2493281 virtual)\n",
      "I0228 01:47:37.265812 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2498545 virtual)\n",
      "I0228 01:47:37.279412 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2503643 virtual)\n",
      "I0228 01:47:37.286627 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2508483 virtual)\n",
      "I0228 01:47:37.296667 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2513792 virtual)\n",
      "I0228 01:47:37.305145 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2518917 virtual)\n",
      "I0228 01:47:37.324408 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2523832 virtual)\n",
      "I0228 01:47:37.333886 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2528888 virtual)\n",
      "I0228 01:47:37.346465 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2533962 virtual)\n",
      "I0228 01:47:37.348270 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2538990 virtual)\n",
      "I0228 01:47:37.371230 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2544236 virtual)\n",
      "I0228 01:47:37.377648 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2549054 virtual)\n",
      "I0228 01:47:37.394435 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2554582 virtual)\n",
      "I0228 01:47:37.396353 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2559981 virtual)\n",
      "I0228 01:47:37.402906 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2564758 virtual)\n",
      "I0228 01:47:37.426848 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2569757 virtual)\n",
      "I0228 01:47:37.430434 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2575315 virtual)\n",
      "I0228 01:47:37.444015 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2580688 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:37.455108 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2585971 virtual)\n",
      "I0228 01:47:37.461389 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2591405 virtual)\n",
      "I0228 01:47:37.483608 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2596616 virtual)\n",
      "I0228 01:47:37.485622 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2602118 virtual)\n",
      "I0228 01:47:37.500789 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2607172 virtual)\n",
      "I0228 01:47:37.514282 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2612115 virtual)\n",
      "I0228 01:47:37.516497 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2617211 virtual)\n",
      "I0228 01:47:37.537336 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2622431 virtual)\n",
      "I0228 01:47:37.549995 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2627020 virtual)\n",
      "I0228 01:47:37.554926 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2632178 virtual)\n",
      "I0228 01:47:37.568477 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2636945 virtual)\n",
      "I0228 01:47:37.579015 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2641995 virtual)\n",
      "I0228 01:47:37.607596 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2647287 virtual)\n",
      "I0228 01:47:37.610267 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2652543 virtual)\n",
      "I0228 01:47:37.615401 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2658073 virtual)\n",
      "I0228 01:47:37.619589 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2663260 virtual)\n",
      "I0228 01:47:37.638821 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2668735 virtual)\n",
      "I0228 01:47:37.661639 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2673826 virtual)\n",
      "I0228 01:47:37.665858 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2679226 virtual)\n",
      "I0228 01:47:37.667741 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2684359 virtual)\n",
      "I0228 01:47:37.669628 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2689458 virtual)\n",
      "I0228 01:47:37.699298 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2694666 virtual)\n",
      "I0228 01:47:37.714431 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2699976 virtual)\n",
      "I0228 01:47:37.723086 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2705501 virtual)\n",
      "I0228 01:47:37.725506 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2710949 virtual)\n",
      "I0228 01:47:37.727477 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2716670 virtual)\n",
      "I0228 01:47:37.761508 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2721774 virtual)\n",
      "I0228 01:47:37.770615 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2726679 virtual)\n",
      "I0228 01:47:37.776704 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2731758 virtual)\n",
      "I0228 01:47:37.781887 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2736834 virtual)\n",
      "I0228 01:47:37.786542 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2742121 virtual)\n",
      "I0228 01:47:37.824110 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2747600 virtual)\n",
      "I0228 01:47:37.836952 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2752684 virtual)\n",
      "I0228 01:47:37.841290 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2757510 virtual)\n",
      "I0228 01:47:37.843580 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2757546 virtual)\n",
      "I0228 01:47:37.897883 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:47:37.900856 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:47:37.905235 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:47:37.934824 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:47:37.939197 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:47:37.943928 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:47:37.951181 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:47:37.909682 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:47:37.947622 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:47:37.938960 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:47:38.359920 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:47:38.381764 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2757838 virtual documents\n",
      "I0228 01:47:38.471421 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:47:38.843224 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (5080 virtual)\n",
      "I0228 01:47:38.847785 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10380 virtual)\n",
      "I0228 01:47:38.851082 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16394 virtual)\n",
      "I0228 01:47:38.857820 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21308 virtual)\n",
      "I0228 01:47:38.860886 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (26974 virtual)\n",
      "I0228 01:47:38.866001 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32183 virtual)\n",
      "I0228 01:47:38.870031 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (37840 virtual)\n",
      "I0228 01:47:38.872824 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43580 virtual)\n",
      "I0228 01:47:38.874995 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (48821 virtual)\n",
      "I0228 01:47:38.877157 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54526 virtual)\n",
      "I0228 01:47:38.910836 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60417 virtual)\n",
      "I0228 01:47:38.917191 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (65837 virtual)\n",
      "I0228 01:47:38.932118 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (70991 virtual)\n",
      "I0228 01:47:38.937117 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (76243 virtual)\n",
      "I0228 01:47:38.939027 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (81772 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:38.964979 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (87905 virtual)\n",
      "I0228 01:47:38.975764 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (93791 virtual)\n",
      "I0228 01:47:38.995675 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (99400 virtual)\n",
      "I0228 01:47:38.997557 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (104482 virtual)\n",
      "I0228 01:47:39.002061 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (109923 virtual)\n",
      "I0228 01:47:39.022472 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (115414 virtual)\n",
      "I0228 01:47:39.031131 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (120652 virtual)\n",
      "I0228 01:47:39.050805 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (126256 virtual)\n",
      "I0228 01:47:39.056542 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (132194 virtual)\n",
      "I0228 01:47:39.069562 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (137748 virtual)\n",
      "I0228 01:47:39.083766 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (143016 virtual)\n",
      "I0228 01:47:39.092339 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (148601 virtual)\n",
      "I0228 01:47:39.105739 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (154669 virtual)\n",
      "I0228 01:47:39.135302 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (160593 virtual)\n",
      "I0228 01:47:39.137935 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (166155 virtual)\n",
      "I0228 01:47:39.141070 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (171458 virtual)\n",
      "I0228 01:47:39.145812 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (176898 virtual)\n",
      "I0228 01:47:39.159942 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (182213 virtual)\n",
      "I0228 01:47:39.199573 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (187483 virtual)\n",
      "I0228 01:47:39.203410 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (193361 virtual)\n",
      "I0228 01:47:39.205325 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (198581 virtual)\n",
      "I0228 01:47:39.207243 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (204339 virtual)\n",
      "I0228 01:47:39.220540 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (210012 virtual)\n",
      "I0228 01:47:39.262252 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (215478 virtual)\n",
      "I0228 01:47:39.265560 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (220999 virtual)\n",
      "I0228 01:47:39.268563 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (228672 virtual)\n",
      "I0228 01:47:39.270441 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (234104 virtual)\n",
      "I0228 01:47:39.274060 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (239511 virtual)\n",
      "I0228 01:47:39.318768 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (245244 virtual)\n",
      "I0228 01:47:39.322484 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (250748 virtual)\n",
      "I0228 01:47:39.332113 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (256051 virtual)\n",
      "I0228 01:47:39.340796 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (261354 virtual)\n",
      "I0228 01:47:39.347312 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (266729 virtual)\n",
      "I0228 01:47:39.373233 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (272354 virtual)\n",
      "I0228 01:47:39.398871 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (277728 virtual)\n",
      "I0228 01:47:39.400829 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (283192 virtual)\n",
      "I0228 01:47:39.404433 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (288866 virtual)\n",
      "I0228 01:47:39.413367 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (294138 virtual)\n",
      "I0228 01:47:39.429166 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (299551 virtual)\n",
      "I0228 01:47:39.452295 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (304887 virtual)\n",
      "I0228 01:47:39.455060 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (310506 virtual)\n",
      "I0228 01:47:39.467182 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (316227 virtual)\n",
      "I0228 01:47:39.481774 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (321961 virtual)\n",
      "I0228 01:47:39.488293 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (327166 virtual)\n",
      "I0228 01:47:39.507192 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (333725 virtual)\n",
      "I0228 01:47:39.513669 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (339338 virtual)\n",
      "I0228 01:47:39.529510 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (345206 virtual)\n",
      "I0228 01:47:39.541540 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (350810 virtual)\n",
      "I0228 01:47:39.546468 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (356864 virtual)\n",
      "I0228 01:47:39.563979 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (362567 virtual)\n",
      "I0228 01:47:39.573321 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (368185 virtual)\n",
      "I0228 01:47:39.592764 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (374187 virtual)\n",
      "I0228 01:47:39.599443 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (379802 virtual)\n",
      "I0228 01:47:39.611690 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (385693 virtual)\n",
      "I0228 01:47:39.628135 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (390337 virtual)\n",
      "I0228 01:47:39.631927 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (395514 virtual)\n",
      "I0228 01:47:39.656020 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398998 virtual)\n",
      "I0228 01:47:39.658124 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (402413 virtual)\n",
      "I0228 01:47:39.684880 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (407081 virtual)\n",
      "I0228 01:47:39.686595 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (411453 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:39.688703 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (415767 virtual)\n",
      "I0228 01:47:39.717289 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (420029 virtual)\n",
      "I0228 01:47:39.722865 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (424538 virtual)\n",
      "I0228 01:47:39.728209 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (428864 virtual)\n",
      "I0228 01:47:39.738148 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (433583 virtual)\n",
      "I0228 01:47:39.757325 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (438469 virtual)\n",
      "I0228 01:47:39.760202 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (443087 virtual)\n",
      "I0228 01:47:39.768210 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (447690 virtual)\n",
      "I0228 01:47:39.774389 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (451864 virtual)\n",
      "I0228 01:47:39.779635 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (455588 virtual)\n",
      "I0228 01:47:39.797322 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (459247 virtual)\n",
      "I0228 01:47:39.811369 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (463341 virtual)\n",
      "I0228 01:47:39.818674 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (467889 virtual)\n",
      "I0228 01:47:39.820462 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (472626 virtual)\n",
      "I0228 01:47:39.825420 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (477292 virtual)\n",
      "I0228 01:47:39.843527 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (481852 virtual)\n",
      "I0228 01:47:39.859357 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (486443 virtual)\n",
      "I0228 01:47:39.863151 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (491324 virtual)\n",
      "I0228 01:47:39.870265 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (496034 virtual)\n",
      "I0228 01:47:39.871998 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (500446 virtual)\n",
      "I0228 01:47:39.878032 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (504800 virtual)\n",
      "I0228 01:47:39.901677 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (510018 virtual)\n",
      "I0228 01:47:39.909416 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (515443 virtual)\n",
      "I0228 01:47:39.924864 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (519407 virtual)\n",
      "I0228 01:47:39.926640 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (523882 virtual)\n",
      "I0228 01:47:39.931811 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (528289 virtual)\n",
      "I0228 01:47:39.950357 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (532406 virtual)\n",
      "I0228 01:47:39.955173 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (536423 virtual)\n",
      "I0228 01:47:39.971290 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (540892 virtual)\n",
      "I0228 01:47:39.978010 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (545202 virtual)\n",
      "I0228 01:47:39.983784 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (549643 virtual)\n",
      "I0228 01:47:40.006900 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (553915 virtual)\n",
      "I0228 01:47:40.010093 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (558506 virtual)\n",
      "I0228 01:47:40.011973 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (562456 virtual)\n",
      "I0228 01:47:40.031725 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (567053 virtual)\n",
      "I0228 01:47:40.037244 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (571371 virtual)\n",
      "I0228 01:47:40.048452 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (575068 virtual)\n",
      "I0228 01:47:40.051065 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (578408 virtual)\n",
      "I0228 01:47:40.058496 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (581793 virtual)\n",
      "I0228 01:47:40.079838 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (587020 virtual)\n",
      "I0228 01:47:40.090406 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (592646 virtual)\n",
      "I0228 01:47:40.092328 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (598170 virtual)\n",
      "I0228 01:47:40.098651 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (602981 virtual)\n",
      "I0228 01:47:40.100656 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (608702 virtual)\n",
      "I0228 01:47:40.128116 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (614031 virtual)\n",
      "I0228 01:47:40.135236 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (618984 virtual)\n",
      "I0228 01:47:40.137025 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (623440 virtual)\n",
      "I0228 01:47:40.138784 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (628004 virtual)\n",
      "I0228 01:47:40.148764 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (632161 virtual)\n",
      "I0228 01:47:40.184811 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (637434 virtual)\n",
      "I0228 01:47:40.187177 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (642912 virtual)\n",
      "I0228 01:47:40.193290 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (648062 virtual)\n",
      "I0228 01:47:40.200865 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (653696 virtual)\n",
      "I0228 01:47:40.217954 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (658398 virtual)\n",
      "I0228 01:47:40.237499 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (663155 virtual)\n",
      "I0228 01:47:40.240084 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (668550 virtual)\n",
      "I0228 01:47:40.249310 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (673878 virtual)\n",
      "I0228 01:47:40.253921 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (678934 virtual)\n",
      "I0228 01:47:40.271920 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (683792 virtual)\n",
      "I0228 01:47:40.289515 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (689082 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:40.292285 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (694000 virtual)\n",
      "I0228 01:47:40.302926 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (699205 virtual)\n",
      "I0228 01:47:40.318529 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (704516 virtual)\n",
      "I0228 01:47:40.324901 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (709756 virtual)\n",
      "I0228 01:47:40.335304 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (714982 virtual)\n",
      "I0228 01:47:40.349599 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (720147 virtual)\n",
      "I0228 01:47:40.356367 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (725461 virtual)\n",
      "I0228 01:47:40.376877 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (730952 virtual)\n",
      "I0228 01:47:40.386219 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (736580 virtual)\n",
      "I0228 01:47:40.393263 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (741675 virtual)\n",
      "I0228 01:47:40.399889 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (746705 virtual)\n",
      "I0228 01:47:40.409496 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (752207 virtual)\n",
      "I0228 01:47:40.435761 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (757318 virtual)\n",
      "I0228 01:47:40.443900 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (762619 virtual)\n",
      "I0228 01:47:40.448060 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (768291 virtual)\n",
      "I0228 01:47:40.450645 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (774186 virtual)\n",
      "I0228 01:47:40.461844 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (780054 virtual)\n",
      "I0228 01:47:40.494191 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (785914 virtual)\n",
      "I0228 01:47:40.496109 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (791831 virtual)\n",
      "I0228 01:47:40.501182 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (797495 virtual)\n",
      "I0228 01:47:40.505034 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (803193 virtual)\n",
      "I0228 01:47:40.512550 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (808597 virtual)\n",
      "I0228 01:47:40.544621 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (814501 virtual)\n",
      "I0228 01:47:40.555093 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (820375 virtual)\n",
      "I0228 01:47:40.568353 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (826117 virtual)\n",
      "I0228 01:47:40.570221 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (831568 virtual)\n",
      "I0228 01:47:40.576930 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (836994 virtual)\n",
      "I0228 01:47:40.603063 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (842775 virtual)\n",
      "I0228 01:47:40.617527 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (848795 virtual)\n",
      "I0228 01:47:40.625020 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (854864 virtual)\n",
      "I0228 01:47:40.631438 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (860752 virtual)\n",
      "I0228 01:47:40.635273 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (866511 virtual)\n",
      "I0228 01:47:40.662405 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (872516 virtual)\n",
      "I0228 01:47:40.685073 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (878516 virtual)\n",
      "I0228 01:47:40.691508 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (884467 virtual)\n",
      "I0228 01:47:40.698865 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (890468 virtual)\n",
      "I0228 01:47:40.706545 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (895982 virtual)\n",
      "I0228 01:47:40.718130 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (901999 virtual)\n",
      "I0228 01:47:40.743981 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (909303 virtual)\n",
      "I0228 01:47:40.749140 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (913474 virtual)\n",
      "I0228 01:47:40.764723 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (917572 virtual)\n",
      "I0228 01:47:40.767850 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (920273 virtual)\n",
      "I0228 01:47:40.774878 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (926499 virtual)\n",
      "I0228 01:47:40.802987 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (930848 virtual)\n",
      "I0228 01:47:40.806663 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (934767 virtual)\n",
      "I0228 01:47:40.825836 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (938626 virtual)\n",
      "I0228 01:47:40.832379 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (942574 virtual)\n",
      "I0228 01:47:40.834184 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (946373 virtual)\n",
      "I0228 01:47:40.847991 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (949876 virtual)\n",
      "I0228 01:47:40.862516 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (953716 virtual)\n",
      "I0228 01:47:40.872215 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (957314 virtual)\n",
      "I0228 01:47:40.878481 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (961288 virtual)\n",
      "I0228 01:47:40.902237 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (965392 virtual)\n",
      "I0228 01:47:40.907488 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (969145 virtual)\n",
      "I0228 01:47:40.909512 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (972782 virtual)\n",
      "I0228 01:47:40.914710 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (976618 virtual)\n",
      "I0228 01:47:40.917783 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (980309 virtual)\n",
      "I0228 01:47:40.937541 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (984195 virtual)\n",
      "I0228 01:47:40.945435 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (987991 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:40.951159 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (991719 virtual)\n",
      "I0228 01:47:40.954998 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (995718 virtual)\n",
      "I0228 01:47:40.956661 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (999781 virtual)\n",
      "I0228 01:47:40.976168 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (1003236 virtual)\n",
      "I0228 01:47:40.983076 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1007025 virtual)\n",
      "I0228 01:47:40.993101 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1010957 virtual)\n",
      "I0228 01:47:40.994748 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1015458 virtual)\n",
      "I0228 01:47:40.998167 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1020551 virtual)\n",
      "I0228 01:47:41.011777 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1024455 virtual)\n",
      "I0228 01:47:41.020577 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1028661 virtual)\n",
      "I0228 01:47:41.031238 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1032299 virtual)\n",
      "I0228 01:47:41.036764 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1037356 virtual)\n",
      "I0228 01:47:41.043069 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1041103 virtual)\n",
      "I0228 01:47:41.044688 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1045251 virtual)\n",
      "I0228 01:47:41.059232 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1049424 virtual)\n",
      "I0228 01:47:41.068393 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1053629 virtual)\n",
      "I0228 01:47:41.081902 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1057577 virtual)\n",
      "I0228 01:47:41.086437 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1061388 virtual)\n",
      "I0228 01:47:41.094426 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1065439 virtual)\n",
      "I0228 01:47:41.100373 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1070002 virtual)\n",
      "I0228 01:47:41.103650 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1074476 virtual)\n",
      "I0228 01:47:41.128513 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1078766 virtual)\n",
      "I0228 01:47:41.130301 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1083486 virtual)\n",
      "I0228 01:47:41.134938 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1087525 virtual)\n",
      "I0228 01:47:41.142416 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1091201 virtual)\n",
      "I0228 01:47:41.144059 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1095254 virtual)\n",
      "I0228 01:47:41.163593 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1099402 virtual)\n",
      "I0228 01:47:41.174446 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1103296 virtual)\n",
      "I0228 01:47:41.177230 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1107732 virtual)\n",
      "I0228 01:47:41.187694 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1111756 virtual)\n",
      "I0228 01:47:41.190873 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1116961 virtual)\n",
      "I0228 01:47:41.206608 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1120696 virtual)\n",
      "I0228 01:47:41.218508 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1125714 virtual)\n",
      "I0228 01:47:41.224864 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1130098 virtual)\n",
      "I0228 01:47:41.226837 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1134989 virtual)\n",
      "I0228 01:47:41.240866 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1140153 virtual)\n",
      "I0228 01:47:41.256573 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1144193 virtual)\n",
      "I0228 01:47:41.266783 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1148022 virtual)\n",
      "I0228 01:47:41.272134 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1152350 virtual)\n",
      "I0228 01:47:41.281233 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1156668 virtual)\n",
      "I0228 01:47:41.293807 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1160576 virtual)\n",
      "I0228 01:47:41.303429 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1165484 virtual)\n",
      "I0228 01:47:41.317075 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1169845 virtual)\n",
      "I0228 01:47:41.324469 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1173845 virtual)\n",
      "I0228 01:47:41.332002 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1178815 virtual)\n",
      "I0228 01:47:41.348629 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1183638 virtual)\n",
      "I0228 01:47:41.350618 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1187891 virtual)\n",
      "I0228 01:47:41.357814 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1192146 virtual)\n",
      "I0228 01:47:41.373284 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1196891 virtual)\n",
      "I0228 01:47:41.376381 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1200848 virtual)\n",
      "I0228 01:47:41.391886 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1204962 virtual)\n",
      "I0228 01:47:41.401876 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1209270 virtual)\n",
      "I0228 01:47:41.407399 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1213729 virtual)\n",
      "I0228 01:47:41.409878 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1218365 virtual)\n",
      "I0228 01:47:41.422302 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1221962 virtual)\n",
      "I0228 01:47:41.441228 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1225322 virtual)\n",
      "I0228 01:47:41.443586 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1228577 virtual)\n",
      "I0228 01:47:41.450272 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1233039 virtual)\n",
      "I0228 01:47:41.451922 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1237175 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:41.457562 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1242429 virtual)\n",
      "I0228 01:47:41.485026 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1247165 virtual)\n",
      "I0228 01:47:41.487843 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1251266 virtual)\n",
      "I0228 01:47:41.489467 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1255565 virtual)\n",
      "I0228 01:47:41.492140 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1259627 virtual)\n",
      "I0228 01:47:41.503654 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1263725 virtual)\n",
      "I0228 01:47:41.516917 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1267634 virtual)\n",
      "I0228 01:47:41.522395 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1271577 virtual)\n",
      "I0228 01:47:41.526213 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1276150 virtual)\n",
      "I0228 01:47:41.532618 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1280304 virtual)\n",
      "I0228 01:47:41.556504 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1284497 virtual)\n",
      "I0228 01:47:41.564382 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1289138 virtual)\n",
      "I0228 01:47:41.566831 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1292815 virtual)\n",
      "I0228 01:47:41.572772 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1296649 virtual)\n",
      "I0228 01:47:41.577746 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1301633 virtual)\n",
      "I0228 01:47:41.596196 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1306373 virtual)\n",
      "I0228 01:47:41.603337 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1309672 virtual)\n",
      "I0228 01:47:41.606053 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1314419 virtual)\n",
      "I0228 01:47:41.616384 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1318325 virtual)\n",
      "I0228 01:47:41.629568 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1321605 virtual)\n",
      "I0228 01:47:41.635850 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1325647 virtual)\n",
      "I0228 01:47:41.638476 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1329888 virtual)\n",
      "I0228 01:47:41.646270 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1333908 virtual)\n",
      "I0228 01:47:41.649758 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1338711 virtual)\n",
      "I0228 01:47:41.667025 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1342919 virtual)\n",
      "I0228 01:47:41.677420 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1347809 virtual)\n",
      "I0228 01:47:41.686654 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1351483 virtual)\n",
      "I0228 01:47:41.689115 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1355056 virtual)\n",
      "I0228 01:47:41.696216 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1360353 virtual)\n",
      "I0228 01:47:41.703187 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1365597 virtual)\n",
      "I0228 01:47:41.711999 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1370330 virtual)\n",
      "I0228 01:47:41.728737 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1374447 virtual)\n",
      "I0228 01:47:41.731453 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1379353 virtual)\n",
      "I0228 01:47:41.736978 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1383622 virtual)\n",
      "I0228 01:47:41.755533 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1388342 virtual)\n",
      "I0228 01:47:41.759489 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1393290 virtual)\n",
      "I0228 01:47:41.765254 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1397958 virtual)\n",
      "I0228 01:47:41.767835 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1402146 virtual)\n",
      "I0228 01:47:41.789844 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1406894 virtual)\n",
      "I0228 01:47:41.802037 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1411521 virtual)\n",
      "I0228 01:47:41.804598 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1416024 virtual)\n",
      "I0228 01:47:41.815770 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1420577 virtual)\n",
      "I0228 01:47:41.826289 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1425187 virtual)\n",
      "I0228 01:47:41.831716 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1429954 virtual)\n",
      "I0228 01:47:41.844429 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1435816 virtual)\n",
      "I0228 01:47:41.851379 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1440801 virtual)\n",
      "I0228 01:47:41.862769 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1446081 virtual)\n",
      "I0228 01:47:41.876941 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1451482 virtual)\n",
      "I0228 01:47:41.884851 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1456833 virtual)\n",
      "I0228 01:47:41.888209 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1463800 virtual)\n",
      "I0228 01:47:41.892429 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1471408 virtual)\n",
      "I0228 01:47:41.907248 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1476905 virtual)\n",
      "I0228 01:47:41.920164 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1482126 virtual)\n",
      "I0228 01:47:41.936498 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1487429 virtual)\n",
      "I0228 01:47:41.943265 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1492680 virtual)\n",
      "I0228 01:47:41.949470 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1498131 virtual)\n",
      "I0228 01:47:41.959450 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1503082 virtual)\n",
      "I0228 01:47:41.970008 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1508258 virtual)\n",
      "I0228 01:47:41.982839 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1516998 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:42.009631 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1523742 virtual)\n",
      "I0228 01:47:42.015910 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1529001 virtual)\n",
      "I0228 01:47:42.017676 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1534895 virtual)\n",
      "I0228 01:47:42.019666 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1539907 virtual)\n",
      "I0228 01:47:42.036820 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1544113 virtual)\n",
      "I0228 01:47:42.062297 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1549198 virtual)\n",
      "I0228 01:47:42.064211 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1554232 virtual)\n",
      "I0228 01:47:42.070742 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1558963 virtual)\n",
      "I0228 01:47:42.076042 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1564150 virtual)\n",
      "I0228 01:47:42.116012 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1570005 virtual)\n",
      "I0228 01:47:42.123869 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1575905 virtual)\n",
      "I0228 01:47:42.125883 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1581844 virtual)\n",
      "I0228 01:47:42.128802 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1586973 virtual)\n",
      "I0228 01:47:42.134619 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1592882 virtual)\n",
      "I0228 01:47:42.157799 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1599058 virtual)\n",
      "I0228 01:47:42.171400 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1604825 virtual)\n",
      "I0228 01:47:42.173128 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1609290 virtual)\n",
      "I0228 01:47:42.179967 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1614703 virtual)\n",
      "I0228 01:47:42.190088 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1619884 virtual)\n",
      "I0228 01:47:42.223789 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1625159 virtual)\n",
      "I0228 01:47:42.232121 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1630696 virtual)\n",
      "I0228 01:47:42.234639 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1635893 virtual)\n",
      "I0228 01:47:42.251927 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1641579 virtual)\n",
      "I0228 01:47:42.254906 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1646684 virtual)\n",
      "I0228 01:47:42.282041 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1651943 virtual)\n",
      "I0228 01:47:42.284436 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1659779 virtual)\n",
      "I0228 01:47:42.286879 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1668163 virtual)\n",
      "I0228 01:47:42.306861 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1676473 virtual)\n",
      "I0228 01:47:42.318563 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1684556 virtual)\n",
      "I0228 01:47:42.331709 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1691962 virtual)\n",
      "I0228 01:47:42.336995 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1696881 virtual)\n",
      "I0228 01:47:42.342354 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1702938 virtual)\n",
      "I0228 01:47:42.359677 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1708575 virtual)\n",
      "I0228 01:47:42.376440 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1714359 virtual)\n",
      "I0228 01:47:42.385031 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1720621 virtual)\n",
      "I0228 01:47:42.429915 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1726303 virtual)\n",
      "I0228 01:47:42.438840 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1732084 virtual)\n",
      "I0228 01:47:42.441019 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1737150 virtual)\n",
      "I0228 01:47:42.464871 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1742424 virtual)\n",
      "I0228 01:47:42.482076 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1747789 virtual)\n",
      "I0228 01:47:42.483951 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1752856 virtual)\n",
      "I0228 01:47:42.495193 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1757712 virtual)\n",
      "I0228 01:47:42.506607 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1763137 virtual)\n",
      "I0228 01:47:42.526008 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1768579 virtual)\n",
      "I0228 01:47:42.537943 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1773403 virtual)\n",
      "I0228 01:47:42.544557 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1778350 virtual)\n",
      "I0228 01:47:42.554087 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1783296 virtual)\n",
      "I0228 01:47:42.560135 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1788428 virtual)\n",
      "I0228 01:47:42.590487 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1793524 virtual)\n",
      "I0228 01:47:42.594176 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1798925 virtual)\n",
      "I0228 01:47:42.596042 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1803999 virtual)\n",
      "I0228 01:47:42.605108 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1809215 virtual)\n",
      "I0228 01:47:42.619955 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1814443 virtual)\n",
      "I0228 01:47:42.643851 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1819485 virtual)\n",
      "I0228 01:47:42.647922 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1824395 virtual)\n",
      "I0228 01:47:42.656784 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1829705 virtual)\n",
      "I0228 01:47:42.661682 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1834771 virtual)\n",
      "I0228 01:47:42.676911 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1840473 virtual)\n",
      "I0228 01:47:42.694443 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1845188 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:42.705557 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1850199 virtual)\n",
      "I0228 01:47:42.707299 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1854874 virtual)\n",
      "I0228 01:47:42.722414 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1860002 virtual)\n",
      "I0228 01:47:42.736893 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1865110 virtual)\n",
      "I0228 01:47:42.749318 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1870132 virtual)\n",
      "I0228 01:47:42.754470 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1875134 virtual)\n",
      "I0228 01:47:42.760864 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1880409 virtual)\n",
      "I0228 01:47:42.784688 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1885905 virtual)\n",
      "I0228 01:47:42.799827 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1891129 virtual)\n",
      "I0228 01:47:42.801605 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1895929 virtual)\n",
      "I0228 01:47:42.806432 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1901485 virtual)\n",
      "I0228 01:47:42.808322 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1906506 virtual)\n",
      "I0228 01:47:42.842960 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1911250 virtual)\n",
      "I0228 01:47:42.855367 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1915777 virtual)\n",
      "I0228 01:47:42.858187 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1920918 virtual)\n",
      "I0228 01:47:42.860067 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1926236 virtual)\n",
      "I0228 01:47:42.861881 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1931116 virtual)\n",
      "I0228 01:47:42.907494 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1936358 virtual)\n",
      "I0228 01:47:42.909348 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1941295 virtual)\n",
      "I0228 01:47:42.911215 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1946368 virtual)\n",
      "I0228 01:47:42.913134 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1951734 virtual)\n",
      "I0228 01:47:42.915440 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1956720 virtual)\n",
      "I0228 01:47:42.962017 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1961967 virtual)\n",
      "I0228 01:47:42.964145 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1966824 virtual)\n",
      "I0228 01:47:42.966052 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1972274 virtual)\n",
      "I0228 01:47:42.967975 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1977533 virtual)\n",
      "I0228 01:47:42.969824 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1982518 virtual)\n",
      "I0228 01:47:43.018534 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1987864 virtual)\n",
      "I0228 01:47:43.020524 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (1993351 virtual)\n",
      "I0228 01:47:43.022469 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (1998669 virtual)\n",
      "I0228 01:47:43.025841 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (2003688 virtual)\n",
      "I0228 01:47:43.028321 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (2009101 virtual)\n",
      "I0228 01:47:43.069709 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (2014379 virtual)\n",
      "I0228 01:47:43.074284 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (2019161 virtual)\n",
      "I0228 01:47:43.078607 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (2024102 virtual)\n",
      "I0228 01:47:43.085117 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (2029250 virtual)\n",
      "I0228 01:47:43.088742 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2033777 virtual)\n",
      "I0228 01:47:43.125910 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2038464 virtual)\n",
      "I0228 01:47:43.132230 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2043125 virtual)\n",
      "I0228 01:47:43.134044 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2048080 virtual)\n",
      "I0228 01:47:43.146610 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2053448 virtual)\n",
      "I0228 01:47:43.149356 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2058330 virtual)\n",
      "I0228 01:47:43.179688 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2063510 virtual)\n",
      "I0228 01:47:43.183299 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2068827 virtual)\n",
      "I0228 01:47:43.188180 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2074185 virtual)\n",
      "I0228 01:47:43.202327 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2079506 virtual)\n",
      "I0228 01:47:43.210854 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2084778 virtual)\n",
      "I0228 01:47:43.230971 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2090305 virtual)\n",
      "I0228 01:47:43.233124 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2094893 virtual)\n",
      "I0228 01:47:43.246685 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2099862 virtual)\n",
      "I0228 01:47:43.265231 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2104663 virtual)\n",
      "I0228 01:47:43.268710 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2109627 virtual)\n",
      "I0228 01:47:43.283807 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2114142 virtual)\n",
      "I0228 01:47:43.288510 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2119531 virtual)\n",
      "I0228 01:47:43.300690 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2124933 virtual)\n",
      "I0228 01:47:43.325230 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2130462 virtual)\n",
      "I0228 01:47:43.330597 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2135429 virtual)\n",
      "I0228 01:47:43.334477 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2140614 virtual)\n",
      "I0228 01:47:43.341660 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2145789 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:43.353330 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2150685 virtual)\n",
      "I0228 01:47:43.380333 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2155816 virtual)\n",
      "I0228 01:47:43.389855 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2161078 virtual)\n",
      "I0228 01:47:43.394376 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2165924 virtual)\n",
      "I0228 01:47:43.398927 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2171109 virtual)\n",
      "I0228 01:47:43.410195 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2175907 virtual)\n",
      "I0228 01:47:43.442319 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2181349 virtual)\n",
      "I0228 01:47:43.444259 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2186616 virtual)\n",
      "I0228 01:47:43.451745 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2191630 virtual)\n",
      "I0228 01:47:43.456899 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2197121 virtual)\n",
      "I0228 01:47:43.460740 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2202064 virtual)\n",
      "I0228 01:47:43.499083 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2207610 virtual)\n",
      "I0228 01:47:43.502041 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2212486 virtual)\n",
      "I0228 01:47:43.504809 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2217211 virtual)\n",
      "I0228 01:47:43.508909 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2222585 virtual)\n",
      "I0228 01:47:43.519519 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2227766 virtual)\n",
      "I0228 01:47:43.555475 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2232707 virtual)\n",
      "I0228 01:47:43.557314 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2237965 virtual)\n",
      "I0228 01:47:43.559684 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2243048 virtual)\n",
      "I0228 01:47:43.567251 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2247923 virtual)\n",
      "I0228 01:47:43.580325 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2252928 virtual)\n",
      "I0228 01:47:43.610010 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2257973 virtual)\n",
      "I0228 01:47:43.611967 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2262908 virtual)\n",
      "I0228 01:47:43.616482 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2268045 virtual)\n",
      "I0228 01:47:43.622810 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2273176 virtual)\n",
      "I0228 01:47:43.642333 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2277892 virtual)\n",
      "I0228 01:47:43.661629 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2283282 virtual)\n",
      "I0228 01:47:43.664922 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2288123 virtual)\n",
      "I0228 01:47:43.673048 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2293281 virtual)\n",
      "I0228 01:47:43.676591 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2298568 virtual)\n",
      "I0228 01:47:43.702249 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2303887 virtual)\n",
      "I0228 01:47:43.712238 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2308990 virtual)\n",
      "I0228 01:47:43.731444 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2314167 virtual)\n",
      "I0228 01:47:43.733319 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2319268 virtual)\n",
      "I0228 01:47:43.735861 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2324457 virtual)\n",
      "I0228 01:47:43.761586 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2330006 virtual)\n",
      "I0228 01:47:43.769729 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2335135 virtual)\n",
      "I0228 01:47:43.784346 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2339955 virtual)\n",
      "I0228 01:47:43.786184 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2344936 virtual)\n",
      "I0228 01:47:43.789783 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2349787 virtual)\n",
      "I0228 01:47:43.821574 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2355417 virtual)\n",
      "I0228 01:47:43.824866 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2360257 virtual)\n",
      "I0228 01:47:43.838609 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2365113 virtual)\n",
      "I0228 01:47:43.840553 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2370721 virtual)\n",
      "I0228 01:47:43.846053 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2375841 virtual)\n",
      "I0228 01:47:43.881333 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2380851 virtual)\n",
      "I0228 01:47:43.885149 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2385852 virtual)\n",
      "I0228 01:47:43.886954 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2390887 virtual)\n",
      "I0228 01:47:43.890680 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2395900 virtual)\n",
      "I0228 01:47:43.903024 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2400780 virtual)\n",
      "I0228 01:47:43.939202 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2406152 virtual)\n",
      "I0228 01:47:43.946299 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2411659 virtual)\n",
      "I0228 01:47:43.949485 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2416796 virtual)\n",
      "I0228 01:47:43.956836 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2421573 virtual)\n",
      "I0228 01:47:43.961187 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2426740 virtual)\n",
      "I0228 01:47:43.987685 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2432059 virtual)\n",
      "I0228 01:47:44.002268 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2436940 virtual)\n",
      "I0228 01:47:44.009113 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2441832 virtual)\n",
      "I0228 01:47:44.010961 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2446906 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:44.012847 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2451756 virtual)\n",
      "I0228 01:47:44.045646 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2457232 virtual)\n",
      "I0228 01:47:44.059509 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2462824 virtual)\n",
      "I0228 01:47:44.063914 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2467829 virtual)\n",
      "I0228 01:47:44.066453 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2473277 virtual)\n",
      "I0228 01:47:44.071456 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2477914 virtual)\n",
      "I0228 01:47:44.100373 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2482661 virtual)\n",
      "I0228 01:47:44.110826 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2487832 virtual)\n",
      "I0228 01:47:44.112736 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2493281 virtual)\n",
      "I0228 01:47:44.119896 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2498545 virtual)\n",
      "I0228 01:47:44.127681 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2503643 virtual)\n",
      "I0228 01:47:44.155422 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2508483 virtual)\n",
      "I0228 01:47:44.165646 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2513792 virtual)\n",
      "I0228 01:47:44.167479 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2518917 virtual)\n",
      "I0228 01:47:44.176465 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2523832 virtual)\n",
      "I0228 01:47:44.180058 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2528888 virtual)\n",
      "I0228 01:47:44.207002 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2533962 virtual)\n",
      "I0228 01:47:44.226081 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2538990 virtual)\n",
      "I0228 01:47:44.228304 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2544236 virtual)\n",
      "I0228 01:47:44.230925 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2549054 virtual)\n",
      "I0228 01:47:44.241197 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2554582 virtual)\n",
      "I0228 01:47:44.257399 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2559981 virtual)\n",
      "I0228 01:47:44.281317 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2564758 virtual)\n",
      "I0228 01:47:44.283178 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2569757 virtual)\n",
      "I0228 01:47:44.285108 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2575315 virtual)\n",
      "I0228 01:47:44.303565 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2580688 virtual)\n",
      "I0228 01:47:44.308296 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2585971 virtual)\n",
      "I0228 01:47:44.334052 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2591405 virtual)\n",
      "I0228 01:47:44.335953 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2596616 virtual)\n",
      "I0228 01:47:44.338379 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2602118 virtual)\n",
      "I0228 01:47:44.370140 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2607172 virtual)\n",
      "I0228 01:47:44.376675 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2612115 virtual)\n",
      "I0228 01:47:44.381465 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2617211 virtual)\n",
      "I0228 01:47:44.388729 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2622431 virtual)\n",
      "I0228 01:47:44.396476 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2627020 virtual)\n",
      "I0228 01:47:44.432489 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2632178 virtual)\n",
      "I0228 01:47:44.435223 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2636945 virtual)\n",
      "I0228 01:47:44.437025 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2641995 virtual)\n",
      "I0228 01:47:44.447308 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2647287 virtual)\n",
      "I0228 01:47:44.450810 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2652543 virtual)\n",
      "I0228 01:47:44.487068 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2658073 virtual)\n",
      "I0228 01:47:44.489033 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2663260 virtual)\n",
      "I0228 01:47:44.497179 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2668735 virtual)\n",
      "I0228 01:47:44.500778 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2673826 virtual)\n",
      "I0228 01:47:44.503101 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2679226 virtual)\n",
      "I0228 01:47:44.535132 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2684359 virtual)\n",
      "I0228 01:47:44.540301 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2689458 virtual)\n",
      "I0228 01:47:44.553982 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2694666 virtual)\n",
      "I0228 01:47:44.556578 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2699976 virtual)\n",
      "I0228 01:47:44.560591 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2705501 virtual)\n",
      "I0228 01:47:44.592468 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2710949 virtual)\n",
      "I0228 01:47:44.595670 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2716670 virtual)\n",
      "I0228 01:47:44.607382 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2721774 virtual)\n",
      "I0228 01:47:44.610910 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2726679 virtual)\n",
      "I0228 01:47:44.622170 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2731758 virtual)\n",
      "I0228 01:47:44.646819 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2736834 virtual)\n",
      "I0228 01:47:44.650986 140370323834688 text_analysis.py:506] 546 batches submitted to accumulate stats from 34944 documents (2742121 virtual)\n",
      "I0228 01:47:44.659583 140370323834688 text_analysis.py:506] 547 batches submitted to accumulate stats from 35008 documents (2747600 virtual)\n",
      "I0228 01:47:44.664566 140370323834688 text_analysis.py:506] 548 batches submitted to accumulate stats from 35072 documents (2752684 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:47:44.687169 140370323834688 text_analysis.py:506] 549 batches submitted to accumulate stats from 35136 documents (2757510 virtual)\n",
      "I0228 01:47:44.705457 140370323834688 text_analysis.py:506] 550 batches submitted to accumulate stats from 35200 documents (2757546 virtual)\n",
      "I0228 01:47:44.758839 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:47:44.761758 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:47:44.762829 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:47:44.767812 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:47:44.770081 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:47:44.770978 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:47:44.785029 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:47:44.790337 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:47:44.800918 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:47:44.775696 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:47:45.223469 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:47:45.240621 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2757838 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores\n",
      "{'epoch': 224, 'cv': 0.6884587193938962, 'umass': -4.284694515013507, 'uci': -0.07300283050019216, 'npmi': 0.07518600952302683, 'rbo': 1.0, 'td': 1.0, 'train_loss': 641.7252850926455, 'topics': [['c0267454', 'c1446219', 'ref', 'heparin', 'c0605290', 'salivary', 'c0521990', 'c0052432', 'c0851891', 'c0536858', 'c0020933', 'c1960870', 'c0751651', 'postpartum', 'c0032821', 'c0036323', 'c0024348', 'c0038174', 'cow', 'colitis', 'somatic', 'altitude', 'c0444584', 'swell', 'shareholder', 'c0007789'], ['c0011900', 'c0015967', 'c0012634', 'c0032285', 'child', 'c0035236', 'c0546788', 'c0003232', 'c1457887', 'common', 'c0010200', 'clinical', 'presentation', 'disclosure', 'c0231221', 'c0021400', 'c0038410', 'c0027442', 'c0809949', 'c0019993', 'hospitalize', 'sars-cov-2', 'c0221423', 'c0039082', 'respiratory', 'manifestation'], ['c0543467', 'c0025080', 'c0031150', 'postoperative', 'c0038930', 'operative', 'c0728940', 'c0002940', 'c0005898', 'c0850292', 'c0229962', 'procedure', 'recurrence', 'c0014245', 'undergo', 'c0547070', 'c0009566', 'perioperative', 'surgical', 'c0582175', 'c0162522', 'c0187996', 'conversion', 'c0019080', 'perform', 'preoperative'], ['c0199470', 'compare', 'c0243095', 'c0034108', 'c0032042', 'significantly', 'difference', 'c0918012', 'measurement', 'regression', 'significant', 'predictor', 'curve', 'c0369768', 'neonate', 'c0021708', 'c0005516', 'c0235195', 'confidence', 'c0032740', 'odds', 'c0038454', 'concentration', 'association', 'c0005823', 'decrease'], ['crisis', 'policy', 'disaster', 'economic', 'threat', 'political', 'market', 'emergency', 'economy', 'sector', 'draw', 'food', 'inequality', 'public', 'trade', 'argue', 'society', 'face', 'c1561598', 'national', 'governance', 'international', 'investment', 'supply', 'financial', 'management'], ['c0042210', 'c1254351', 'c0030956', 'c0029224', 'c1167622', 'c0003316', 'c0003320', 'c1706082', 'c0003250', 'affinity', 'c0020971', 'active', 'c1514562', 'nanoparticles', 'potency', 'candidate', 'drug', 'elicit', 'promise', 'potential', 'c3687832', 'c0243077', 'potent', 'synthetic', 'c0034861', 'c0005479'], ['activation', 'c1171362', 'mechanism', 'c0007613', 'c0024432', 'pathway', 'role', 'c0007634', 'c0079189', 'c0162638', 'c0025929', 'c0017262', 'c0021747', 'activate', 'c3539881', 'induction', 'c0004391', 'induce', 'c1327622', 'c1101610', 'suppress', 'mediate', 'c0013081', 'c0023810', 'c0014597', 'c0021368'], ['propose', 'c0002045', 'c3161035', 'c0150098', 'automate', 'machine', 'c0025663', 'c0679083', 'accuracy', 'performance', 'sensor', 'algorithm', 'outperform', 'prediction', 'image', 'representation', 'c0037585', 'c1710191', 'compute', 'computational', 'c1704254', 'filter', 'input', 'automatically', 'solve', 'real-world'], ['c0679646', 'search', 'c2603343', 'conduct', 'c0025353', 'c0242356', 'report', 'train', 'c0038951', 'c0086388', 'evidence', 'c0027361', 'c0003467', 'include', 'c0282122', 'c1257890', 'c0030971', 'impact', 'parent', 'psychological', 'c0242481', 'c1955832', 'c0034394', 'c0184661', 'recommendation', 'c1706852'], ['c1705920', 'c0684063', 'c0042776', 'c0017428', 'c0032098', 'c0005595', 'genetic', 'c0003062', 'c0012984', 'c1764827', 'genotype', 'sample', 'c0017337', 'c0039005', 'c0007452', 'c0015733', 'c0004793', 'c0026882', 'c0015219', 'diversity', 'c0162326', 'c1519068', 'isolate', 'c0242781', 'specie', 'c0017446']]}\n",
      "Epoch: [226/250]\tSamples: [8300076/9181500]\tTrain Loss: 641.5911225400602\tTime: 0:00:04.534075\n",
      "Epoch: [227/250]\tSamples: [8336802/9181500]\tTrain Loss: 641.7605238795404\tTime: 0:00:04.541609\n",
      "Epoch: [228/250]\tSamples: [8373528/9181500]\tTrain Loss: 641.7334967691486\tTime: 0:00:04.597274\n",
      "Epoch: [229/250]\tSamples: [8410254/9181500]\tTrain Loss: 641.8154862188912\tTime: 0:00:04.641027\n",
      "Epoch: [230/250]\tSamples: [8446980/9181500]\tTrain Loss: 641.7201507872488\tTime: 0:00:04.833887\n",
      "Epoch: [231/250]\tSamples: [8483706/9181500]\tTrain Loss: 641.737357815308\tTime: 0:00:04.857232\n",
      "Epoch: [232/250]\tSamples: [8520432/9181500]\tTrain Loss: 641.7166005307888\tTime: 0:00:04.856989\n",
      "Epoch: [233/250]\tSamples: [8557158/9181500]\tTrain Loss: 641.823362748802\tTime: 0:00:04.823260\n",
      "Epoch: [234/250]\tSamples: [8593884/9181500]\tTrain Loss: 641.9818508175407\tTime: 0:00:04.851740\n",
      "Epoch: [235/250]\tSamples: [8630610/9181500]\tTrain Loss: 641.9304086342102\tTime: 0:00:04.885455\n",
      "Epoch: [236/250]\tSamples: [8667336/9181500]\tTrain Loss: 641.7561761209702\tTime: 0:00:04.867899\n",
      "Epoch: [237/250]\tSamples: [8704062/9181500]\tTrain Loss: 641.6707690481539\tTime: 0:00:04.892109\n",
      "Epoch: [238/250]\tSamples: [8740788/9181500]\tTrain Loss: 641.9715667842605\tTime: 0:00:04.843182\n",
      "Epoch: [239/250]\tSamples: [8777514/9181500]\tTrain Loss: 641.8646016275935\tTime: 0:00:04.856550\n",
      "Epoch: [240/250]\tSamples: [8814240/9181500]\tTrain Loss: 641.8721537534717\tTime: 0:00:04.808869\n",
      "Epoch: [241/250]\tSamples: [8850966/9181500]\tTrain Loss: 641.7472344822129\tTime: 0:00:04.863938\n",
      "Epoch: [242/250]\tSamples: [8887692/9181500]\tTrain Loss: 641.7796933924536\tTime: 0:00:04.878523\n",
      "Epoch: [243/250]\tSamples: [8924418/9181500]\tTrain Loss: 642.1461924113706\tTime: 0:00:04.875092\n",
      "Epoch: [244/250]\tSamples: [8961144/9181500]\tTrain Loss: 641.947049603823\tTime: 0:00:04.829101\n",
      "Epoch: [245/250]\tSamples: [8997870/9181500]\tTrain Loss: 641.8014091047555\tTime: 0:00:04.856760\n",
      "Epoch: [246/250]\tSamples: [9034596/9181500]\tTrain Loss: 641.8449539112278\tTime: 0:00:04.887609\n",
      "Epoch: [247/250]\tSamples: [9071322/9181500]\tTrain Loss: 641.4961654379731\tTime: 0:00:04.914435\n",
      "Epoch: [248/250]\tSamples: [9108048/9181500]\tTrain Loss: 641.5791467131596\tTime: 0:00:04.848712\n",
      "Epoch: [249/250]\tSamples: [9144774/9181500]\tTrain Loss: 641.7731636179069\tTime: 0:00:04.898636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:49:45.817716 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [250/250]\tSamples: [9181500/9181500]\tTrain Loss: 641.681149977196\tTime: 0:00:04.910991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:49:46.628934 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:49:47.332260 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:49:48.166170 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:49:48.705174 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:49:48.711642 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:49:49.480440 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:49:50.168308 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:49:50.984318 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:49:51.508897 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:49:51.513997 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:49:52.273612 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:49:52.950742 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:49:53.761212 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:49:54.293340 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:49:54.299997 140370323834688 dictionary.py:209] adding document #0 to Dictionary(0 unique tokens: [])\n",
      "I0228 01:49:55.063555 140370323834688 dictionary.py:209] adding document #10000 to Dictionary(47025 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:49:55.750785 140370323834688 dictionary.py:209] adding document #20000 to Dictionary(69295 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:49:56.565004 140370323834688 dictionary.py:209] adding document #30000 to Dictionary(90493 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...)\n",
      "I0228 01:49:57.091180 140370323834688 dictionary.py:216] built Dictionary(102477 unique tokens: ['admission', 'affect', 'associate', 'breathe', 'bronchial']...) from 36726 documents (total 3132041 corpus positions)\n",
      "I0228 01:49:57.113904 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:49:57.633400 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (-32914 virtual)\n",
      "I0228 01:49:58.012724 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (-201181 virtual)\n",
      "I0228 01:49:58.304112 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (-463165 virtual)\n",
      "I0228 01:49:58.306366 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (-461934 virtual)\n",
      "I0228 01:49:58.337163 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (-466106 virtual)\n",
      "I0228 01:49:58.427694 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (-489230 virtual)\n",
      "I0228 01:49:58.430284 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (-486993 virtual)\n",
      "I0228 01:49:58.432412 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (-485265 virtual)\n",
      "I0228 01:49:58.436434 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (-483740 virtual)\n",
      "I0228 01:49:58.446325 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (-485602 virtual)\n",
      "I0228 01:49:59.075568 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:49:59.077588 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:49:59.078789 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:49:59.081293 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:49:59.082828 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:49:59.084105 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:49:59.087294 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:49:59.089541 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:49:59.086779 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:49:59.095505 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:49:59.520827 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:49:59.543076 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 228631 virtual documents\n",
      "I0228 01:50:01.921350 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 1000 documents\n",
      "I0228 01:50:01.936522 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 2000 documents\n",
      "I0228 01:50:01.960638 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 3000 documents\n",
      "I0228 01:50:01.976322 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 4000 documents\n",
      "I0228 01:50:01.990653 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 5000 documents\n",
      "I0228 01:50:02.003558 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 6000 documents\n",
      "I0228 01:50:02.016999 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 7000 documents\n",
      "I0228 01:50:02.030631 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 8000 documents\n",
      "I0228 01:50:02.045362 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 9000 documents\n",
      "I0228 01:50:02.060588 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 10000 documents\n",
      "I0228 01:50:02.078014 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 11000 documents\n",
      "I0228 01:50:02.095420 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 12000 documents\n",
      "I0228 01:50:02.107130 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 13000 documents\n",
      "I0228 01:50:02.119246 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 14000 documents\n",
      "I0228 01:50:02.131255 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 15000 documents\n",
      "I0228 01:50:02.143657 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 16000 documents\n",
      "I0228 01:50:02.155106 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 17000 documents\n",
      "I0228 01:50:02.167003 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 18000 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:02.179766 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 19000 documents\n",
      "I0228 01:50:02.192393 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 20000 documents\n",
      "I0228 01:50:02.206719 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 21000 documents\n",
      "I0228 01:50:02.222667 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 22000 documents\n",
      "I0228 01:50:02.238666 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 23000 documents\n",
      "I0228 01:50:02.254127 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 24000 documents\n",
      "I0228 01:50:02.268509 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 25000 documents\n",
      "I0228 01:50:02.282936 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 26000 documents\n",
      "I0228 01:50:02.297603 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 27000 documents\n",
      "I0228 01:50:02.311916 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 28000 documents\n",
      "I0228 01:50:02.326284 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 29000 documents\n",
      "I0228 01:50:02.340682 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 30000 documents\n",
      "I0228 01:50:02.356681 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 31000 documents\n",
      "I0228 01:50:02.371016 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 32000 documents\n",
      "I0228 01:50:02.385242 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 33000 documents\n",
      "I0228 01:50:02.399424 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 34000 documents\n",
      "I0228 01:50:02.414335 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 35000 documents\n",
      "I0228 01:50:02.429045 140370323834688 text_analysis.py:124] CorpusAccumulator accumulated stats from 36000 documents\n",
      "I0228 01:50:02.579069 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:50:02.971443 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (5147 virtual)\n",
      "I0228 01:50:02.976124 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10416 virtual)\n",
      "I0228 01:50:02.979012 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16377 virtual)\n",
      "I0228 01:50:02.981433 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21345 virtual)\n",
      "I0228 01:50:02.984123 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27120 virtual)\n",
      "I0228 01:50:02.986229 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32426 virtual)\n",
      "I0228 01:50:02.988364 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (38036 virtual)\n",
      "I0228 01:50:02.990524 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43845 virtual)\n",
      "I0228 01:50:02.992835 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (49175 virtual)\n",
      "I0228 01:50:02.996920 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54827 virtual)\n",
      "I0228 01:50:03.041787 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60678 virtual)\n",
      "I0228 01:50:03.045085 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (66144 virtual)\n",
      "I0228 01:50:03.053716 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (71241 virtual)\n",
      "I0228 01:50:03.062178 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (76404 virtual)\n",
      "I0228 01:50:03.073569 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (82276 virtual)\n",
      "I0228 01:50:03.102154 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (88346 virtual)\n",
      "I0228 01:50:03.104284 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (94139 virtual)\n",
      "I0228 01:50:03.110709 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (99686 virtual)\n",
      "I0228 01:50:03.117521 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (105070 virtual)\n",
      "I0228 01:50:03.139904 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (110565 virtual)\n",
      "I0228 01:50:03.159839 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (115989 virtual)\n",
      "I0228 01:50:03.162942 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (121164 virtual)\n",
      "I0228 01:50:03.165008 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (126875 virtual)\n",
      "I0228 01:50:03.170367 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (132724 virtual)\n",
      "I0228 01:50:03.209707 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (138214 virtual)\n",
      "I0228 01:50:03.222125 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (143466 virtual)\n",
      "I0228 01:50:03.225555 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (149242 virtual)\n",
      "I0228 01:50:03.228540 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (155443 virtual)\n",
      "I0228 01:50:03.231435 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (161270 virtual)\n",
      "I0228 01:50:03.274478 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (166677 virtual)\n",
      "I0228 01:50:03.277355 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (172008 virtual)\n",
      "I0228 01:50:03.284276 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (177472 virtual)\n",
      "I0228 01:50:03.287364 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (182713 virtual)\n",
      "I0228 01:50:03.305919 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (188282 virtual)\n",
      "I0228 01:50:03.329016 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (193942 virtual)\n",
      "I0228 01:50:03.331119 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (199446 virtual)\n",
      "I0228 01:50:03.340541 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (205063 virtual)\n",
      "I0228 01:50:03.347079 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (210539 virtual)\n",
      "I0228 01:50:03.375252 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (216113 virtual)\n",
      "I0228 01:50:03.381106 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (221775 virtual)\n",
      "I0228 01:50:03.392771 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (229486 virtual)\n",
      "I0228 01:50:03.396541 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (234904 virtual)\n",
      "I0228 01:50:03.400587 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (240473 virtual)\n",
      "I0228 01:50:03.436349 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (246064 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:03.443406 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (251429 virtual)\n",
      "I0228 01:50:03.455929 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (256742 virtual)\n",
      "I0228 01:50:03.457751 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (261985 virtual)\n",
      "I0228 01:50:03.459949 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (267581 virtual)\n",
      "I0228 01:50:03.490707 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (273098 virtual)\n",
      "I0228 01:50:03.508759 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (278406 virtual)\n",
      "I0228 01:50:03.512653 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (284230 virtual)\n",
      "I0228 01:50:03.519680 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (289398 virtual)\n",
      "I0228 01:50:03.524018 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (294826 virtual)\n",
      "I0228 01:50:03.545068 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (300438 virtual)\n",
      "I0228 01:50:03.564182 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (305889 virtual)\n",
      "I0228 01:50:03.569640 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (311283 virtual)\n",
      "I0228 01:50:03.573808 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (316960 virtual)\n",
      "I0228 01:50:03.593783 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (322437 virtual)\n",
      "I0228 01:50:03.599953 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (328585 virtual)\n",
      "I0228 01:50:03.617795 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (334640 virtual)\n",
      "I0228 01:50:03.625238 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (340306 virtual)\n",
      "I0228 01:50:03.636287 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (346052 virtual)\n",
      "I0228 01:50:03.654591 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (351906 virtual)\n",
      "I0228 01:50:03.658705 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (357949 virtual)\n",
      "I0228 01:50:03.671755 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (363469 virtual)\n",
      "I0228 01:50:03.681414 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (369287 virtual)\n",
      "I0228 01:50:03.697534 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (375149 virtual)\n",
      "I0228 01:50:03.710195 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (381002 virtual)\n",
      "I0228 01:50:03.728400 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (386314 virtual)\n",
      "I0228 01:50:03.736266 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (390914 virtual)\n",
      "I0228 01:50:03.742166 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (395397 virtual)\n",
      "I0228 01:50:03.759974 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398896 virtual)\n",
      "I0228 01:50:03.768397 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (402773 virtual)\n",
      "I0228 01:50:03.786069 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (407483 virtual)\n",
      "I0228 01:50:03.789190 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (411816 virtual)\n",
      "I0228 01:50:03.805958 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (416296 virtual)\n",
      "I0228 01:50:03.826053 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (420520 virtual)\n",
      "I0228 01:50:03.827780 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (424931 virtual)\n",
      "I0228 01:50:03.829478 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (429299 virtual)\n",
      "I0228 01:50:03.838189 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (434202 virtual)\n",
      "I0228 01:50:03.860637 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (439059 virtual)\n",
      "I0228 01:50:03.870015 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (444024 virtual)\n",
      "I0228 01:50:03.872021 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (448272 virtual)\n",
      "I0228 01:50:03.873706 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (452141 virtual)\n",
      "I0228 01:50:03.885262 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (456220 virtual)\n",
      "I0228 01:50:03.910581 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (459450 virtual)\n",
      "I0228 01:50:03.912325 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (463980 virtual)\n",
      "I0228 01:50:03.914754 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (468660 virtual)\n",
      "I0228 01:50:03.923232 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (473377 virtual)\n",
      "I0228 01:50:03.930915 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (477876 virtual)\n",
      "I0228 01:50:03.957524 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (482592 virtual)\n",
      "I0228 01:50:03.961198 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (487337 virtual)\n",
      "I0228 01:50:03.968210 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (492127 virtual)\n",
      "I0228 01:50:03.972546 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (496571 virtual)\n",
      "I0228 01:50:03.978745 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (500926 virtual)\n",
      "I0228 01:50:03.988779 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (506209 virtual)\n",
      "I0228 01:50:04.003031 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (510944 virtual)\n",
      "I0228 01:50:04.021793 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (515605 virtual)\n",
      "I0228 01:50:04.024323 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (519849 virtual)\n",
      "I0228 01:50:04.036052 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (524512 virtual)\n",
      "I0228 01:50:04.038537 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (528520 virtual)\n",
      "I0228 01:50:04.048424 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (533173 virtual)\n",
      "I0228 01:50:04.067487 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (536912 virtual)\n",
      "I0228 01:50:04.088161 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (541511 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:04.090723 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (546096 virtual)\n",
      "I0228 01:50:04.092332 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (550393 virtual)\n",
      "I0228 01:50:04.098003 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (555162 virtual)\n",
      "I0228 01:50:04.115057 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (558995 virtual)\n",
      "I0228 01:50:04.133528 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (563760 virtual)\n",
      "I0228 01:50:04.140181 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (567801 virtual)\n",
      "I0228 01:50:04.146043 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (571933 virtual)\n",
      "I0228 01:50:04.148390 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (575772 virtual)\n",
      "I0228 01:50:04.155657 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (578840 virtual)\n",
      "I0228 01:50:04.183449 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (583535 virtual)\n",
      "I0228 01:50:04.186722 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (588945 virtual)\n",
      "I0228 01:50:04.191751 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (594411 virtual)\n",
      "I0228 01:50:04.197376 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (599281 virtual)\n",
      "I0228 01:50:04.208506 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (604741 virtual)\n",
      "I0228 01:50:04.229307 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (610620 virtual)\n",
      "I0228 01:50:04.239409 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (615767 virtual)\n",
      "I0228 01:50:04.241071 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (620067 virtual)\n",
      "I0228 01:50:04.248709 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (624865 virtual)\n",
      "I0228 01:50:04.258590 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (628663 virtual)\n",
      "I0228 01:50:04.275247 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (634161 virtual)\n",
      "I0228 01:50:04.295975 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (639403 virtual)\n",
      "I0228 01:50:04.305683 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (644635 virtual)\n",
      "I0228 01:50:04.309566 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (650290 virtual)\n",
      "I0228 01:50:04.327108 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (655264 virtual)\n",
      "I0228 01:50:04.337540 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (659917 virtual)\n",
      "I0228 01:50:04.350132 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (665153 virtual)\n",
      "I0228 01:50:04.358172 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (670503 virtual)\n",
      "I0228 01:50:04.368677 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (675595 virtual)\n",
      "I0228 01:50:04.372347 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (680585 virtual)\n",
      "I0228 01:50:04.391853 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (685705 virtual)\n",
      "I0228 01:50:04.400454 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (690742 virtual)\n",
      "I0228 01:50:04.415114 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (695883 virtual)\n",
      "I0228 01:50:04.422910 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (701204 virtual)\n",
      "I0228 01:50:04.434710 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (706339 virtual)\n",
      "I0228 01:50:04.450924 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (711719 virtual)\n",
      "I0228 01:50:04.453074 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (716768 virtual)\n",
      "I0228 01:50:04.476825 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (722144 virtual)\n",
      "I0228 01:50:04.480110 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (727436 virtual)\n",
      "I0228 01:50:04.482507 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (733054 virtual)\n",
      "I0228 01:50:04.501500 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (738504 virtual)\n",
      "I0228 01:50:04.503257 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (743381 virtual)\n",
      "I0228 01:50:04.533456 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (748915 virtual)\n",
      "I0228 01:50:04.536966 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (753958 virtual)\n",
      "I0228 01:50:04.539894 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (759491 virtual)\n",
      "I0228 01:50:04.548125 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (765049 virtual)\n",
      "I0228 01:50:04.582798 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (770926 virtual)\n",
      "I0228 01:50:04.586127 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (776750 virtual)\n",
      "I0228 01:50:04.597555 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (782701 virtual)\n",
      "I0228 01:50:04.599527 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (788615 virtual)\n",
      "I0228 01:50:04.601490 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (794166 virtual)\n",
      "I0228 01:50:04.626027 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (799763 virtual)\n",
      "I0228 01:50:04.637153 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (805460 virtual)\n",
      "I0228 01:50:04.653697 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (811282 virtual)\n",
      "I0228 01:50:04.656342 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (817112 virtual)\n",
      "I0228 01:50:04.660876 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (823037 virtual)\n",
      "I0228 01:50:04.688600 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (828423 virtual)\n",
      "I0228 01:50:04.715541 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (833736 virtual)\n",
      "I0228 01:50:04.717509 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (839273 virtual)\n",
      "I0228 01:50:04.721250 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (845526 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:04.738366 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (851482 virtual)\n",
      "I0228 01:50:04.741243 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (857473 virtual)\n",
      "I0228 01:50:04.773052 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (863312 virtual)\n",
      "I0228 01:50:04.775356 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (869264 virtual)\n",
      "I0228 01:50:04.786015 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (875186 virtual)\n",
      "I0228 01:50:04.794917 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (881086 virtual)\n",
      "I0228 01:50:04.797760 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (887257 virtual)\n",
      "I0228 01:50:04.827281 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (892626 virtual)\n",
      "I0228 01:50:04.829715 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (898312 virtual)\n",
      "I0228 01:50:04.851495 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (906019 virtual)\n",
      "I0228 01:50:04.854227 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (910667 virtual)\n",
      "I0228 01:50:04.858185 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (914539 virtual)\n",
      "I0228 01:50:04.884247 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (917376 virtual)\n",
      "I0228 01:50:04.887780 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (923664 virtual)\n",
      "I0228 01:50:04.904720 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (928053 virtual)\n",
      "I0228 01:50:04.916134 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (932034 virtual)\n",
      "I0228 01:50:04.917793 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (935941 virtual)\n",
      "I0228 01:50:04.934932 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (939941 virtual)\n",
      "I0228 01:50:04.943403 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (943590 virtual)\n",
      "I0228 01:50:04.957270 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (947127 virtual)\n",
      "I0228 01:50:04.964424 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (951083 virtual)\n",
      "I0228 01:50:04.970046 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (954671 virtual)\n",
      "I0228 01:50:04.979139 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (958751 virtual)\n",
      "I0228 01:50:05.000607 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (962790 virtual)\n",
      "I0228 01:50:05.003121 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (966588 virtual)\n",
      "I0228 01:50:05.010478 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (970086 virtual)\n",
      "I0228 01:50:05.013227 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (974117 virtual)\n",
      "I0228 01:50:05.014905 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (977906 virtual)\n",
      "I0228 01:50:05.032400 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (981735 virtual)\n",
      "I0228 01:50:05.036420 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (985467 virtual)\n",
      "I0228 01:50:05.049297 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (989361 virtual)\n",
      "I0228 01:50:05.052824 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (993380 virtual)\n",
      "I0228 01:50:05.055954 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (997424 virtual)\n",
      "I0228 01:50:05.070668 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (1001033 virtual)\n",
      "I0228 01:50:05.072278 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (1004853 virtual)\n",
      "I0228 01:50:05.083706 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1009456 virtual)\n",
      "I0228 01:50:05.090675 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1014726 virtual)\n",
      "I0228 01:50:05.100323 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1018588 virtual)\n",
      "I0228 01:50:05.104170 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1022165 virtual)\n",
      "I0228 01:50:05.105895 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1026367 virtual)\n",
      "I0228 01:50:05.119627 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1030191 virtual)\n",
      "I0228 01:50:05.126111 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1035183 virtual)\n",
      "I0228 01:50:05.135281 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1039131 virtual)\n",
      "I0228 01:50:05.145262 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1043437 virtual)\n",
      "I0228 01:50:05.150946 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1047367 virtual)\n",
      "I0228 01:50:05.162626 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1051448 virtual)\n",
      "I0228 01:50:05.168868 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1055247 virtual)\n",
      "I0228 01:50:05.170747 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1059086 virtual)\n",
      "I0228 01:50:05.181726 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1063948 virtual)\n",
      "I0228 01:50:05.198924 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1067749 virtual)\n",
      "I0228 01:50:05.201153 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1072345 virtual)\n",
      "I0228 01:50:05.204653 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1077214 virtual)\n",
      "I0228 01:50:05.215081 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1081549 virtual)\n",
      "I0228 01:50:05.220017 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1085400 virtual)\n",
      "I0228 01:50:05.234661 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1088892 virtual)\n",
      "I0228 01:50:05.241275 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1093383 virtual)\n",
      "I0228 01:50:05.246058 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1097253 virtual)\n",
      "I0228 01:50:05.258821 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1101841 virtual)\n",
      "I0228 01:50:05.265516 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1106107 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:05.270504 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1110887 virtual)\n",
      "I0228 01:50:05.286124 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1115055 virtual)\n",
      "I0228 01:50:05.296277 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1119680 virtual)\n",
      "I0228 01:50:05.299057 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1124821 virtual)\n",
      "I0228 01:50:05.303506 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1128964 virtual)\n",
      "I0228 01:50:05.305449 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1134655 virtual)\n",
      "I0228 01:50:05.326949 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1138669 virtual)\n",
      "I0228 01:50:05.331535 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1142535 virtual)\n",
      "I0228 01:50:05.346693 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1147111 virtual)\n",
      "I0228 01:50:05.348541 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1151444 virtual)\n",
      "I0228 01:50:05.351883 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1155379 virtual)\n",
      "I0228 01:50:05.368497 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1160261 virtual)\n",
      "I0228 01:50:05.375800 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1164743 virtual)\n",
      "I0228 01:50:05.397753 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1168611 virtual)\n",
      "I0228 01:50:05.399594 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1173745 virtual)\n",
      "I0228 01:50:05.408638 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1178581 virtual)\n",
      "I0228 01:50:05.411191 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1182820 virtual)\n",
      "I0228 01:50:05.412931 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1187075 virtual)\n",
      "I0228 01:50:05.441661 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1191705 virtual)\n",
      "I0228 01:50:05.443612 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1195653 virtual)\n",
      "I0228 01:50:05.449219 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1199829 virtual)\n",
      "I0228 01:50:05.453822 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1204178 virtual)\n",
      "I0228 01:50:05.457907 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1208684 virtual)\n",
      "I0228 01:50:05.477702 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1213412 virtual)\n",
      "I0228 01:50:05.486936 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1217060 virtual)\n",
      "I0228 01:50:05.491580 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1220365 virtual)\n",
      "I0228 01:50:05.496510 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1223734 virtual)\n",
      "I0228 01:50:05.501985 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1228349 virtual)\n",
      "I0228 01:50:05.517153 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1232590 virtual)\n",
      "I0228 01:50:05.519374 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1237754 virtual)\n",
      "I0228 01:50:05.527598 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1242248 virtual)\n",
      "I0228 01:50:05.538496 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1246518 virtual)\n",
      "I0228 01:50:05.548360 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1250967 virtual)\n",
      "I0228 01:50:05.551503 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1254837 virtual)\n",
      "I0228 01:50:05.555972 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1258855 virtual)\n",
      "I0228 01:50:05.562367 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1262835 virtual)\n",
      "I0228 01:50:05.568467 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1266769 virtual)\n",
      "I0228 01:50:05.586654 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1271417 virtual)\n",
      "I0228 01:50:05.596289 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1275405 virtual)\n",
      "I0228 01:50:05.606148 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1279803 virtual)\n",
      "I0228 01:50:05.608577 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1284522 virtual)\n",
      "I0228 01:50:05.621048 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1287851 virtual)\n",
      "I0228 01:50:05.625702 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1292237 virtual)\n",
      "I0228 01:50:05.640213 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1297173 virtual)\n",
      "I0228 01:50:05.645504 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1301369 virtual)\n",
      "I0228 01:50:05.648203 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1305168 virtual)\n",
      "I0228 01:50:05.658003 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1309546 virtual)\n",
      "I0228 01:50:05.670199 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1313231 virtual)\n",
      "I0228 01:50:05.684114 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1316938 virtual)\n",
      "I0228 01:50:05.685815 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1320983 virtual)\n",
      "I0228 01:50:05.687524 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1325163 virtual)\n",
      "I0228 01:50:05.695084 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1329658 virtual)\n",
      "I0228 01:50:05.707973 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1334116 virtual)\n",
      "I0228 01:50:05.718360 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1338393 virtual)\n",
      "I0228 01:50:05.722969 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1343662 virtual)\n",
      "I0228 01:50:05.734024 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1346862 virtual)\n",
      "I0228 01:50:05.737287 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1352300 virtual)\n",
      "I0228 01:50:05.741672 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1357529 virtual)\n",
      "I0228 01:50:05.752532 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1362603 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:05.758204 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1366891 virtual)\n",
      "I0228 01:50:05.775574 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1371339 virtual)\n",
      "I0228 01:50:05.781203 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1376271 virtual)\n",
      "I0228 01:50:05.787196 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1380416 virtual)\n",
      "I0228 01:50:05.792344 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1385658 virtual)\n",
      "I0228 01:50:05.807295 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1390560 virtual)\n",
      "I0228 01:50:05.808950 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1394678 virtual)\n",
      "I0228 01:50:05.831367 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1399369 virtual)\n",
      "I0228 01:50:05.837797 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1404425 virtual)\n",
      "I0228 01:50:05.844178 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1408572 virtual)\n",
      "I0228 01:50:05.846951 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1413508 virtual)\n",
      "I0228 01:50:05.872692 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1418059 virtual)\n",
      "I0228 01:50:05.874817 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1422803 virtual)\n",
      "I0228 01:50:05.878303 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1428559 virtual)\n",
      "I0228 01:50:05.899377 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1433647 virtual)\n",
      "I0228 01:50:05.902396 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1439027 virtual)\n",
      "I0228 01:50:05.913680 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1444517 virtual)\n",
      "I0228 01:50:05.917822 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1449868 virtual)\n",
      "I0228 01:50:05.924250 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1456835 virtual)\n",
      "I0228 01:50:05.946109 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1464466 virtual)\n",
      "I0228 01:50:05.948017 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1469968 virtual)\n",
      "I0228 01:50:05.955630 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1475221 virtual)\n",
      "I0228 01:50:05.959831 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1480513 virtual)\n",
      "I0228 01:50:05.980195 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1485842 virtual)\n",
      "I0228 01:50:05.996130 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1491289 virtual)\n",
      "I0228 01:50:06.002551 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1496269 virtual)\n",
      "I0228 01:50:06.004620 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1501420 virtual)\n",
      "I0228 01:50:06.007376 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1511494 virtual)\n",
      "I0228 01:50:06.039317 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1517146 virtual)\n",
      "I0228 01:50:06.050753 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1522337 virtual)\n",
      "I0228 01:50:06.052507 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1528210 virtual)\n",
      "I0228 01:50:06.056160 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1533091 virtual)\n",
      "I0228 01:50:06.070666 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1537502 virtual)\n",
      "I0228 01:50:06.088976 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1542477 virtual)\n",
      "I0228 01:50:06.097303 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1547724 virtual)\n",
      "I0228 01:50:06.102028 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1552548 virtual)\n",
      "I0228 01:50:06.103841 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1558038 virtual)\n",
      "I0228 01:50:06.140028 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1564212 virtual)\n",
      "I0228 01:50:06.148279 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1569854 virtual)\n",
      "I0228 01:50:06.150949 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1575473 virtual)\n",
      "I0228 01:50:06.152864 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1580989 virtual)\n",
      "I0228 01:50:06.175490 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1586965 virtual)\n",
      "I0228 01:50:06.181683 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1593184 virtual)\n",
      "I0228 01:50:06.192539 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1597908 virtual)\n",
      "I0228 01:50:06.198251 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1603150 virtual)\n",
      "I0228 01:50:06.202041 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1608132 virtual)\n",
      "I0228 01:50:06.233139 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1613527 virtual)\n",
      "I0228 01:50:06.241755 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1618751 virtual)\n",
      "I0228 01:50:06.250359 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1624425 virtual)\n",
      "I0228 01:50:06.254561 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1629492 virtual)\n",
      "I0228 01:50:06.256519 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1635306 virtual)\n",
      "I0228 01:50:06.294796 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1640222 virtual)\n",
      "I0228 01:50:06.296863 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1646601 virtual)\n",
      "I0228 01:50:06.299797 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1654770 virtual)\n",
      "I0228 01:50:06.304265 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1663407 virtual)\n",
      "I0228 01:50:06.307425 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1671535 virtual)\n",
      "I0228 01:50:06.343726 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1679460 virtual)\n",
      "I0228 01:50:06.345394 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1685369 virtual)\n",
      "I0228 01:50:06.352577 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1690299 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:06.360957 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1696798 virtual)\n",
      "I0228 01:50:06.362913 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1702560 virtual)\n",
      "I0228 01:50:06.389406 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1708473 virtual)\n",
      "I0228 01:50:06.412751 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1714551 virtual)\n",
      "I0228 01:50:06.431766 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1720193 virtual)\n",
      "I0228 01:50:06.440474 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1725182 virtual)\n",
      "I0228 01:50:06.453176 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1730856 virtual)\n",
      "I0228 01:50:06.467016 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1735878 virtual)\n",
      "I0228 01:50:06.472908 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1741137 virtual)\n",
      "I0228 01:50:06.479077 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1745837 virtual)\n",
      "I0228 01:50:06.504985 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1751337 virtual)\n",
      "I0228 01:50:06.513453 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1756808 virtual)\n",
      "I0228 01:50:06.518410 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1761961 virtual)\n",
      "I0228 01:50:06.532888 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1766978 virtual)\n",
      "I0228 01:50:06.534763 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1771881 virtual)\n",
      "I0228 01:50:06.552658 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1777196 virtual)\n",
      "I0228 01:50:06.567631 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1781949 virtual)\n",
      "I0228 01:50:06.579797 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1787341 virtual)\n",
      "I0228 01:50:06.582573 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1792810 virtual)\n",
      "I0228 01:50:06.585035 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1797835 virtual)\n",
      "I0228 01:50:06.608556 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1802940 virtual)\n",
      "I0228 01:50:06.621042 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1808046 virtual)\n",
      "I0228 01:50:06.632828 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1813022 virtual)\n",
      "I0228 01:50:06.634704 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1818246 virtual)\n",
      "I0228 01:50:06.641516 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1823360 virtual)\n",
      "I0228 01:50:06.662576 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1828788 virtual)\n",
      "I0228 01:50:06.668088 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1834039 virtual)\n",
      "I0228 01:50:06.684225 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1838753 virtual)\n",
      "I0228 01:50:06.686067 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1843732 virtual)\n",
      "I0228 01:50:06.697827 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1848508 virtual)\n",
      "I0228 01:50:06.714209 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1853773 virtual)\n",
      "I0228 01:50:06.725384 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1858691 virtual)\n",
      "I0228 01:50:06.734136 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1863833 virtual)\n",
      "I0228 01:50:06.736410 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1869254 virtual)\n",
      "I0228 01:50:06.753907 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1874443 virtual)\n",
      "I0228 01:50:06.769948 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1880049 virtual)\n",
      "I0228 01:50:06.778701 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1884749 virtual)\n",
      "I0228 01:50:06.783005 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1890104 virtual)\n",
      "I0228 01:50:06.789998 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1895285 virtual)\n",
      "I0228 01:50:06.800055 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1900245 virtual)\n",
      "I0228 01:50:06.827022 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1904883 virtual)\n",
      "I0228 01:50:06.828778 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1909748 virtual)\n",
      "I0228 01:50:06.843369 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1914915 virtual)\n",
      "I0228 01:50:06.845134 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1919972 virtual)\n",
      "I0228 01:50:06.854453 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1925163 virtual)\n",
      "I0228 01:50:06.872829 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1930091 virtual)\n",
      "I0228 01:50:06.892334 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1934928 virtual)\n",
      "I0228 01:50:06.894462 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1940513 virtual)\n",
      "I0228 01:50:06.905016 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1945583 virtual)\n",
      "I0228 01:50:06.908441 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1950814 virtual)\n",
      "I0228 01:50:06.918731 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1955604 virtual)\n",
      "I0228 01:50:06.941305 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1960948 virtual)\n",
      "I0228 01:50:06.946123 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1966549 virtual)\n",
      "I0228 01:50:06.959474 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1971661 virtual)\n",
      "I0228 01:50:06.963602 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1976972 virtual)\n",
      "I0228 01:50:06.967984 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1982423 virtual)\n",
      "I0228 01:50:06.988943 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1987514 virtual)\n",
      "I0228 01:50:07.002267 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1992974 virtual)\n",
      "I0228 01:50:07.011447 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1998204 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:07.017010 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (2003607 virtual)\n",
      "I0228 01:50:07.028521 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (2008731 virtual)\n",
      "I0228 01:50:07.042926 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (2013376 virtual)\n",
      "I0228 01:50:07.061117 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (2018515 virtual)\n",
      "I0228 01:50:07.063122 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (2023325 virtual)\n",
      "I0228 01:50:07.073505 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (2028035 virtual)\n",
      "I0228 01:50:07.092933 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (2032767 virtual)\n",
      "I0228 01:50:07.094700 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (2037662 virtual)\n",
      "I0228 01:50:07.114413 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2043124 virtual)\n",
      "I0228 01:50:07.118513 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2048075 virtual)\n",
      "I0228 01:50:07.127301 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2053343 virtual)\n",
      "I0228 01:50:07.141951 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2058671 virtual)\n",
      "I0228 01:50:07.167211 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2063964 virtual)\n",
      "I0228 01:50:07.169207 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2069249 virtual)\n",
      "I0228 01:50:07.175538 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2074686 virtual)\n",
      "I0228 01:50:07.184443 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2080066 virtual)\n",
      "I0228 01:50:07.192385 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2084604 virtual)\n",
      "I0228 01:50:07.215830 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2089540 virtual)\n",
      "I0228 01:50:07.223942 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2094383 virtual)\n",
      "I0228 01:50:07.225796 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2099409 virtual)\n",
      "I0228 01:50:07.242823 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2103980 virtual)\n",
      "I0228 01:50:07.245115 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2109369 virtual)\n",
      "I0228 01:50:07.266593 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2114771 virtual)\n",
      "I0228 01:50:07.282048 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2120264 virtual)\n",
      "I0228 01:50:07.285827 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2125259 virtual)\n",
      "I0228 01:50:07.289362 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2130457 virtual)\n",
      "I0228 01:50:07.309225 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2135774 virtual)\n",
      "I0228 01:50:07.318034 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2140601 virtual)\n",
      "I0228 01:50:07.331287 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2145885 virtual)\n",
      "I0228 01:50:07.337832 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2151070 virtual)\n",
      "I0228 01:50:07.339655 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2155929 virtual)\n",
      "I0228 01:50:07.373000 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2161217 virtual)\n",
      "I0228 01:50:07.387260 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2165957 virtual)\n",
      "I0228 01:50:07.389128 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2171273 virtual)\n",
      "I0228 01:50:07.391146 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2176658 virtual)\n",
      "I0228 01:50:07.393047 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2181737 virtual)\n",
      "I0228 01:50:07.435036 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2187265 virtual)\n",
      "I0228 01:50:07.437097 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2192391 virtual)\n",
      "I0228 01:50:07.442989 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2197779 virtual)\n",
      "I0228 01:50:07.444899 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2202523 virtual)\n",
      "I0228 01:50:07.446797 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2207429 virtual)\n",
      "I0228 01:50:07.486464 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2212783 virtual)\n",
      "I0228 01:50:07.492285 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2218036 virtual)\n",
      "I0228 01:50:07.495416 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2223110 virtual)\n",
      "I0228 01:50:07.498723 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2228124 virtual)\n",
      "I0228 01:50:07.527477 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2233272 virtual)\n",
      "I0228 01:50:07.548187 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2238354 virtual)\n",
      "I0228 01:50:07.549936 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2243283 virtual)\n",
      "I0228 01:50:07.551702 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2248215 virtual)\n",
      "I0228 01:50:07.553516 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2253190 virtual)\n",
      "I0228 01:50:07.577099 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2258298 virtual)\n",
      "I0228 01:50:07.599598 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2263193 virtual)\n",
      "I0228 01:50:07.601340 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2268239 virtual)\n",
      "I0228 01:50:07.603788 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2273702 virtual)\n",
      "I0228 01:50:07.609772 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2278691 virtual)\n",
      "I0228 01:50:07.625176 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2283819 virtual)\n",
      "I0228 01:50:07.650177 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2289025 virtual)\n",
      "I0228 01:50:07.652420 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2294306 virtual)\n",
      "I0228 01:50:07.654492 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2299354 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:07.667219 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2304628 virtual)\n",
      "I0228 01:50:07.676748 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2309584 virtual)\n",
      "I0228 01:50:07.701734 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2315045 virtual)\n",
      "I0228 01:50:07.708003 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2320490 virtual)\n",
      "I0228 01:50:07.711297 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2325657 virtual)\n",
      "I0228 01:50:07.725450 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2330703 virtual)\n",
      "I0228 01:50:07.727726 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2335557 virtual)\n",
      "I0228 01:50:07.753674 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2340491 virtual)\n",
      "I0228 01:50:07.759970 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2346088 virtual)\n",
      "I0228 01:50:07.761826 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2350992 virtual)\n",
      "I0228 01:50:07.779289 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2356005 virtual)\n",
      "I0228 01:50:07.787974 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2361747 virtual)\n",
      "I0228 01:50:07.804572 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2366560 virtual)\n",
      "I0228 01:50:07.811032 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2371676 virtual)\n",
      "I0228 01:50:07.821176 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2376609 virtual)\n",
      "I0228 01:50:07.829444 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2382061 virtual)\n",
      "I0228 01:50:07.844908 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2386808 virtual)\n",
      "I0228 01:50:07.853899 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2392005 virtual)\n",
      "I0228 01:50:07.868306 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2397361 virtual)\n",
      "I0228 01:50:07.872200 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2402696 virtual)\n",
      "I0228 01:50:07.878896 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2407802 virtual)\n",
      "I0228 01:50:07.902939 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2412797 virtual)\n",
      "I0228 01:50:07.915126 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2417885 virtual)\n",
      "I0228 01:50:07.919821 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2423060 virtual)\n",
      "I0228 01:50:07.929874 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2427936 virtual)\n",
      "I0228 01:50:07.932643 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2433036 virtual)\n",
      "I0228 01:50:07.956989 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2438251 virtual)\n",
      "I0228 01:50:07.971448 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2443596 virtual)\n",
      "I0228 01:50:07.976665 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2449142 virtual)\n",
      "I0228 01:50:07.983829 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2454320 virtual)\n",
      "I0228 01:50:07.987153 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2459414 virtual)\n",
      "I0228 01:50:08.007042 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2464378 virtual)\n",
      "I0228 01:50:08.027500 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2469339 virtual)\n",
      "I0228 01:50:08.030324 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2474266 virtual)\n",
      "I0228 01:50:08.032330 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2479591 virtual)\n",
      "I0228 01:50:08.042355 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2484941 virtual)\n",
      "I0228 01:50:08.058317 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2490285 virtual)\n",
      "I0228 01:50:08.080168 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2495274 virtual)\n",
      "I0228 01:50:08.087812 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2500419 virtual)\n",
      "I0228 01:50:08.092688 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2505773 virtual)\n",
      "I0228 01:50:08.094529 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2510790 virtual)\n",
      "I0228 01:50:08.105532 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2515655 virtual)\n",
      "I0228 01:50:08.131628 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2520789 virtual)\n",
      "I0228 01:50:08.136844 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2525862 virtual)\n",
      "I0228 01:50:08.148549 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2531141 virtual)\n",
      "I0228 01:50:08.156572 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2536268 virtual)\n",
      "I0228 01:50:08.158361 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2541256 virtual)\n",
      "I0228 01:50:08.179247 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2546726 virtual)\n",
      "I0228 01:50:08.192496 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2551630 virtual)\n",
      "I0228 01:50:08.203021 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2556725 virtual)\n",
      "I0228 01:50:08.204909 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2562102 virtual)\n",
      "I0228 01:50:08.213652 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2567611 virtual)\n",
      "I0228 01:50:08.231984 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2573077 virtual)\n",
      "I0228 01:50:08.241961 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2578388 virtual)\n",
      "I0228 01:50:08.257446 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2583646 virtual)\n",
      "I0228 01:50:08.265283 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2589098 virtual)\n",
      "I0228 01:50:08.273847 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2594444 virtual)\n",
      "I0228 01:50:08.291575 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2599089 virtual)\n",
      "I0228 01:50:08.293503 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2604328 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:08.309445 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2609468 virtual)\n",
      "I0228 01:50:08.319540 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2614282 virtual)\n",
      "I0228 01:50:08.338270 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2619278 virtual)\n",
      "I0228 01:50:08.340912 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2624373 virtual)\n",
      "I0228 01:50:08.347770 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2629429 virtual)\n",
      "I0228 01:50:08.366736 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2634797 virtual)\n",
      "I0228 01:50:08.373586 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2639806 virtual)\n",
      "I0228 01:50:08.389571 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2645504 virtual)\n",
      "I0228 01:50:08.400685 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2650643 virtual)\n",
      "I0228 01:50:08.404101 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2656206 virtual)\n",
      "I0228 01:50:08.418495 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2661277 virtual)\n",
      "I0228 01:50:08.424046 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2666732 virtual)\n",
      "I0228 01:50:08.440598 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2671878 virtual)\n",
      "I0228 01:50:08.450921 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2676862 virtual)\n",
      "I0228 01:50:08.462554 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2682109 virtual)\n",
      "I0228 01:50:08.469062 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2687431 virtual)\n",
      "I0228 01:50:08.473475 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2692911 virtual)\n",
      "I0228 01:50:08.497803 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2698537 virtual)\n",
      "I0228 01:50:08.505107 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2704122 virtual)\n",
      "I0228 01:50:08.520020 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2709232 virtual)\n",
      "I0228 01:50:08.525233 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2714212 virtual)\n",
      "I0228 01:50:08.527066 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2719326 virtual)\n",
      "I0228 01:50:08.551428 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2724449 virtual)\n",
      "I0228 01:50:08.558259 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2729736 virtual)\n",
      "I0228 01:50:08.571726 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2735215 virtual)\n",
      "I0228 01:50:08.580062 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2740299 virtual)\n",
      "I0228 01:50:08.586567 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2745125 virtual)\n",
      "I0228 01:50:08.608247 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2745161 virtual)\n",
      "I0228 01:50:08.661214 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:50:08.665453 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:50:08.664104 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:50:08.675487 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:50:08.687028 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:50:08.690250 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:50:08.694023 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:50:08.693250 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:50:08.705312 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:50:08.705368 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:50:09.110939 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:50:09.131628 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2745410 virtual documents\n",
      "I0228 01:50:09.219792 140370323834688 probability_estimation.py:155] using ParallelWordOccurrenceAccumulator(processes=5, batch_size=64) to estimate probabilities from sliding windows\n",
      "I0228 01:50:09.613472 140370323834688 text_analysis.py:506] 1 batches submitted to accumulate stats from 64 documents (5147 virtual)\n",
      "I0228 01:50:09.617924 140370323834688 text_analysis.py:506] 2 batches submitted to accumulate stats from 128 documents (10416 virtual)\n",
      "I0228 01:50:09.620548 140370323834688 text_analysis.py:506] 3 batches submitted to accumulate stats from 192 documents (16377 virtual)\n",
      "I0228 01:50:09.624577 140370323834688 text_analysis.py:506] 4 batches submitted to accumulate stats from 256 documents (21345 virtual)\n",
      "I0228 01:50:09.630614 140370323834688 text_analysis.py:506] 5 batches submitted to accumulate stats from 320 documents (27120 virtual)\n",
      "I0228 01:50:09.634380 140370323834688 text_analysis.py:506] 6 batches submitted to accumulate stats from 384 documents (32426 virtual)\n",
      "I0228 01:50:09.638047 140370323834688 text_analysis.py:506] 7 batches submitted to accumulate stats from 448 documents (38036 virtual)\n",
      "I0228 01:50:09.640335 140370323834688 text_analysis.py:506] 8 batches submitted to accumulate stats from 512 documents (43845 virtual)\n",
      "I0228 01:50:09.642363 140370323834688 text_analysis.py:506] 9 batches submitted to accumulate stats from 576 documents (49175 virtual)\n",
      "I0228 01:50:09.644443 140370323834688 text_analysis.py:506] 10 batches submitted to accumulate stats from 640 documents (54827 virtual)\n",
      "I0228 01:50:09.683232 140370323834688 text_analysis.py:506] 11 batches submitted to accumulate stats from 704 documents (60678 virtual)\n",
      "I0228 01:50:09.685239 140370323834688 text_analysis.py:506] 12 batches submitted to accumulate stats from 768 documents (66144 virtual)\n",
      "I0228 01:50:09.687183 140370323834688 text_analysis.py:506] 13 batches submitted to accumulate stats from 832 documents (71241 virtual)\n",
      "I0228 01:50:09.690067 140370323834688 text_analysis.py:506] 14 batches submitted to accumulate stats from 896 documents (76404 virtual)\n",
      "I0228 01:50:09.709758 140370323834688 text_analysis.py:506] 15 batches submitted to accumulate stats from 960 documents (82276 virtual)\n",
      "I0228 01:50:09.738697 140370323834688 text_analysis.py:506] 16 batches submitted to accumulate stats from 1024 documents (88346 virtual)\n",
      "I0228 01:50:09.740664 140370323834688 text_analysis.py:506] 17 batches submitted to accumulate stats from 1088 documents (94139 virtual)\n",
      "I0228 01:50:09.742573 140370323834688 text_analysis.py:506] 18 batches submitted to accumulate stats from 1152 documents (99686 virtual)\n",
      "I0228 01:50:09.748864 140370323834688 text_analysis.py:506] 19 batches submitted to accumulate stats from 1216 documents (105070 virtual)\n",
      "I0228 01:50:09.774937 140370323834688 text_analysis.py:506] 20 batches submitted to accumulate stats from 1280 documents (110565 virtual)\n",
      "I0228 01:50:09.794426 140370323834688 text_analysis.py:506] 21 batches submitted to accumulate stats from 1344 documents (115989 virtual)\n",
      "I0228 01:50:09.796324 140370323834688 text_analysis.py:506] 22 batches submitted to accumulate stats from 1408 documents (121164 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:09.798334 140370323834688 text_analysis.py:506] 23 batches submitted to accumulate stats from 1472 documents (126875 virtual)\n",
      "I0228 01:50:09.806297 140370323834688 text_analysis.py:506] 24 batches submitted to accumulate stats from 1536 documents (132724 virtual)\n",
      "I0228 01:50:09.843182 140370323834688 text_analysis.py:506] 25 batches submitted to accumulate stats from 1600 documents (138214 virtual)\n",
      "I0228 01:50:09.852740 140370323834688 text_analysis.py:506] 26 batches submitted to accumulate stats from 1664 documents (143466 virtual)\n",
      "I0228 01:50:09.854717 140370323834688 text_analysis.py:506] 27 batches submitted to accumulate stats from 1728 documents (149242 virtual)\n",
      "I0228 01:50:09.856786 140370323834688 text_analysis.py:506] 28 batches submitted to accumulate stats from 1792 documents (155443 virtual)\n",
      "I0228 01:50:09.866691 140370323834688 text_analysis.py:506] 29 batches submitted to accumulate stats from 1856 documents (161270 virtual)\n",
      "I0228 01:50:09.904644 140370323834688 text_analysis.py:506] 30 batches submitted to accumulate stats from 1920 documents (166677 virtual)\n",
      "I0228 01:50:09.906761 140370323834688 text_analysis.py:506] 31 batches submitted to accumulate stats from 1984 documents (172008 virtual)\n",
      "I0228 01:50:09.913308 140370323834688 text_analysis.py:506] 32 batches submitted to accumulate stats from 2048 documents (177472 virtual)\n",
      "I0228 01:50:09.923955 140370323834688 text_analysis.py:506] 33 batches submitted to accumulate stats from 2112 documents (182713 virtual)\n",
      "I0228 01:50:09.927921 140370323834688 text_analysis.py:506] 34 batches submitted to accumulate stats from 2176 documents (188282 virtual)\n",
      "I0228 01:50:09.959305 140370323834688 text_analysis.py:506] 35 batches submitted to accumulate stats from 2240 documents (193942 virtual)\n",
      "I0228 01:50:09.961215 140370323834688 text_analysis.py:506] 36 batches submitted to accumulate stats from 2304 documents (199446 virtual)\n",
      "I0228 01:50:09.976854 140370323834688 text_analysis.py:506] 37 batches submitted to accumulate stats from 2368 documents (205063 virtual)\n",
      "I0228 01:50:09.987971 140370323834688 text_analysis.py:506] 38 batches submitted to accumulate stats from 2432 documents (210539 virtual)\n",
      "I0228 01:50:09.989820 140370323834688 text_analysis.py:506] 39 batches submitted to accumulate stats from 2496 documents (216113 virtual)\n",
      "I0228 01:50:10.012087 140370323834688 text_analysis.py:506] 40 batches submitted to accumulate stats from 2560 documents (221775 virtual)\n",
      "I0228 01:50:10.017336 140370323834688 text_analysis.py:506] 41 batches submitted to accumulate stats from 2624 documents (229486 virtual)\n",
      "I0228 01:50:10.040618 140370323834688 text_analysis.py:506] 42 batches submitted to accumulate stats from 2688 documents (234904 virtual)\n",
      "I0228 01:50:10.046773 140370323834688 text_analysis.py:506] 43 batches submitted to accumulate stats from 2752 documents (240473 virtual)\n",
      "I0228 01:50:10.049675 140370323834688 text_analysis.py:506] 44 batches submitted to accumulate stats from 2816 documents (246064 virtual)\n",
      "I0228 01:50:10.067481 140370323834688 text_analysis.py:506] 45 batches submitted to accumulate stats from 2880 documents (251429 virtual)\n",
      "I0228 01:50:10.076099 140370323834688 text_analysis.py:506] 46 batches submitted to accumulate stats from 2944 documents (256742 virtual)\n",
      "I0228 01:50:10.106110 140370323834688 text_analysis.py:506] 47 batches submitted to accumulate stats from 3008 documents (261985 virtual)\n",
      "I0228 01:50:10.108022 140370323834688 text_analysis.py:506] 48 batches submitted to accumulate stats from 3072 documents (267581 virtual)\n",
      "I0228 01:50:10.110791 140370323834688 text_analysis.py:506] 49 batches submitted to accumulate stats from 3136 documents (273098 virtual)\n",
      "I0228 01:50:10.124531 140370323834688 text_analysis.py:506] 50 batches submitted to accumulate stats from 3200 documents (278406 virtual)\n",
      "I0228 01:50:10.143703 140370323834688 text_analysis.py:506] 51 batches submitted to accumulate stats from 3264 documents (284230 virtual)\n",
      "I0228 01:50:10.166382 140370323834688 text_analysis.py:506] 52 batches submitted to accumulate stats from 3328 documents (289398 virtual)\n",
      "I0228 01:50:10.169586 140370323834688 text_analysis.py:506] 53 batches submitted to accumulate stats from 3392 documents (294826 virtual)\n",
      "I0228 01:50:10.177338 140370323834688 text_analysis.py:506] 54 batches submitted to accumulate stats from 3456 documents (300438 virtual)\n",
      "I0228 01:50:10.179237 140370323834688 text_analysis.py:506] 55 batches submitted to accumulate stats from 3520 documents (305889 virtual)\n",
      "I0228 01:50:10.217357 140370323834688 text_analysis.py:506] 56 batches submitted to accumulate stats from 3584 documents (311283 virtual)\n",
      "I0228 01:50:10.221986 140370323834688 text_analysis.py:506] 57 batches submitted to accumulate stats from 3648 documents (316960 virtual)\n",
      "I0228 01:50:10.231975 140370323834688 text_analysis.py:506] 58 batches submitted to accumulate stats from 3712 documents (322437 virtual)\n",
      "I0228 01:50:10.235774 140370323834688 text_analysis.py:506] 59 batches submitted to accumulate stats from 3776 documents (328585 virtual)\n",
      "I0228 01:50:10.243619 140370323834688 text_analysis.py:506] 60 batches submitted to accumulate stats from 3840 documents (334640 virtual)\n",
      "I0228 01:50:10.273730 140370323834688 text_analysis.py:506] 61 batches submitted to accumulate stats from 3904 documents (340306 virtual)\n",
      "I0228 01:50:10.285902 140370323834688 text_analysis.py:506] 62 batches submitted to accumulate stats from 3968 documents (346052 virtual)\n",
      "I0228 01:50:10.287928 140370323834688 text_analysis.py:506] 63 batches submitted to accumulate stats from 4032 documents (351906 virtual)\n",
      "I0228 01:50:10.298672 140370323834688 text_analysis.py:506] 64 batches submitted to accumulate stats from 4096 documents (357949 virtual)\n",
      "I0228 01:50:10.308122 140370323834688 text_analysis.py:506] 65 batches submitted to accumulate stats from 4160 documents (363469 virtual)\n",
      "I0228 01:50:10.331195 140370323834688 text_analysis.py:506] 66 batches submitted to accumulate stats from 4224 documents (369287 virtual)\n",
      "I0228 01:50:10.345597 140370323834688 text_analysis.py:506] 67 batches submitted to accumulate stats from 4288 documents (375149 virtual)\n",
      "I0228 01:50:10.350089 140370323834688 text_analysis.py:506] 68 batches submitted to accumulate stats from 4352 documents (381002 virtual)\n",
      "I0228 01:50:10.368471 140370323834688 text_analysis.py:506] 69 batches submitted to accumulate stats from 4416 documents (386314 virtual)\n",
      "I0228 01:50:10.376712 140370323834688 text_analysis.py:506] 70 batches submitted to accumulate stats from 4480 documents (390914 virtual)\n",
      "I0228 01:50:10.387198 140370323834688 text_analysis.py:506] 71 batches submitted to accumulate stats from 4544 documents (395397 virtual)\n",
      "I0228 01:50:10.404628 140370323834688 text_analysis.py:506] 72 batches submitted to accumulate stats from 4608 documents (398896 virtual)\n",
      "I0228 01:50:10.407966 140370323834688 text_analysis.py:506] 73 batches submitted to accumulate stats from 4672 documents (402773 virtual)\n",
      "I0228 01:50:10.431738 140370323834688 text_analysis.py:506] 74 batches submitted to accumulate stats from 4736 documents (407483 virtual)\n",
      "I0228 01:50:10.439120 140370323834688 text_analysis.py:506] 75 batches submitted to accumulate stats from 4800 documents (411816 virtual)\n",
      "I0228 01:50:10.443128 140370323834688 text_analysis.py:506] 76 batches submitted to accumulate stats from 4864 documents (416296 virtual)\n",
      "I0228 01:50:10.467569 140370323834688 text_analysis.py:506] 77 batches submitted to accumulate stats from 4928 documents (420520 virtual)\n",
      "I0228 01:50:10.470547 140370323834688 text_analysis.py:506] 78 batches submitted to accumulate stats from 4992 documents (424931 virtual)\n",
      "I0228 01:50:10.487082 140370323834688 text_analysis.py:506] 79 batches submitted to accumulate stats from 5056 documents (429299 virtual)\n",
      "I0228 01:50:10.490194 140370323834688 text_analysis.py:506] 80 batches submitted to accumulate stats from 5120 documents (434202 virtual)\n",
      "I0228 01:50:10.506387 140370323834688 text_analysis.py:506] 81 batches submitted to accumulate stats from 5184 documents (439059 virtual)\n",
      "I0228 01:50:10.508270 140370323834688 text_analysis.py:506] 82 batches submitted to accumulate stats from 5248 documents (444024 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:10.520091 140370323834688 text_analysis.py:506] 83 batches submitted to accumulate stats from 5312 documents (448272 virtual)\n",
      "I0228 01:50:10.531949 140370323834688 text_analysis.py:506] 84 batches submitted to accumulate stats from 5376 documents (452141 virtual)\n",
      "I0228 01:50:10.541971 140370323834688 text_analysis.py:506] 85 batches submitted to accumulate stats from 5440 documents (456220 virtual)\n",
      "I0228 01:50:10.548809 140370323834688 text_analysis.py:506] 86 batches submitted to accumulate stats from 5504 documents (459450 virtual)\n",
      "I0228 01:50:10.550608 140370323834688 text_analysis.py:506] 87 batches submitted to accumulate stats from 5568 documents (463980 virtual)\n",
      "I0228 01:50:10.571296 140370323834688 text_analysis.py:506] 88 batches submitted to accumulate stats from 5632 documents (468660 virtual)\n",
      "I0228 01:50:10.576579 140370323834688 text_analysis.py:506] 89 batches submitted to accumulate stats from 5696 documents (473377 virtual)\n",
      "I0228 01:50:10.595741 140370323834688 text_analysis.py:506] 90 batches submitted to accumulate stats from 5760 documents (477876 virtual)\n",
      "I0228 01:50:10.597535 140370323834688 text_analysis.py:506] 91 batches submitted to accumulate stats from 5824 documents (482592 virtual)\n",
      "I0228 01:50:10.599284 140370323834688 text_analysis.py:506] 92 batches submitted to accumulate stats from 5888 documents (487337 virtual)\n",
      "I0228 01:50:10.613965 140370323834688 text_analysis.py:506] 93 batches submitted to accumulate stats from 5952 documents (492127 virtual)\n",
      "I0228 01:50:10.621953 140370323834688 text_analysis.py:506] 94 batches submitted to accumulate stats from 6016 documents (496571 virtual)\n",
      "I0228 01:50:10.632513 140370323834688 text_analysis.py:506] 95 batches submitted to accumulate stats from 6080 documents (500926 virtual)\n",
      "I0228 01:50:10.636933 140370323834688 text_analysis.py:506] 96 batches submitted to accumulate stats from 6144 documents (506209 virtual)\n",
      "I0228 01:50:10.640870 140370323834688 text_analysis.py:506] 97 batches submitted to accumulate stats from 6208 documents (510944 virtual)\n",
      "I0228 01:50:10.659051 140370323834688 text_analysis.py:506] 98 batches submitted to accumulate stats from 6272 documents (515605 virtual)\n",
      "I0228 01:50:10.675873 140370323834688 text_analysis.py:506] 99 batches submitted to accumulate stats from 6336 documents (519849 virtual)\n",
      "I0228 01:50:10.677766 140370323834688 text_analysis.py:506] 100 batches submitted to accumulate stats from 6400 documents (524512 virtual)\n",
      "I0228 01:50:10.698452 140370323834688 text_analysis.py:506] 101 batches submitted to accumulate stats from 6464 documents (528520 virtual)\n",
      "I0228 01:50:10.700970 140370323834688 text_analysis.py:506] 102 batches submitted to accumulate stats from 6528 documents (533173 virtual)\n",
      "I0228 01:50:10.705453 140370323834688 text_analysis.py:506] 103 batches submitted to accumulate stats from 6592 documents (536912 virtual)\n",
      "I0228 01:50:10.719306 140370323834688 text_analysis.py:506] 104 batches submitted to accumulate stats from 6656 documents (541511 virtual)\n",
      "I0228 01:50:10.729464 140370323834688 text_analysis.py:506] 105 batches submitted to accumulate stats from 6720 documents (546096 virtual)\n",
      "I0228 01:50:10.753242 140370323834688 text_analysis.py:506] 106 batches submitted to accumulate stats from 6784 documents (550393 virtual)\n",
      "I0228 01:50:10.755093 140370323834688 text_analysis.py:506] 107 batches submitted to accumulate stats from 6848 documents (555162 virtual)\n",
      "I0228 01:50:10.760874 140370323834688 text_analysis.py:506] 108 batches submitted to accumulate stats from 6912 documents (558995 virtual)\n",
      "I0228 01:50:10.762637 140370323834688 text_analysis.py:506] 109 batches submitted to accumulate stats from 6976 documents (563760 virtual)\n",
      "I0228 01:50:10.785807 140370323834688 text_analysis.py:506] 110 batches submitted to accumulate stats from 7040 documents (567801 virtual)\n",
      "I0228 01:50:10.794617 140370323834688 text_analysis.py:506] 111 batches submitted to accumulate stats from 7104 documents (571933 virtual)\n",
      "I0228 01:50:10.802385 140370323834688 text_analysis.py:506] 112 batches submitted to accumulate stats from 7168 documents (575772 virtual)\n",
      "I0228 01:50:10.810354 140370323834688 text_analysis.py:506] 113 batches submitted to accumulate stats from 7232 documents (578840 virtual)\n",
      "I0228 01:50:10.819878 140370323834688 text_analysis.py:506] 114 batches submitted to accumulate stats from 7296 documents (583535 virtual)\n",
      "I0228 01:50:10.834571 140370323834688 text_analysis.py:506] 115 batches submitted to accumulate stats from 7360 documents (588945 virtual)\n",
      "I0228 01:50:10.836587 140370323834688 text_analysis.py:506] 116 batches submitted to accumulate stats from 7424 documents (594411 virtual)\n",
      "I0228 01:50:10.855259 140370323834688 text_analysis.py:506] 117 batches submitted to accumulate stats from 7488 documents (599281 virtual)\n",
      "I0228 01:50:10.858058 140370323834688 text_analysis.py:506] 118 batches submitted to accumulate stats from 7552 documents (604741 virtual)\n",
      "I0228 01:50:10.869600 140370323834688 text_analysis.py:506] 119 batches submitted to accumulate stats from 7616 documents (610620 virtual)\n",
      "I0228 01:50:10.877768 140370323834688 text_analysis.py:506] 120 batches submitted to accumulate stats from 7680 documents (615767 virtual)\n",
      "I0228 01:50:10.884855 140370323834688 text_analysis.py:506] 121 batches submitted to accumulate stats from 7744 documents (620067 virtual)\n",
      "I0228 01:50:10.900636 140370323834688 text_analysis.py:506] 122 batches submitted to accumulate stats from 7808 documents (624865 virtual)\n",
      "I0228 01:50:10.903996 140370323834688 text_analysis.py:506] 123 batches submitted to accumulate stats from 7872 documents (628663 virtual)\n",
      "I0228 01:50:10.916514 140370323834688 text_analysis.py:506] 124 batches submitted to accumulate stats from 7936 documents (634161 virtual)\n",
      "I0228 01:50:10.934205 140370323834688 text_analysis.py:506] 125 batches submitted to accumulate stats from 8000 documents (639403 virtual)\n",
      "I0228 01:50:10.950699 140370323834688 text_analysis.py:506] 126 batches submitted to accumulate stats from 8064 documents (644635 virtual)\n",
      "I0228 01:50:10.958338 140370323834688 text_analysis.py:506] 127 batches submitted to accumulate stats from 8128 documents (650290 virtual)\n",
      "I0228 01:50:10.963542 140370323834688 text_analysis.py:506] 128 batches submitted to accumulate stats from 8192 documents (655264 virtual)\n",
      "I0228 01:50:10.978312 140370323834688 text_analysis.py:506] 129 batches submitted to accumulate stats from 8256 documents (659917 virtual)\n",
      "I0228 01:50:10.989224 140370323834688 text_analysis.py:506] 130 batches submitted to accumulate stats from 8320 documents (665153 virtual)\n",
      "I0228 01:50:11.001017 140370323834688 text_analysis.py:506] 131 batches submitted to accumulate stats from 8384 documents (670503 virtual)\n",
      "I0228 01:50:11.007045 140370323834688 text_analysis.py:506] 132 batches submitted to accumulate stats from 8448 documents (675595 virtual)\n",
      "I0228 01:50:11.013581 140370323834688 text_analysis.py:506] 133 batches submitted to accumulate stats from 8512 documents (680585 virtual)\n",
      "I0228 01:50:11.033122 140370323834688 text_analysis.py:506] 134 batches submitted to accumulate stats from 8576 documents (685705 virtual)\n",
      "I0228 01:50:11.039605 140370323834688 text_analysis.py:506] 135 batches submitted to accumulate stats from 8640 documents (690742 virtual)\n",
      "I0228 01:50:11.056271 140370323834688 text_analysis.py:506] 136 batches submitted to accumulate stats from 8704 documents (695883 virtual)\n",
      "I0228 01:50:11.064435 140370323834688 text_analysis.py:506] 137 batches submitted to accumulate stats from 8768 documents (701204 virtual)\n",
      "I0228 01:50:11.068108 140370323834688 text_analysis.py:506] 138 batches submitted to accumulate stats from 8832 documents (706339 virtual)\n",
      "I0228 01:50:11.075663 140370323834688 text_analysis.py:506] 139 batches submitted to accumulate stats from 8896 documents (711719 virtual)\n",
      "I0228 01:50:11.092556 140370323834688 text_analysis.py:506] 140 batches submitted to accumulate stats from 8960 documents (716768 virtual)\n",
      "I0228 01:50:11.115900 140370323834688 text_analysis.py:506] 141 batches submitted to accumulate stats from 9024 documents (722144 virtual)\n",
      "I0228 01:50:11.117807 140370323834688 text_analysis.py:506] 142 batches submitted to accumulate stats from 9088 documents (727436 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:11.125519 140370323834688 text_analysis.py:506] 143 batches submitted to accumulate stats from 9152 documents (733054 virtual)\n",
      "I0228 01:50:11.127389 140370323834688 text_analysis.py:506] 144 batches submitted to accumulate stats from 9216 documents (738504 virtual)\n",
      "I0228 01:50:11.142845 140370323834688 text_analysis.py:506] 145 batches submitted to accumulate stats from 9280 documents (743381 virtual)\n",
      "I0228 01:50:11.170335 140370323834688 text_analysis.py:506] 146 batches submitted to accumulate stats from 9344 documents (748915 virtual)\n",
      "I0228 01:50:11.175399 140370323834688 text_analysis.py:506] 147 batches submitted to accumulate stats from 9408 documents (753958 virtual)\n",
      "I0228 01:50:11.177337 140370323834688 text_analysis.py:506] 148 batches submitted to accumulate stats from 9472 documents (759491 virtual)\n",
      "I0228 01:50:11.188122 140370323834688 text_analysis.py:506] 149 batches submitted to accumulate stats from 9536 documents (765049 virtual)\n",
      "I0228 01:50:11.192686 140370323834688 text_analysis.py:506] 150 batches submitted to accumulate stats from 9600 documents (770926 virtual)\n",
      "I0228 01:50:11.220818 140370323834688 text_analysis.py:506] 151 batches submitted to accumulate stats from 9664 documents (776750 virtual)\n",
      "I0228 01:50:11.228075 140370323834688 text_analysis.py:506] 152 batches submitted to accumulate stats from 9728 documents (782701 virtual)\n",
      "I0228 01:50:11.232377 140370323834688 text_analysis.py:506] 153 batches submitted to accumulate stats from 9792 documents (788615 virtual)\n",
      "I0228 01:50:11.238350 140370323834688 text_analysis.py:506] 154 batches submitted to accumulate stats from 9856 documents (794166 virtual)\n",
      "I0228 01:50:11.245587 140370323834688 text_analysis.py:506] 155 batches submitted to accumulate stats from 9920 documents (799763 virtual)\n",
      "I0228 01:50:11.271703 140370323834688 text_analysis.py:506] 156 batches submitted to accumulate stats from 9984 documents (805460 virtual)\n",
      "I0228 01:50:11.275418 140370323834688 text_analysis.py:506] 157 batches submitted to accumulate stats from 10048 documents (811282 virtual)\n",
      "I0228 01:50:11.293199 140370323834688 text_analysis.py:506] 158 batches submitted to accumulate stats from 10112 documents (817112 virtual)\n",
      "I0228 01:50:11.305012 140370323834688 text_analysis.py:506] 159 batches submitted to accumulate stats from 10176 documents (823037 virtual)\n",
      "I0228 01:50:11.316140 140370323834688 text_analysis.py:506] 160 batches submitted to accumulate stats from 10240 documents (828423 virtual)\n",
      "I0228 01:50:11.335517 140370323834688 text_analysis.py:506] 161 batches submitted to accumulate stats from 10304 documents (833736 virtual)\n",
      "I0228 01:50:11.337382 140370323834688 text_analysis.py:506] 162 batches submitted to accumulate stats from 10368 documents (839273 virtual)\n",
      "I0228 01:50:11.356055 140370323834688 text_analysis.py:506] 163 batches submitted to accumulate stats from 10432 documents (845526 virtual)\n",
      "I0228 01:50:11.364547 140370323834688 text_analysis.py:506] 164 batches submitted to accumulate stats from 10496 documents (851482 virtual)\n",
      "I0228 01:50:11.370734 140370323834688 text_analysis.py:506] 165 batches submitted to accumulate stats from 10560 documents (857473 virtual)\n",
      "I0228 01:50:11.391284 140370323834688 text_analysis.py:506] 166 batches submitted to accumulate stats from 10624 documents (863312 virtual)\n",
      "I0228 01:50:11.394236 140370323834688 text_analysis.py:506] 167 batches submitted to accumulate stats from 10688 documents (869264 virtual)\n",
      "I0228 01:50:11.419457 140370323834688 text_analysis.py:506] 168 batches submitted to accumulate stats from 10752 documents (875186 virtual)\n",
      "I0228 01:50:11.426368 140370323834688 text_analysis.py:506] 169 batches submitted to accumulate stats from 10816 documents (881086 virtual)\n",
      "I0228 01:50:11.432543 140370323834688 text_analysis.py:506] 170 batches submitted to accumulate stats from 10880 documents (887257 virtual)\n",
      "I0228 01:50:11.445406 140370323834688 text_analysis.py:506] 171 batches submitted to accumulate stats from 10944 documents (892626 virtual)\n",
      "I0228 01:50:11.447533 140370323834688 text_analysis.py:506] 172 batches submitted to accumulate stats from 11008 documents (898312 virtual)\n",
      "I0228 01:50:11.483784 140370323834688 text_analysis.py:506] 173 batches submitted to accumulate stats from 11072 documents (906019 virtual)\n",
      "I0228 01:50:11.488147 140370323834688 text_analysis.py:506] 174 batches submitted to accumulate stats from 11136 documents (910667 virtual)\n",
      "I0228 01:50:11.496280 140370323834688 text_analysis.py:506] 175 batches submitted to accumulate stats from 11200 documents (914539 virtual)\n",
      "I0228 01:50:11.497796 140370323834688 text_analysis.py:506] 176 batches submitted to accumulate stats from 11264 documents (917376 virtual)\n",
      "I0228 01:50:11.504323 140370323834688 text_analysis.py:506] 177 batches submitted to accumulate stats from 11328 documents (923664 virtual)\n",
      "I0228 01:50:11.539565 140370323834688 text_analysis.py:506] 178 batches submitted to accumulate stats from 11392 documents (928053 virtual)\n",
      "I0228 01:50:11.546912 140370323834688 text_analysis.py:506] 179 batches submitted to accumulate stats from 11456 documents (932034 virtual)\n",
      "I0228 01:50:11.549656 140370323834688 text_analysis.py:506] 180 batches submitted to accumulate stats from 11520 documents (935941 virtual)\n",
      "I0228 01:50:11.556761 140370323834688 text_analysis.py:506] 181 batches submitted to accumulate stats from 11584 documents (939941 virtual)\n",
      "I0228 01:50:11.558828 140370323834688 text_analysis.py:506] 182 batches submitted to accumulate stats from 11648 documents (943590 virtual)\n",
      "I0228 01:50:11.586688 140370323834688 text_analysis.py:506] 183 batches submitted to accumulate stats from 11712 documents (947127 virtual)\n",
      "I0228 01:50:11.592153 140370323834688 text_analysis.py:506] 184 batches submitted to accumulate stats from 11776 documents (951083 virtual)\n",
      "I0228 01:50:11.596352 140370323834688 text_analysis.py:506] 185 batches submitted to accumulate stats from 11840 documents (954671 virtual)\n",
      "I0228 01:50:11.614881 140370323834688 text_analysis.py:506] 186 batches submitted to accumulate stats from 11904 documents (958751 virtual)\n",
      "I0228 01:50:11.624105 140370323834688 text_analysis.py:506] 187 batches submitted to accumulate stats from 11968 documents (962790 virtual)\n",
      "I0228 01:50:11.630724 140370323834688 text_analysis.py:506] 188 batches submitted to accumulate stats from 12032 documents (966588 virtual)\n",
      "I0228 01:50:11.632318 140370323834688 text_analysis.py:506] 189 batches submitted to accumulate stats from 12096 documents (970086 virtual)\n",
      "I0228 01:50:11.639936 140370323834688 text_analysis.py:506] 190 batches submitted to accumulate stats from 12160 documents (974117 virtual)\n",
      "I0228 01:50:11.650091 140370323834688 text_analysis.py:506] 191 batches submitted to accumulate stats from 12224 documents (977906 virtual)\n",
      "I0228 01:50:11.657756 140370323834688 text_analysis.py:506] 192 batches submitted to accumulate stats from 12288 documents (981735 virtual)\n",
      "I0228 01:50:11.663869 140370323834688 text_analysis.py:506] 193 batches submitted to accumulate stats from 12352 documents (985467 virtual)\n",
      "I0228 01:50:11.667775 140370323834688 text_analysis.py:506] 194 batches submitted to accumulate stats from 12416 documents (989361 virtual)\n",
      "I0228 01:50:11.681069 140370323834688 text_analysis.py:506] 195 batches submitted to accumulate stats from 12480 documents (993380 virtual)\n",
      "I0228 01:50:11.691568 140370323834688 text_analysis.py:506] 196 batches submitted to accumulate stats from 12544 documents (997424 virtual)\n",
      "I0228 01:50:11.697900 140370323834688 text_analysis.py:506] 197 batches submitted to accumulate stats from 12608 documents (1001033 virtual)\n",
      "I0228 01:50:11.699551 140370323834688 text_analysis.py:506] 198 batches submitted to accumulate stats from 12672 documents (1004853 virtual)\n",
      "I0228 01:50:11.701309 140370323834688 text_analysis.py:506] 199 batches submitted to accumulate stats from 12736 documents (1009456 virtual)\n",
      "I0228 01:50:11.727478 140370323834688 text_analysis.py:506] 200 batches submitted to accumulate stats from 12800 documents (1014726 virtual)\n",
      "I0228 01:50:11.729177 140370323834688 text_analysis.py:506] 201 batches submitted to accumulate stats from 12864 documents (1018588 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:11.732267 140370323834688 text_analysis.py:506] 202 batches submitted to accumulate stats from 12928 documents (1022165 virtual)\n",
      "I0228 01:50:11.734194 140370323834688 text_analysis.py:506] 203 batches submitted to accumulate stats from 12992 documents (1026367 virtual)\n",
      "I0228 01:50:11.756645 140370323834688 text_analysis.py:506] 204 batches submitted to accumulate stats from 13056 documents (1030191 virtual)\n",
      "I0228 01:50:11.762715 140370323834688 text_analysis.py:506] 205 batches submitted to accumulate stats from 13120 documents (1035183 virtual)\n",
      "I0228 01:50:11.764408 140370323834688 text_analysis.py:506] 206 batches submitted to accumulate stats from 13184 documents (1039131 virtual)\n",
      "I0228 01:50:11.770742 140370323834688 text_analysis.py:506] 207 batches submitted to accumulate stats from 13248 documents (1043437 virtual)\n",
      "I0228 01:50:11.779381 140370323834688 text_analysis.py:506] 208 batches submitted to accumulate stats from 13312 documents (1047367 virtual)\n",
      "I0228 01:50:11.796847 140370323834688 text_analysis.py:506] 209 batches submitted to accumulate stats from 13376 documents (1051448 virtual)\n",
      "I0228 01:50:11.799396 140370323834688 text_analysis.py:506] 210 batches submitted to accumulate stats from 13440 documents (1055247 virtual)\n",
      "I0228 01:50:11.804404 140370323834688 text_analysis.py:506] 211 batches submitted to accumulate stats from 13504 documents (1059086 virtual)\n",
      "I0228 01:50:11.806178 140370323834688 text_analysis.py:506] 212 batches submitted to accumulate stats from 13568 documents (1063948 virtual)\n",
      "I0228 01:50:11.827921 140370323834688 text_analysis.py:506] 213 batches submitted to accumulate stats from 13632 documents (1067749 virtual)\n",
      "I0228 01:50:11.832386 140370323834688 text_analysis.py:506] 214 batches submitted to accumulate stats from 13696 documents (1072345 virtual)\n",
      "I0228 01:50:11.839887 140370323834688 text_analysis.py:506] 215 batches submitted to accumulate stats from 13760 documents (1077214 virtual)\n",
      "I0228 01:50:11.843126 140370323834688 text_analysis.py:506] 216 batches submitted to accumulate stats from 13824 documents (1081549 virtual)\n",
      "I0228 01:50:11.846226 140370323834688 text_analysis.py:506] 217 batches submitted to accumulate stats from 13888 documents (1085400 virtual)\n",
      "I0228 01:50:11.866423 140370323834688 text_analysis.py:506] 218 batches submitted to accumulate stats from 13952 documents (1088892 virtual)\n",
      "I0228 01:50:11.872750 140370323834688 text_analysis.py:506] 219 batches submitted to accumulate stats from 14016 documents (1093383 virtual)\n",
      "I0228 01:50:11.876845 140370323834688 text_analysis.py:506] 220 batches submitted to accumulate stats from 14080 documents (1097253 virtual)\n",
      "I0228 01:50:11.878605 140370323834688 text_analysis.py:506] 221 batches submitted to accumulate stats from 14144 documents (1101841 virtual)\n",
      "I0228 01:50:11.892328 140370323834688 text_analysis.py:506] 222 batches submitted to accumulate stats from 14208 documents (1106107 virtual)\n",
      "I0228 01:50:11.901546 140370323834688 text_analysis.py:506] 223 batches submitted to accumulate stats from 14272 documents (1110887 virtual)\n",
      "I0228 01:50:11.913714 140370323834688 text_analysis.py:506] 224 batches submitted to accumulate stats from 14336 documents (1115055 virtual)\n",
      "I0228 01:50:11.924973 140370323834688 text_analysis.py:506] 225 batches submitted to accumulate stats from 14400 documents (1119680 virtual)\n",
      "I0228 01:50:11.930220 140370323834688 text_analysis.py:506] 226 batches submitted to accumulate stats from 14464 documents (1124821 virtual)\n",
      "I0228 01:50:11.931885 140370323834688 text_analysis.py:506] 227 batches submitted to accumulate stats from 14528 documents (1128964 virtual)\n",
      "I0228 01:50:11.933831 140370323834688 text_analysis.py:506] 228 batches submitted to accumulate stats from 14592 documents (1134655 virtual)\n",
      "I0228 01:50:11.954392 140370323834688 text_analysis.py:506] 229 batches submitted to accumulate stats from 14656 documents (1138669 virtual)\n",
      "I0228 01:50:11.960403 140370323834688 text_analysis.py:506] 230 batches submitted to accumulate stats from 14720 documents (1142535 virtual)\n",
      "I0228 01:50:11.974914 140370323834688 text_analysis.py:506] 231 batches submitted to accumulate stats from 14784 documents (1147111 virtual)\n",
      "I0228 01:50:11.980478 140370323834688 text_analysis.py:506] 232 batches submitted to accumulate stats from 14848 documents (1151444 virtual)\n",
      "I0228 01:50:11.995889 140370323834688 text_analysis.py:506] 233 batches submitted to accumulate stats from 14912 documents (1155379 virtual)\n",
      "I0228 01:50:11.998928 140370323834688 text_analysis.py:506] 234 batches submitted to accumulate stats from 14976 documents (1160261 virtual)\n",
      "I0228 01:50:12.004321 140370323834688 text_analysis.py:506] 235 batches submitted to accumulate stats from 15040 documents (1164743 virtual)\n",
      "I0228 01:50:12.024450 140370323834688 text_analysis.py:506] 236 batches submitted to accumulate stats from 15104 documents (1168611 virtual)\n",
      "I0228 01:50:12.031517 140370323834688 text_analysis.py:506] 237 batches submitted to accumulate stats from 15168 documents (1173745 virtual)\n",
      "I0228 01:50:12.039099 140370323834688 text_analysis.py:506] 238 batches submitted to accumulate stats from 15232 documents (1178581 virtual)\n",
      "I0228 01:50:12.040794 140370323834688 text_analysis.py:506] 239 batches submitted to accumulate stats from 15296 documents (1182820 virtual)\n",
      "I0228 01:50:12.061148 140370323834688 text_analysis.py:506] 240 batches submitted to accumulate stats from 15360 documents (1187075 virtual)\n",
      "I0228 01:50:12.067754 140370323834688 text_analysis.py:506] 241 batches submitted to accumulate stats from 15424 documents (1191705 virtual)\n",
      "I0228 01:50:12.073929 140370323834688 text_analysis.py:506] 242 batches submitted to accumulate stats from 15488 documents (1195653 virtual)\n",
      "I0228 01:50:12.081886 140370323834688 text_analysis.py:506] 243 batches submitted to accumulate stats from 15552 documents (1199829 virtual)\n",
      "I0228 01:50:12.086965 140370323834688 text_analysis.py:506] 244 batches submitted to accumulate stats from 15616 documents (1204178 virtual)\n",
      "I0228 01:50:12.103280 140370323834688 text_analysis.py:506] 245 batches submitted to accumulate stats from 15680 documents (1208684 virtual)\n",
      "I0228 01:50:12.109583 140370323834688 text_analysis.py:506] 246 batches submitted to accumulate stats from 15744 documents (1213412 virtual)\n",
      "I0228 01:50:12.117515 140370323834688 text_analysis.py:506] 247 batches submitted to accumulate stats from 15808 documents (1217060 virtual)\n",
      "I0228 01:50:12.125266 140370323834688 text_analysis.py:506] 248 batches submitted to accumulate stats from 15872 documents (1220365 virtual)\n",
      "I0228 01:50:12.133116 140370323834688 text_analysis.py:506] 249 batches submitted to accumulate stats from 15936 documents (1223734 virtual)\n",
      "I0228 01:50:12.141589 140370323834688 text_analysis.py:506] 250 batches submitted to accumulate stats from 16000 documents (1228349 virtual)\n",
      "I0228 01:50:12.149965 140370323834688 text_analysis.py:506] 251 batches submitted to accumulate stats from 16064 documents (1232590 virtual)\n",
      "I0228 01:50:12.154400 140370323834688 text_analysis.py:506] 252 batches submitted to accumulate stats from 16128 documents (1237754 virtual)\n",
      "I0228 01:50:12.161596 140370323834688 text_analysis.py:506] 253 batches submitted to accumulate stats from 16192 documents (1242248 virtual)\n",
      "I0228 01:50:12.181145 140370323834688 text_analysis.py:506] 254 batches submitted to accumulate stats from 16256 documents (1246518 virtual)\n",
      "I0228 01:50:12.185506 140370323834688 text_analysis.py:506] 255 batches submitted to accumulate stats from 16320 documents (1250967 virtual)\n",
      "I0228 01:50:12.189883 140370323834688 text_analysis.py:506] 256 batches submitted to accumulate stats from 16384 documents (1254837 virtual)\n",
      "I0228 01:50:12.193482 140370323834688 text_analysis.py:506] 257 batches submitted to accumulate stats from 16448 documents (1258855 virtual)\n",
      "I0228 01:50:12.195247 140370323834688 text_analysis.py:506] 258 batches submitted to accumulate stats from 16512 documents (1262835 virtual)\n",
      "I0228 01:50:12.211097 140370323834688 text_analysis.py:506] 259 batches submitted to accumulate stats from 16576 documents (1266769 virtual)\n",
      "I0228 01:50:12.225506 140370323834688 text_analysis.py:506] 260 batches submitted to accumulate stats from 16640 documents (1271417 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:12.233757 140370323834688 text_analysis.py:506] 261 batches submitted to accumulate stats from 16704 documents (1275405 virtual)\n",
      "I0228 01:50:12.238727 140370323834688 text_analysis.py:506] 262 batches submitted to accumulate stats from 16768 documents (1279803 virtual)\n",
      "I0228 01:50:12.243686 140370323834688 text_analysis.py:506] 263 batches submitted to accumulate stats from 16832 documents (1284522 virtual)\n",
      "I0228 01:50:12.253968 140370323834688 text_analysis.py:506] 264 batches submitted to accumulate stats from 16896 documents (1287851 virtual)\n",
      "I0228 01:50:12.265418 140370323834688 text_analysis.py:506] 265 batches submitted to accumulate stats from 16960 documents (1292237 virtual)\n",
      "I0228 01:50:12.276472 140370323834688 text_analysis.py:506] 266 batches submitted to accumulate stats from 17024 documents (1297173 virtual)\n",
      "I0228 01:50:12.280429 140370323834688 text_analysis.py:506] 267 batches submitted to accumulate stats from 17088 documents (1301369 virtual)\n",
      "I0228 01:50:12.288652 140370323834688 text_analysis.py:506] 268 batches submitted to accumulate stats from 17152 documents (1305168 virtual)\n",
      "I0228 01:50:12.299691 140370323834688 text_analysis.py:506] 269 batches submitted to accumulate stats from 17216 documents (1309546 virtual)\n",
      "I0228 01:50:12.310503 140370323834688 text_analysis.py:506] 270 batches submitted to accumulate stats from 17280 documents (1313231 virtual)\n",
      "I0228 01:50:12.314162 140370323834688 text_analysis.py:506] 271 batches submitted to accumulate stats from 17344 documents (1316938 virtual)\n",
      "I0228 01:50:12.326015 140370323834688 text_analysis.py:506] 272 batches submitted to accumulate stats from 17408 documents (1320983 virtual)\n",
      "I0228 01:50:12.327922 140370323834688 text_analysis.py:506] 273 batches submitted to accumulate stats from 17472 documents (1325163 virtual)\n",
      "I0228 01:50:12.339036 140370323834688 text_analysis.py:506] 274 batches submitted to accumulate stats from 17536 documents (1329658 virtual)\n",
      "I0228 01:50:12.349841 140370323834688 text_analysis.py:506] 275 batches submitted to accumulate stats from 17600 documents (1334116 virtual)\n",
      "I0228 01:50:12.360749 140370323834688 text_analysis.py:506] 276 batches submitted to accumulate stats from 17664 documents (1338393 virtual)\n",
      "I0228 01:50:12.362517 140370323834688 text_analysis.py:506] 277 batches submitted to accumulate stats from 17728 documents (1343662 virtual)\n",
      "I0228 01:50:12.366377 140370323834688 text_analysis.py:506] 278 batches submitted to accumulate stats from 17792 documents (1346862 virtual)\n",
      "I0228 01:50:12.382340 140370323834688 text_analysis.py:506] 279 batches submitted to accumulate stats from 17856 documents (1352300 virtual)\n",
      "I0228 01:50:12.384226 140370323834688 text_analysis.py:506] 280 batches submitted to accumulate stats from 17920 documents (1357529 virtual)\n",
      "I0228 01:50:12.395741 140370323834688 text_analysis.py:506] 281 batches submitted to accumulate stats from 17984 documents (1362603 virtual)\n",
      "I0228 01:50:12.403986 140370323834688 text_analysis.py:506] 282 batches submitted to accumulate stats from 18048 documents (1366891 virtual)\n",
      "I0228 01:50:12.410773 140370323834688 text_analysis.py:506] 283 batches submitted to accumulate stats from 18112 documents (1371339 virtual)\n",
      "I0228 01:50:12.423752 140370323834688 text_analysis.py:506] 284 batches submitted to accumulate stats from 18176 documents (1376271 virtual)\n",
      "I0228 01:50:12.431331 140370323834688 text_analysis.py:506] 285 batches submitted to accumulate stats from 18240 documents (1380416 virtual)\n",
      "I0228 01:50:12.433145 140370323834688 text_analysis.py:506] 286 batches submitted to accumulate stats from 18304 documents (1385658 virtual)\n",
      "I0228 01:50:12.444936 140370323834688 text_analysis.py:506] 287 batches submitted to accumulate stats from 18368 documents (1390560 virtual)\n",
      "I0228 01:50:12.461764 140370323834688 text_analysis.py:506] 288 batches submitted to accumulate stats from 18432 documents (1394678 virtual)\n",
      "I0228 01:50:12.474866 140370323834688 text_analysis.py:506] 289 batches submitted to accumulate stats from 18496 documents (1399369 virtual)\n",
      "I0228 01:50:12.481194 140370323834688 text_analysis.py:506] 290 batches submitted to accumulate stats from 18560 documents (1404425 virtual)\n",
      "I0228 01:50:12.482858 140370323834688 text_analysis.py:506] 291 batches submitted to accumulate stats from 18624 documents (1408572 virtual)\n",
      "I0228 01:50:12.484756 140370323834688 text_analysis.py:506] 292 batches submitted to accumulate stats from 18688 documents (1413508 virtual)\n",
      "I0228 01:50:12.512748 140370323834688 text_analysis.py:506] 293 batches submitted to accumulate stats from 18752 documents (1418059 virtual)\n",
      "I0228 01:50:12.521926 140370323834688 text_analysis.py:506] 294 batches submitted to accumulate stats from 18816 documents (1422803 virtual)\n",
      "I0228 01:50:12.523873 140370323834688 text_analysis.py:506] 295 batches submitted to accumulate stats from 18880 documents (1428559 virtual)\n",
      "I0228 01:50:12.528758 140370323834688 text_analysis.py:506] 296 batches submitted to accumulate stats from 18944 documents (1433647 virtual)\n",
      "I0228 01:50:12.533069 140370323834688 text_analysis.py:506] 297 batches submitted to accumulate stats from 19008 documents (1439027 virtual)\n",
      "I0228 01:50:12.565842 140370323834688 text_analysis.py:506] 298 batches submitted to accumulate stats from 19072 documents (1444517 virtual)\n",
      "I0228 01:50:12.574611 140370323834688 text_analysis.py:506] 299 batches submitted to accumulate stats from 19136 documents (1449868 virtual)\n",
      "I0228 01:50:12.576963 140370323834688 text_analysis.py:506] 300 batches submitted to accumulate stats from 19200 documents (1456835 virtual)\n",
      "I0228 01:50:12.579148 140370323834688 text_analysis.py:506] 301 batches submitted to accumulate stats from 19264 documents (1464466 virtual)\n",
      "I0228 01:50:12.581309 140370323834688 text_analysis.py:506] 302 batches submitted to accumulate stats from 19328 documents (1469968 virtual)\n",
      "I0228 01:50:12.607692 140370323834688 text_analysis.py:506] 303 batches submitted to accumulate stats from 19392 documents (1475221 virtual)\n",
      "I0228 01:50:12.620252 140370323834688 text_analysis.py:506] 304 batches submitted to accumulate stats from 19456 documents (1480513 virtual)\n",
      "I0228 01:50:12.625920 140370323834688 text_analysis.py:506] 305 batches submitted to accumulate stats from 19520 documents (1485842 virtual)\n",
      "I0228 01:50:12.633042 140370323834688 text_analysis.py:506] 306 batches submitted to accumulate stats from 19584 documents (1491289 virtual)\n",
      "I0228 01:50:12.635782 140370323834688 text_analysis.py:506] 307 batches submitted to accumulate stats from 19648 documents (1496269 virtual)\n",
      "I0228 01:50:12.657306 140370323834688 text_analysis.py:506] 308 batches submitted to accumulate stats from 19712 documents (1501420 virtual)\n",
      "I0228 01:50:12.664807 140370323834688 text_analysis.py:506] 309 batches submitted to accumulate stats from 19776 documents (1511494 virtual)\n",
      "I0228 01:50:12.683801 140370323834688 text_analysis.py:506] 310 batches submitted to accumulate stats from 19840 documents (1517146 virtual)\n",
      "I0228 01:50:12.696903 140370323834688 text_analysis.py:506] 311 batches submitted to accumulate stats from 19904 documents (1522337 virtual)\n",
      "I0228 01:50:12.698640 140370323834688 text_analysis.py:506] 312 batches submitted to accumulate stats from 19968 documents (1528210 virtual)\n",
      "I0228 01:50:12.704949 140370323834688 text_analysis.py:506] 313 batches submitted to accumulate stats from 20032 documents (1533091 virtual)\n",
      "I0228 01:50:12.715743 140370323834688 text_analysis.py:506] 314 batches submitted to accumulate stats from 20096 documents (1537502 virtual)\n",
      "I0228 01:50:12.733727 140370323834688 text_analysis.py:506] 315 batches submitted to accumulate stats from 20160 documents (1542477 virtual)\n",
      "I0228 01:50:12.743434 140370323834688 text_analysis.py:506] 316 batches submitted to accumulate stats from 20224 documents (1547724 virtual)\n",
      "I0228 01:50:12.750582 140370323834688 text_analysis.py:506] 317 batches submitted to accumulate stats from 20288 documents (1552548 virtual)\n",
      "I0228 01:50:12.756720 140370323834688 text_analysis.py:506] 318 batches submitted to accumulate stats from 20352 documents (1558038 virtual)\n",
      "I0228 01:50:12.790980 140370323834688 text_analysis.py:506] 319 batches submitted to accumulate stats from 20416 documents (1564212 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:12.794496 140370323834688 text_analysis.py:506] 320 batches submitted to accumulate stats from 20480 documents (1569854 virtual)\n",
      "I0228 01:50:12.802564 140370323834688 text_analysis.py:506] 321 batches submitted to accumulate stats from 20544 documents (1575473 virtual)\n",
      "I0228 01:50:12.805282 140370323834688 text_analysis.py:506] 322 batches submitted to accumulate stats from 20608 documents (1580989 virtual)\n",
      "I0228 01:50:12.807207 140370323834688 text_analysis.py:506] 323 batches submitted to accumulate stats from 20672 documents (1586965 virtual)\n",
      "I0228 01:50:12.834225 140370323834688 text_analysis.py:506] 324 batches submitted to accumulate stats from 20736 documents (1593184 virtual)\n",
      "I0228 01:50:12.841584 140370323834688 text_analysis.py:506] 325 batches submitted to accumulate stats from 20800 documents (1597908 virtual)\n",
      "I0228 01:50:12.852687 140370323834688 text_analysis.py:506] 326 batches submitted to accumulate stats from 20864 documents (1603150 virtual)\n",
      "I0228 01:50:12.855838 140370323834688 text_analysis.py:506] 327 batches submitted to accumulate stats from 20928 documents (1608132 virtual)\n",
      "I0228 01:50:12.857954 140370323834688 text_analysis.py:506] 328 batches submitted to accumulate stats from 20992 documents (1613527 virtual)\n",
      "I0228 01:50:12.894156 140370323834688 text_analysis.py:506] 329 batches submitted to accumulate stats from 21056 documents (1618751 virtual)\n",
      "I0228 01:50:12.896110 140370323834688 text_analysis.py:506] 330 batches submitted to accumulate stats from 21120 documents (1624425 virtual)\n",
      "I0228 01:50:12.906125 140370323834688 text_analysis.py:506] 331 batches submitted to accumulate stats from 21184 documents (1629492 virtual)\n",
      "I0228 01:50:12.908320 140370323834688 text_analysis.py:506] 332 batches submitted to accumulate stats from 21248 documents (1635306 virtual)\n",
      "I0228 01:50:12.927119 140370323834688 text_analysis.py:506] 333 batches submitted to accumulate stats from 21312 documents (1640222 virtual)\n",
      "I0228 01:50:12.940622 140370323834688 text_analysis.py:506] 334 batches submitted to accumulate stats from 21376 documents (1646601 virtual)\n",
      "I0228 01:50:12.946996 140370323834688 text_analysis.py:506] 335 batches submitted to accumulate stats from 21440 documents (1654770 virtual)\n",
      "I0228 01:50:12.955921 140370323834688 text_analysis.py:506] 336 batches submitted to accumulate stats from 21504 documents (1663407 virtual)\n",
      "I0228 01:50:12.958097 140370323834688 text_analysis.py:506] 337 batches submitted to accumulate stats from 21568 documents (1671535 virtual)\n",
      "I0228 01:50:12.985794 140370323834688 text_analysis.py:506] 338 batches submitted to accumulate stats from 21632 documents (1679460 virtual)\n",
      "I0228 01:50:12.990407 140370323834688 text_analysis.py:506] 339 batches submitted to accumulate stats from 21696 documents (1685369 virtual)\n",
      "I0228 01:50:12.997308 140370323834688 text_analysis.py:506] 340 batches submitted to accumulate stats from 21760 documents (1690299 virtual)\n",
      "I0228 01:50:13.003992 140370323834688 text_analysis.py:506] 341 batches submitted to accumulate stats from 21824 documents (1696798 virtual)\n",
      "I0228 01:50:13.010515 140370323834688 text_analysis.py:506] 342 batches submitted to accumulate stats from 21888 documents (1702560 virtual)\n",
      "I0228 01:50:13.039737 140370323834688 text_analysis.py:506] 343 batches submitted to accumulate stats from 21952 documents (1708473 virtual)\n",
      "I0228 01:50:13.057353 140370323834688 text_analysis.py:506] 344 batches submitted to accumulate stats from 22016 documents (1714551 virtual)\n",
      "I0228 01:50:13.076475 140370323834688 text_analysis.py:506] 345 batches submitted to accumulate stats from 22080 documents (1720193 virtual)\n",
      "I0228 01:50:13.086202 140370323834688 text_analysis.py:506] 346 batches submitted to accumulate stats from 22144 documents (1725182 virtual)\n",
      "I0228 01:50:13.088306 140370323834688 text_analysis.py:506] 347 batches submitted to accumulate stats from 22208 documents (1730856 virtual)\n",
      "I0228 01:50:13.117318 140370323834688 text_analysis.py:506] 348 batches submitted to accumulate stats from 22272 documents (1735878 virtual)\n",
      "I0228 01:50:13.123125 140370323834688 text_analysis.py:506] 349 batches submitted to accumulate stats from 22336 documents (1741137 virtual)\n",
      "I0228 01:50:13.131345 140370323834688 text_analysis.py:506] 350 batches submitted to accumulate stats from 22400 documents (1745837 virtual)\n",
      "I0228 01:50:13.140474 140370323834688 text_analysis.py:506] 351 batches submitted to accumulate stats from 22464 documents (1751337 virtual)\n",
      "I0228 01:50:13.148502 140370323834688 text_analysis.py:506] 352 batches submitted to accumulate stats from 22528 documents (1756808 virtual)\n",
      "I0228 01:50:13.167271 140370323834688 text_analysis.py:506] 353 batches submitted to accumulate stats from 22592 documents (1761961 virtual)\n",
      "I0228 01:50:13.182562 140370323834688 text_analysis.py:506] 354 batches submitted to accumulate stats from 22656 documents (1766978 virtual)\n",
      "I0228 01:50:13.187701 140370323834688 text_analysis.py:506] 355 batches submitted to accumulate stats from 22720 documents (1771881 virtual)\n",
      "I0228 01:50:13.196659 140370323834688 text_analysis.py:506] 356 batches submitted to accumulate stats from 22784 documents (1777196 virtual)\n",
      "I0228 01:50:13.204935 140370323834688 text_analysis.py:506] 357 batches submitted to accumulate stats from 22848 documents (1781949 virtual)\n",
      "I0228 01:50:13.219483 140370323834688 text_analysis.py:506] 358 batches submitted to accumulate stats from 22912 documents (1787341 virtual)\n",
      "I0228 01:50:13.234872 140370323834688 text_analysis.py:506] 359 batches submitted to accumulate stats from 22976 documents (1792810 virtual)\n",
      "I0228 01:50:13.236725 140370323834688 text_analysis.py:506] 360 batches submitted to accumulate stats from 23040 documents (1797835 virtual)\n",
      "I0228 01:50:13.258332 140370323834688 text_analysis.py:506] 361 batches submitted to accumulate stats from 23104 documents (1802940 virtual)\n",
      "I0228 01:50:13.262219 140370323834688 text_analysis.py:506] 362 batches submitted to accumulate stats from 23168 documents (1808046 virtual)\n",
      "I0228 01:50:13.273094 140370323834688 text_analysis.py:506] 363 batches submitted to accumulate stats from 23232 documents (1813022 virtual)\n",
      "I0228 01:50:13.288388 140370323834688 text_analysis.py:506] 364 batches submitted to accumulate stats from 23296 documents (1818246 virtual)\n",
      "I0228 01:50:13.311125 140370323834688 text_analysis.py:506] 365 batches submitted to accumulate stats from 23360 documents (1823360 virtual)\n",
      "I0228 01:50:13.313070 140370323834688 text_analysis.py:506] 366 batches submitted to accumulate stats from 23424 documents (1828788 virtual)\n",
      "I0228 01:50:13.319266 140370323834688 text_analysis.py:506] 367 batches submitted to accumulate stats from 23488 documents (1834039 virtual)\n",
      "I0228 01:50:13.325211 140370323834688 text_analysis.py:506] 368 batches submitted to accumulate stats from 23552 documents (1838753 virtual)\n",
      "I0228 01:50:13.341298 140370323834688 text_analysis.py:506] 369 batches submitted to accumulate stats from 23616 documents (1843732 virtual)\n",
      "I0228 01:50:13.358901 140370323834688 text_analysis.py:506] 370 batches submitted to accumulate stats from 23680 documents (1848508 virtual)\n",
      "I0228 01:50:13.363260 140370323834688 text_analysis.py:506] 371 batches submitted to accumulate stats from 23744 documents (1853773 virtual)\n",
      "I0228 01:50:13.375033 140370323834688 text_analysis.py:506] 372 batches submitted to accumulate stats from 23808 documents (1858691 virtual)\n",
      "I0228 01:50:13.383994 140370323834688 text_analysis.py:506] 373 batches submitted to accumulate stats from 23872 documents (1863833 virtual)\n",
      "I0228 01:50:13.391532 140370323834688 text_analysis.py:506] 374 batches submitted to accumulate stats from 23936 documents (1869254 virtual)\n",
      "I0228 01:50:13.408764 140370323834688 text_analysis.py:506] 375 batches submitted to accumulate stats from 24000 documents (1874443 virtual)\n",
      "I0228 01:50:13.419589 140370323834688 text_analysis.py:506] 376 batches submitted to accumulate stats from 24064 documents (1880049 virtual)\n",
      "I0228 01:50:13.428115 140370323834688 text_analysis.py:506] 377 batches submitted to accumulate stats from 24128 documents (1884749 virtual)\n",
      "I0228 01:50:13.438525 140370323834688 text_analysis.py:506] 378 batches submitted to accumulate stats from 24192 documents (1890104 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:13.443477 140370323834688 text_analysis.py:506] 379 batches submitted to accumulate stats from 24256 documents (1895285 virtual)\n",
      "I0228 01:50:13.451823 140370323834688 text_analysis.py:506] 380 batches submitted to accumulate stats from 24320 documents (1900245 virtual)\n",
      "I0228 01:50:13.476321 140370323834688 text_analysis.py:506] 381 batches submitted to accumulate stats from 24384 documents (1904883 virtual)\n",
      "I0228 01:50:13.478113 140370323834688 text_analysis.py:506] 382 batches submitted to accumulate stats from 24448 documents (1909748 virtual)\n",
      "I0228 01:50:13.491451 140370323834688 text_analysis.py:506] 383 batches submitted to accumulate stats from 24512 documents (1914915 virtual)\n",
      "I0228 01:50:13.503208 140370323834688 text_analysis.py:506] 384 batches submitted to accumulate stats from 24576 documents (1919972 virtual)\n",
      "I0228 01:50:13.507677 140370323834688 text_analysis.py:506] 385 batches submitted to accumulate stats from 24640 documents (1925163 virtual)\n",
      "I0228 01:50:13.521989 140370323834688 text_analysis.py:506] 386 batches submitted to accumulate stats from 24704 documents (1930091 virtual)\n",
      "I0228 01:50:13.535094 140370323834688 text_analysis.py:506] 387 batches submitted to accumulate stats from 24768 documents (1934928 virtual)\n",
      "I0228 01:50:13.556082 140370323834688 text_analysis.py:506] 388 batches submitted to accumulate stats from 24832 documents (1940513 virtual)\n",
      "I0228 01:50:13.562737 140370323834688 text_analysis.py:506] 389 batches submitted to accumulate stats from 24896 documents (1945583 virtual)\n",
      "I0228 01:50:13.567397 140370323834688 text_analysis.py:506] 390 batches submitted to accumulate stats from 24960 documents (1950814 virtual)\n",
      "I0228 01:50:13.570132 140370323834688 text_analysis.py:506] 391 batches submitted to accumulate stats from 25024 documents (1955604 virtual)\n",
      "I0228 01:50:13.583405 140370323834688 text_analysis.py:506] 392 batches submitted to accumulate stats from 25088 documents (1960948 virtual)\n",
      "I0228 01:50:13.608283 140370323834688 text_analysis.py:506] 393 batches submitted to accumulate stats from 25152 documents (1966549 virtual)\n",
      "I0228 01:50:13.610469 140370323834688 text_analysis.py:506] 394 batches submitted to accumulate stats from 25216 documents (1971661 virtual)\n",
      "I0228 01:50:13.619949 140370323834688 text_analysis.py:506] 395 batches submitted to accumulate stats from 25280 documents (1976972 virtual)\n",
      "I0228 01:50:13.628268 140370323834688 text_analysis.py:506] 396 batches submitted to accumulate stats from 25344 documents (1982423 virtual)\n",
      "I0228 01:50:13.631188 140370323834688 text_analysis.py:506] 397 batches submitted to accumulate stats from 25408 documents (1987514 virtual)\n",
      "I0228 01:50:13.661372 140370323834688 text_analysis.py:506] 398 batches submitted to accumulate stats from 25472 documents (1992974 virtual)\n",
      "I0228 01:50:13.664685 140370323834688 text_analysis.py:506] 399 batches submitted to accumulate stats from 25536 documents (1998204 virtual)\n",
      "I0228 01:50:13.674258 140370323834688 text_analysis.py:506] 400 batches submitted to accumulate stats from 25600 documents (2003607 virtual)\n",
      "I0228 01:50:13.688482 140370323834688 text_analysis.py:506] 401 batches submitted to accumulate stats from 25664 documents (2008731 virtual)\n",
      "I0228 01:50:13.693591 140370323834688 text_analysis.py:506] 402 batches submitted to accumulate stats from 25728 documents (2013376 virtual)\n",
      "I0228 01:50:13.715698 140370323834688 text_analysis.py:506] 403 batches submitted to accumulate stats from 25792 documents (2018515 virtual)\n",
      "I0228 01:50:13.719697 140370323834688 text_analysis.py:506] 404 batches submitted to accumulate stats from 25856 documents (2023325 virtual)\n",
      "I0228 01:50:13.730030 140370323834688 text_analysis.py:506] 405 batches submitted to accumulate stats from 25920 documents (2028035 virtual)\n",
      "I0228 01:50:13.745523 140370323834688 text_analysis.py:506] 406 batches submitted to accumulate stats from 25984 documents (2032767 virtual)\n",
      "I0228 01:50:13.754973 140370323834688 text_analysis.py:506] 407 batches submitted to accumulate stats from 26048 documents (2037662 virtual)\n",
      "I0228 01:50:13.770477 140370323834688 text_analysis.py:506] 408 batches submitted to accumulate stats from 26112 documents (2043124 virtual)\n",
      "I0228 01:50:13.772976 140370323834688 text_analysis.py:506] 409 batches submitted to accumulate stats from 26176 documents (2048075 virtual)\n",
      "I0228 01:50:13.783751 140370323834688 text_analysis.py:506] 410 batches submitted to accumulate stats from 26240 documents (2053343 virtual)\n",
      "I0228 01:50:13.798609 140370323834688 text_analysis.py:506] 411 batches submitted to accumulate stats from 26304 documents (2058671 virtual)\n",
      "I0228 01:50:13.811702 140370323834688 text_analysis.py:506] 412 batches submitted to accumulate stats from 26368 documents (2063964 virtual)\n",
      "I0228 01:50:13.822708 140370323834688 text_analysis.py:506] 413 batches submitted to accumulate stats from 26432 documents (2069249 virtual)\n",
      "I0228 01:50:13.824892 140370323834688 text_analysis.py:506] 414 batches submitted to accumulate stats from 26496 documents (2074686 virtual)\n",
      "I0228 01:50:13.832870 140370323834688 text_analysis.py:506] 415 batches submitted to accumulate stats from 26560 documents (2080066 virtual)\n",
      "I0228 01:50:13.845677 140370323834688 text_analysis.py:506] 416 batches submitted to accumulate stats from 26624 documents (2084604 virtual)\n",
      "I0228 01:50:13.870288 140370323834688 text_analysis.py:506] 417 batches submitted to accumulate stats from 26688 documents (2089540 virtual)\n",
      "I0228 01:50:13.873715 140370323834688 text_analysis.py:506] 418 batches submitted to accumulate stats from 26752 documents (2094383 virtual)\n",
      "I0228 01:50:13.878692 140370323834688 text_analysis.py:506] 419 batches submitted to accumulate stats from 26816 documents (2099409 virtual)\n",
      "I0228 01:50:13.883566 140370323834688 text_analysis.py:506] 420 batches submitted to accumulate stats from 26880 documents (2103980 virtual)\n",
      "I0228 01:50:13.898364 140370323834688 text_analysis.py:506] 421 batches submitted to accumulate stats from 26944 documents (2109369 virtual)\n",
      "I0228 01:50:13.925128 140370323834688 text_analysis.py:506] 422 batches submitted to accumulate stats from 27008 documents (2114771 virtual)\n",
      "I0228 01:50:13.933885 140370323834688 text_analysis.py:506] 423 batches submitted to accumulate stats from 27072 documents (2120264 virtual)\n",
      "I0228 01:50:13.938639 140370323834688 text_analysis.py:506] 424 batches submitted to accumulate stats from 27136 documents (2125259 virtual)\n",
      "I0228 01:50:13.941323 140370323834688 text_analysis.py:506] 425 batches submitted to accumulate stats from 27200 documents (2130457 virtual)\n",
      "I0228 01:50:13.943178 140370323834688 text_analysis.py:506] 426 batches submitted to accumulate stats from 27264 documents (2135774 virtual)\n",
      "I0228 01:50:13.975028 140370323834688 text_analysis.py:506] 427 batches submitted to accumulate stats from 27328 documents (2140601 virtual)\n",
      "I0228 01:50:13.981841 140370323834688 text_analysis.py:506] 428 batches submitted to accumulate stats from 27392 documents (2145885 virtual)\n",
      "I0228 01:50:13.992493 140370323834688 text_analysis.py:506] 429 batches submitted to accumulate stats from 27456 documents (2151070 virtual)\n",
      "I0228 01:50:13.997884 140370323834688 text_analysis.py:506] 430 batches submitted to accumulate stats from 27520 documents (2155929 virtual)\n",
      "I0228 01:50:14.000590 140370323834688 text_analysis.py:506] 431 batches submitted to accumulate stats from 27584 documents (2161217 virtual)\n",
      "I0228 01:50:14.029847 140370323834688 text_analysis.py:506] 432 batches submitted to accumulate stats from 27648 documents (2165957 virtual)\n",
      "I0228 01:50:14.037389 140370323834688 text_analysis.py:506] 433 batches submitted to accumulate stats from 27712 documents (2171273 virtual)\n",
      "I0228 01:50:14.043039 140370323834688 text_analysis.py:506] 434 batches submitted to accumulate stats from 27776 documents (2176658 virtual)\n",
      "I0228 01:50:14.047994 140370323834688 text_analysis.py:506] 435 batches submitted to accumulate stats from 27840 documents (2181737 virtual)\n",
      "I0228 01:50:14.064665 140370323834688 text_analysis.py:506] 436 batches submitted to accumulate stats from 27904 documents (2187265 virtual)\n",
      "I0228 01:50:14.077662 140370323834688 text_analysis.py:506] 437 batches submitted to accumulate stats from 27968 documents (2192391 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:14.092110 140370323834688 text_analysis.py:506] 438 batches submitted to accumulate stats from 28032 documents (2197779 virtual)\n",
      "I0228 01:50:14.097621 140370323834688 text_analysis.py:506] 439 batches submitted to accumulate stats from 28096 documents (2202523 virtual)\n",
      "I0228 01:50:14.099449 140370323834688 text_analysis.py:506] 440 batches submitted to accumulate stats from 28160 documents (2207429 virtual)\n",
      "I0228 01:50:14.123247 140370323834688 text_analysis.py:506] 441 batches submitted to accumulate stats from 28224 documents (2212783 virtual)\n",
      "I0228 01:50:14.125100 140370323834688 text_analysis.py:506] 442 batches submitted to accumulate stats from 28288 documents (2218036 virtual)\n",
      "I0228 01:50:14.146966 140370323834688 text_analysis.py:506] 443 batches submitted to accumulate stats from 28352 documents (2223110 virtual)\n",
      "I0228 01:50:14.148767 140370323834688 text_analysis.py:506] 444 batches submitted to accumulate stats from 28416 documents (2228124 virtual)\n",
      "I0228 01:50:14.150578 140370323834688 text_analysis.py:506] 445 batches submitted to accumulate stats from 28480 documents (2233272 virtual)\n",
      "I0228 01:50:14.175121 140370323834688 text_analysis.py:506] 446 batches submitted to accumulate stats from 28544 documents (2238354 virtual)\n",
      "I0228 01:50:14.190935 140370323834688 text_analysis.py:506] 447 batches submitted to accumulate stats from 28608 documents (2243283 virtual)\n",
      "I0228 01:50:14.198831 140370323834688 text_analysis.py:506] 448 batches submitted to accumulate stats from 28672 documents (2248215 virtual)\n",
      "I0228 01:50:14.201149 140370323834688 text_analysis.py:506] 449 batches submitted to accumulate stats from 28736 documents (2253190 virtual)\n",
      "I0228 01:50:14.202984 140370323834688 text_analysis.py:506] 450 batches submitted to accumulate stats from 28800 documents (2258298 virtual)\n",
      "I0228 01:50:14.226858 140370323834688 text_analysis.py:506] 451 batches submitted to accumulate stats from 28864 documents (2263193 virtual)\n",
      "I0228 01:50:14.250299 140370323834688 text_analysis.py:506] 452 batches submitted to accumulate stats from 28928 documents (2268239 virtual)\n",
      "I0228 01:50:14.252212 140370323834688 text_analysis.py:506] 453 batches submitted to accumulate stats from 28992 documents (2273702 virtual)\n",
      "I0228 01:50:14.254044 140370323834688 text_analysis.py:506] 454 batches submitted to accumulate stats from 29056 documents (2278691 virtual)\n",
      "I0228 01:50:14.258152 140370323834688 text_analysis.py:506] 455 batches submitted to accumulate stats from 29120 documents (2283819 virtual)\n",
      "I0228 01:50:14.280314 140370323834688 text_analysis.py:506] 456 batches submitted to accumulate stats from 29184 documents (2289025 virtual)\n",
      "I0228 01:50:14.300240 140370323834688 text_analysis.py:506] 457 batches submitted to accumulate stats from 29248 documents (2294306 virtual)\n",
      "I0228 01:50:14.302379 140370323834688 text_analysis.py:506] 458 batches submitted to accumulate stats from 29312 documents (2299354 virtual)\n",
      "I0228 01:50:14.309073 140370323834688 text_analysis.py:506] 459 batches submitted to accumulate stats from 29376 documents (2304628 virtual)\n",
      "I0228 01:50:14.320549 140370323834688 text_analysis.py:506] 460 batches submitted to accumulate stats from 29440 documents (2309584 virtual)\n",
      "I0228 01:50:14.332166 140370323834688 text_analysis.py:506] 461 batches submitted to accumulate stats from 29504 documents (2315045 virtual)\n",
      "I0228 01:50:14.351986 140370323834688 text_analysis.py:506] 462 batches submitted to accumulate stats from 29568 documents (2320490 virtual)\n",
      "I0228 01:50:14.358360 140370323834688 text_analysis.py:506] 463 batches submitted to accumulate stats from 29632 documents (2325657 virtual)\n",
      "I0228 01:50:14.360175 140370323834688 text_analysis.py:506] 464 batches submitted to accumulate stats from 29696 documents (2330703 virtual)\n",
      "I0228 01:50:14.378883 140370323834688 text_analysis.py:506] 465 batches submitted to accumulate stats from 29760 documents (2335557 virtual)\n",
      "I0228 01:50:14.384118 140370323834688 text_analysis.py:506] 466 batches submitted to accumulate stats from 29824 documents (2340491 virtual)\n",
      "I0228 01:50:14.403683 140370323834688 text_analysis.py:506] 467 batches submitted to accumulate stats from 29888 documents (2346088 virtual)\n",
      "I0228 01:50:14.406607 140370323834688 text_analysis.py:506] 468 batches submitted to accumulate stats from 29952 documents (2350992 virtual)\n",
      "I0228 01:50:14.414901 140370323834688 text_analysis.py:506] 469 batches submitted to accumulate stats from 30016 documents (2356005 virtual)\n",
      "I0228 01:50:14.435073 140370323834688 text_analysis.py:506] 470 batches submitted to accumulate stats from 30080 documents (2361747 virtual)\n",
      "I0228 01:50:14.441774 140370323834688 text_analysis.py:506] 471 batches submitted to accumulate stats from 30144 documents (2366560 virtual)\n",
      "I0228 01:50:14.456866 140370323834688 text_analysis.py:506] 472 batches submitted to accumulate stats from 30208 documents (2371676 virtual)\n",
      "I0228 01:50:14.461548 140370323834688 text_analysis.py:506] 473 batches submitted to accumulate stats from 30272 documents (2376609 virtual)\n",
      "I0228 01:50:14.466317 140370323834688 text_analysis.py:506] 474 batches submitted to accumulate stats from 30336 documents (2382061 virtual)\n",
      "I0228 01:50:14.483129 140370323834688 text_analysis.py:506] 475 batches submitted to accumulate stats from 30400 documents (2386808 virtual)\n",
      "I0228 01:50:14.500704 140370323834688 text_analysis.py:506] 476 batches submitted to accumulate stats from 30464 documents (2392005 virtual)\n",
      "I0228 01:50:14.512606 140370323834688 text_analysis.py:506] 477 batches submitted to accumulate stats from 30528 documents (2397361 virtual)\n",
      "I0228 01:50:14.514477 140370323834688 text_analysis.py:506] 478 batches submitted to accumulate stats from 30592 documents (2402696 virtual)\n",
      "I0228 01:50:14.516991 140370323834688 text_analysis.py:506] 479 batches submitted to accumulate stats from 30656 documents (2407802 virtual)\n",
      "I0228 01:50:14.543059 140370323834688 text_analysis.py:506] 480 batches submitted to accumulate stats from 30720 documents (2412797 virtual)\n",
      "I0228 01:50:14.559541 140370323834688 text_analysis.py:506] 481 batches submitted to accumulate stats from 30784 documents (2417885 virtual)\n",
      "I0228 01:50:14.563220 140370323834688 text_analysis.py:506] 482 batches submitted to accumulate stats from 30848 documents (2423060 virtual)\n",
      "I0228 01:50:14.565004 140370323834688 text_analysis.py:506] 483 batches submitted to accumulate stats from 30912 documents (2427936 virtual)\n",
      "I0228 01:50:14.571680 140370323834688 text_analysis.py:506] 484 batches submitted to accumulate stats from 30976 documents (2433036 virtual)\n",
      "I0228 01:50:14.590607 140370323834688 text_analysis.py:506] 485 batches submitted to accumulate stats from 31040 documents (2438251 virtual)\n",
      "I0228 01:50:14.618382 140370323834688 text_analysis.py:506] 486 batches submitted to accumulate stats from 31104 documents (2443596 virtual)\n",
      "I0228 01:50:14.620321 140370323834688 text_analysis.py:506] 487 batches submitted to accumulate stats from 31168 documents (2449142 virtual)\n",
      "I0228 01:50:14.622692 140370323834688 text_analysis.py:506] 488 batches submitted to accumulate stats from 31232 documents (2454320 virtual)\n",
      "I0228 01:50:14.627334 140370323834688 text_analysis.py:506] 489 batches submitted to accumulate stats from 31296 documents (2459414 virtual)\n",
      "I0228 01:50:14.640830 140370323834688 text_analysis.py:506] 490 batches submitted to accumulate stats from 31360 documents (2464378 virtual)\n",
      "I0228 01:50:14.669734 140370323834688 text_analysis.py:506] 491 batches submitted to accumulate stats from 31424 documents (2469339 virtual)\n",
      "I0228 01:50:14.671534 140370323834688 text_analysis.py:506] 492 batches submitted to accumulate stats from 31488 documents (2474266 virtual)\n",
      "I0228 01:50:14.673169 140370323834688 text_analysis.py:506] 493 batches submitted to accumulate stats from 31552 documents (2479591 virtual)\n",
      "I0228 01:50:14.688173 140370323834688 text_analysis.py:506] 494 batches submitted to accumulate stats from 31616 documents (2484941 virtual)\n",
      "I0228 01:50:14.691658 140370323834688 text_analysis.py:506] 495 batches submitted to accumulate stats from 31680 documents (2490285 virtual)\n",
      "I0228 01:50:14.723020 140370323834688 text_analysis.py:506] 496 batches submitted to accumulate stats from 31744 documents (2495274 virtual)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:50:14.726654 140370323834688 text_analysis.py:506] 497 batches submitted to accumulate stats from 31808 documents (2500419 virtual)\n",
      "I0228 01:50:14.728725 140370323834688 text_analysis.py:506] 498 batches submitted to accumulate stats from 31872 documents (2505773 virtual)\n",
      "I0228 01:50:14.741453 140370323834688 text_analysis.py:506] 499 batches submitted to accumulate stats from 31936 documents (2510790 virtual)\n",
      "I0228 01:50:14.745700 140370323834688 text_analysis.py:506] 500 batches submitted to accumulate stats from 32000 documents (2515655 virtual)\n",
      "I0228 01:50:14.769877 140370323834688 text_analysis.py:506] 501 batches submitted to accumulate stats from 32064 documents (2520789 virtual)\n",
      "I0228 01:50:14.776424 140370323834688 text_analysis.py:506] 502 batches submitted to accumulate stats from 32128 documents (2525862 virtual)\n",
      "I0228 01:50:14.795516 140370323834688 text_analysis.py:506] 503 batches submitted to accumulate stats from 32192 documents (2531141 virtual)\n",
      "I0228 01:50:14.806349 140370323834688 text_analysis.py:506] 504 batches submitted to accumulate stats from 32256 documents (2536268 virtual)\n",
      "I0228 01:50:14.808210 140370323834688 text_analysis.py:506] 505 batches submitted to accumulate stats from 32320 documents (2541256 virtual)\n",
      "I0228 01:50:14.816454 140370323834688 text_analysis.py:506] 506 batches submitted to accumulate stats from 32384 documents (2546726 virtual)\n",
      "I0228 01:50:14.829537 140370323834688 text_analysis.py:506] 507 batches submitted to accumulate stats from 32448 documents (2551630 virtual)\n",
      "I0228 01:50:14.850118 140370323834688 text_analysis.py:506] 508 batches submitted to accumulate stats from 32512 documents (2556725 virtual)\n",
      "I0228 01:50:14.857125 140370323834688 text_analysis.py:506] 509 batches submitted to accumulate stats from 32576 documents (2562102 virtual)\n",
      "I0228 01:50:14.863969 140370323834688 text_analysis.py:506] 510 batches submitted to accumulate stats from 32640 documents (2567611 virtual)\n",
      "I0228 01:50:14.867510 140370323834688 text_analysis.py:506] 511 batches submitted to accumulate stats from 32704 documents (2573077 virtual)\n",
      "I0228 01:50:14.878916 140370323834688 text_analysis.py:506] 512 batches submitted to accumulate stats from 32768 documents (2578388 virtual)\n",
      "I0228 01:50:14.904876 140370323834688 text_analysis.py:506] 513 batches submitted to accumulate stats from 32832 documents (2583646 virtual)\n",
      "I0228 01:50:14.917612 140370323834688 text_analysis.py:506] 514 batches submitted to accumulate stats from 32896 documents (2589098 virtual)\n",
      "I0228 01:50:14.924076 140370323834688 text_analysis.py:506] 515 batches submitted to accumulate stats from 32960 documents (2594444 virtual)\n",
      "I0228 01:50:14.925855 140370323834688 text_analysis.py:506] 516 batches submitted to accumulate stats from 33024 documents (2599089 virtual)\n",
      "I0228 01:50:14.927962 140370323834688 text_analysis.py:506] 517 batches submitted to accumulate stats from 33088 documents (2604328 virtual)\n",
      "I0228 01:50:14.955073 140370323834688 text_analysis.py:506] 518 batches submitted to accumulate stats from 33152 documents (2609468 virtual)\n",
      "I0228 01:50:14.972478 140370323834688 text_analysis.py:506] 519 batches submitted to accumulate stats from 33216 documents (2614282 virtual)\n",
      "I0228 01:50:14.976178 140370323834688 text_analysis.py:506] 520 batches submitted to accumulate stats from 33280 documents (2619278 virtual)\n",
      "I0228 01:50:14.978716 140370323834688 text_analysis.py:506] 521 batches submitted to accumulate stats from 33344 documents (2624373 virtual)\n",
      "I0228 01:50:14.993283 140370323834688 text_analysis.py:506] 522 batches submitted to accumulate stats from 33408 documents (2629429 virtual)\n",
      "I0228 01:50:15.011885 140370323834688 text_analysis.py:506] 523 batches submitted to accumulate stats from 33472 documents (2634797 virtual)\n",
      "I0228 01:50:15.025438 140370323834688 text_analysis.py:506] 524 batches submitted to accumulate stats from 33536 documents (2639806 virtual)\n",
      "I0228 01:50:15.027404 140370323834688 text_analysis.py:506] 525 batches submitted to accumulate stats from 33600 documents (2645504 virtual)\n",
      "I0228 01:50:15.029689 140370323834688 text_analysis.py:506] 526 batches submitted to accumulate stats from 33664 documents (2650643 virtual)\n",
      "I0228 01:50:15.055659 140370323834688 text_analysis.py:506] 527 batches submitted to accumulate stats from 33728 documents (2656206 virtual)\n",
      "I0228 01:50:15.074767 140370323834688 text_analysis.py:506] 528 batches submitted to accumulate stats from 33792 documents (2661277 virtual)\n",
      "I0228 01:50:15.077807 140370323834688 text_analysis.py:506] 529 batches submitted to accumulate stats from 33856 documents (2666732 virtual)\n",
      "I0228 01:50:15.080318 140370323834688 text_analysis.py:506] 530 batches submitted to accumulate stats from 33920 documents (2671878 virtual)\n",
      "I0228 01:50:15.083830 140370323834688 text_analysis.py:506] 531 batches submitted to accumulate stats from 33984 documents (2676862 virtual)\n",
      "I0228 01:50:15.116175 140370323834688 text_analysis.py:506] 532 batches submitted to accumulate stats from 34048 documents (2682109 virtual)\n",
      "I0228 01:50:15.126907 140370323834688 text_analysis.py:506] 533 batches submitted to accumulate stats from 34112 documents (2687431 virtual)\n",
      "I0228 01:50:15.128834 140370323834688 text_analysis.py:506] 534 batches submitted to accumulate stats from 34176 documents (2692911 virtual)\n",
      "I0228 01:50:15.136981 140370323834688 text_analysis.py:506] 535 batches submitted to accumulate stats from 34240 documents (2698537 virtual)\n",
      "I0228 01:50:15.138912 140370323834688 text_analysis.py:506] 536 batches submitted to accumulate stats from 34304 documents (2704122 virtual)\n",
      "I0228 01:50:15.178887 140370323834688 text_analysis.py:506] 537 batches submitted to accumulate stats from 34368 documents (2709232 virtual)\n",
      "I0228 01:50:15.181814 140370323834688 text_analysis.py:506] 538 batches submitted to accumulate stats from 34432 documents (2714212 virtual)\n",
      "I0228 01:50:15.183669 140370323834688 text_analysis.py:506] 539 batches submitted to accumulate stats from 34496 documents (2719326 virtual)\n",
      "I0228 01:50:15.190148 140370323834688 text_analysis.py:506] 540 batches submitted to accumulate stats from 34560 documents (2724449 virtual)\n",
      "I0228 01:50:15.192018 140370323834688 text_analysis.py:506] 541 batches submitted to accumulate stats from 34624 documents (2729736 virtual)\n",
      "I0228 01:50:15.232186 140370323834688 text_analysis.py:506] 542 batches submitted to accumulate stats from 34688 documents (2735215 virtual)\n",
      "I0228 01:50:15.238148 140370323834688 text_analysis.py:506] 543 batches submitted to accumulate stats from 34752 documents (2740299 virtual)\n",
      "I0228 01:50:15.244072 140370323834688 text_analysis.py:506] 544 batches submitted to accumulate stats from 34816 documents (2745125 virtual)\n",
      "I0228 01:50:15.244757 140370323834688 text_analysis.py:506] 545 batches submitted to accumulate stats from 34880 documents (2745161 virtual)\n",
      "I0228 01:50:15.300172 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:50:15.303363 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:50:15.307179 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:50:15.316659 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:50:15.341399 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:50:15.341640 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:50:15.344844 140370323834688 text_analysis.py:600] serializing accumulator to return to master...\n",
      "I0228 01:50:15.346074 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:50:15.349983 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:50:15.349761 140370323834688 text_analysis.py:602] accumulator serialized\n",
      "I0228 01:50:15.755156 140370323834688 text_analysis.py:530] 5 accumulators retrieved from output queue\n",
      "I0228 01:50:15.770943 140370323834688 text_analysis.py:552] accumulated word occurrence stats for 2745410 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_scores\n",
      "{'epoch': 249, 'cv': 0.6829638229667272, 'umass': -4.269305145812943, 'uci': -0.13500463625257364, 'npmi': 0.07018310067964657, 'rbo': 1.0, 'td': 1.0, 'train_loss': 641.681149977196, 'topics': [['c0267454', 'c0605290', 'heparin', 'ref', 'c1446219', 'salivary', 'c0536858', 'c0052432', 'c0521990', 'c0020933', 'c0851891', 'c0751651', 'postpartum', 'c1960870', 'c0032821', 'c0038174', 'c0444584', 'c0036323', 'c0024348', 'c0245109', 'excretion', 'c0007789', 'c2003941', 'colitis', 'swell', 'c1443650'], ['c0015967', 'c0032285', 'c0011900', 'c0012634', 'child', 'c0546788', 'c0035236', 'common', 'c0003232', 'disclosure', 'c1457887', 'c0010200', 'c0019993', 'hospitalize', 'clinical', 'c0221423', 'presentation', 'sars-cov-2', 'c0035235', 'c0039082', 'c0027442', 'c0038410', 'manifestation', 'respiratory', 'c0809949', 'c0021400'], ['c0543467', 'c0025080', 'c0031150', 'postoperative', 'c0002940', 'c0038930', 'c0728940', 'operative', 'c0005898', 'procedure', 'c0850292', 'recurrence', 'c0229962', 'perioperative', 'undergo', 'c0547070', 'c0019080', 'c0014245', 'c0009566', 'c0582175', 'surgical', 'c0162522', 'conversion', 'c0187996', 'c4039858', 'c0085590'], ['c0032042', 'c0199470', 'c0034108', 'compare', 'c0243095', 'difference', 'significantly', 'c0918012', 'c0235195', 'measurement', 'predictor', 'c0032740', 'significant', 'confidence', 'neonate', 'curve', 'c0369768', 'mmol', 'decrease', 'c0021708', 'c0005516', 'receive', 'infant', 'c0038454', 'odds', 'regression'], ['policy', 'crisis', 'disaster', 'threat', 'economic', 'political', 'market', 'economy', 'sector', 'emergency', 'supply', 'international', 'trade', 'draw', 'public', 'inequality', 'c0242456', 'governance', 'national', 'contemporary', 'food', 'argue', 'society', 'face', 'investment', 'financial'], ['c0042210', 'c1254351', 'c0030956', 'c0003320', 'c0029224', 'c0020971', 'c0003316', 'nanoparticles', 'c0003250', 'c1706082', 'c1167622', 'affinity', 'active', 'potency', 'c1514562', 'candidate', 'c3687832', 'potential', 'synthetic', 'drug', 'elicit', 'c0021083', 'c0005479', 'potent', 'promise', 'therapeutic'], ['activation', 'c1171362', 'c0024432', 'c0162638', 'c0007613', 'c0079189', 'mechanism', 'c0007634', 'c0025929', 'activate', 'role', 'c3539881', 'c0021747', 'induction', 'c0017262', 'pathway', 'c0035696', 'c0023810', 'c0004391', 'c0013081', 'induce', 'c1101610', 'c0596290', 'c1327622', 'c0014597', 'suppress'], ['c0002045', 'propose', 'c3161035', 'automate', 'machine', 'c0150098', 'c0025663', 'c0679083', 'accuracy', 'sensor', 'prediction', 'image', 'performance', 'c0037585', 'compute', 'input', 'outperform', 'computational', 'algorithm', 'c1710191', 'representation', 'filter', 'c1704254', 'c0037589', 'equation', 'solve'], ['c0679646', 'search', 'conduct', 'c0038951', 'c0025353', 'c2603343', 'evidence', 'c0003467', 'c0242356', 'c0282122', 'train', 'report', 'c0086388', 'include', 'c1955832', 'psychological', 'c0027361', 'c0030971', 'parent', 'impact', 'c1257890', 'c0034394', 'c0011570', 'c0242481', 'recommendation', 'c1706852'], ['c1705920', 'c0684063', 'c0042776', 'c0032098', 'c0005595', 'c0017428', 'genetic', 'c0039005', 'c0007452', 'sample', 'c0003062', 'genotype', 'c1764827', 'c0012984', 'diversity', 'c1519068', 'c0017337', 'c0004793', 'c0015733', 'c0162326', 'isolate', 'c1294197', 'specie', 'c0015219', 'c0442726', 'c0026882']]}\n",
      "elapsed time = 1496.0187120437622\n"
     ]
    }
   ],
   "source": [
    "from contextualized_topic_models.models.ctm import CTM\n",
    "import os\n",
    "import pickle\n",
    "from contextualized_topic_models.utils.data_preparation import TextHandler\n",
    "from contextualized_topic_models.datasets.dataset import CTMDataset, CTMNoBERTDataset\n",
    "import time\n",
    "\n",
    "tmp = {k: v for k, v in sorted(count_vect.vocabulary_.items(), key=lambda item: item[1])}\n",
    "idx2token = dict((v,k) for k,v in tmp.items())\n",
    "X_test_counts = count_vect.transform(test_df['prep_text'])\n",
    "dataset = CTMNoBERTDataset(X_train_counts,  idx2token)\n",
    "test_set = CTMNoBERTDataset(X_test_counts,  idx2token)\n",
    "# # create the dataset\n",
    "\n",
    "start = time.time()\n",
    "ctm = CTM(input_size=len(count_vect.vocabulary_), bert_input_size=768, num_epochs=250, inference_type=\"noBERT\", n_components=10, texts = texts, model_type='prodLDA')\n",
    "\n",
    "ctm.fit(dataset, save_dir = 'saved_models/vocab_10k/', save_every = 25)\n",
    "end = time.time()\n",
    "print('elapsed time = {}'.format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load saved topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhamzeia/Thesis/BiomedicalTopicModelling/contextualized_topic_models/models/ctm.py:537: Warning: This is an experimental feature that we has not been fully tested. Refer to the following issue:https://github.com/MilaNLProc/contextualized-topic-models/issues/38\n",
      "  Warning)\n"
     ]
    }
   ],
   "source": [
    "# path_2k_25 = \"saved_models/vocab_2k/contextualized_topic_model_nc_25_tpm_0.0_tpv_0.96_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99/\"\n",
    "# path_2k_10 = \"saved_models/vocab_2k/contextualized_topic_model_nc_10_tpm_0.0_tpv_0.9_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99\"\n",
    "# path_2k_50 = \"saved_models/vocab_2k/contextualized_topic_model_nc_50_tpm_0.0_tpv_0.98_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99/\"\n",
    "\n",
    "# path_10k_10 = \"saved_models/vocab_10k/contextualized_topic_model_nc_10_tpm_0.0_tpv_0.9_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99/\"\n",
    "path_10k_15 = \"saved_models/vocab_10k/contextualized_topic_model_nc_15_tpm_0.0_tpv_0.9333333333333333_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99\"\n",
    "\n",
    "\n",
    "# path_10k_25 = \"saved_models/vocab_10k/contextualized_topic_model_nc_25_tpm_0.0_tpv_0.96_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99/\"\n",
    "# path_10k_50 = \"saved_models/vocab_10k/contextualized_topic_model_nc_50_tpm_0.0_tpv_0.98_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99/\"\n",
    "# path_10k_25 = \"saved_models/vocab_10k/contextualized_topic_model_nc_25_tpm_0.0_tpv_0.96_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99/\"\n",
    "path_10k_20 = \"saved_models/vocab_10k/contextualized_topic_model_nc_20_tpm_0.0_tpv_0.95_hs_prodLDA_ac_(100, 100)_do_softplus_lr_0.2_mo_0.002_rp_0.99/\"\n",
    "ctm.load(path_10k_15)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>cv</th>\n",
       "      <th>umass</th>\n",
       "      <th>uci</th>\n",
       "      <th>npmi</th>\n",
       "      <th>rbo</th>\n",
       "      <th>td</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>149</td>\n",
       "      <td>0.698884</td>\n",
       "      <td>-4.043187</td>\n",
       "      <td>-0.002791</td>\n",
       "      <td>0.078487</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>642.106910</td>\n",
       "      <td>[[c0267454, ref, salivary, heparin, c0020933, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>199</td>\n",
       "      <td>0.696658</td>\n",
       "      <td>-4.152843</td>\n",
       "      <td>-0.008847</td>\n",
       "      <td>0.077808</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>641.871704</td>\n",
       "      <td>[[c0267454, ref, heparin, c1446219, salivary, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>224</td>\n",
       "      <td>0.688459</td>\n",
       "      <td>-4.284695</td>\n",
       "      <td>-0.073003</td>\n",
       "      <td>0.075186</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>641.725285</td>\n",
       "      <td>[[c0267454, c1446219, ref, heparin, c0605290, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74</td>\n",
       "      <td>0.688210</td>\n",
       "      <td>-3.781730</td>\n",
       "      <td>0.024983</td>\n",
       "      <td>0.077232</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.992</td>\n",
       "      <td>642.115541</td>\n",
       "      <td>[[c0267454, salivary, c0020933, ref, heparin, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>124</td>\n",
       "      <td>0.686185</td>\n",
       "      <td>-4.128048</td>\n",
       "      <td>-0.043299</td>\n",
       "      <td>0.075438</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>642.054074</td>\n",
       "      <td>[[c0267454, salivary, ref, c0605290, heparin, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch        cv     umass       uci      npmi  rbo     td  train_loss  \\\n",
       "5    149  0.698884 -4.043187 -0.002791  0.078487  1.0  1.000  642.106910   \n",
       "7    199  0.696658 -4.152843 -0.008847  0.077808  1.0  1.000  641.871704   \n",
       "8    224  0.688459 -4.284695 -0.073003  0.075186  1.0  1.000  641.725285   \n",
       "2     74  0.688210 -3.781730  0.024983  0.077232  1.0  0.992  642.115541   \n",
       "4    124  0.686185 -4.128048 -0.043299  0.075438  1.0  1.000  642.054074   \n",
       "\n",
       "                                              topics  \n",
       "5  [[c0267454, ref, salivary, heparin, c0020933, ...  \n",
       "7  [[c0267454, ref, heparin, c1446219, salivary, ...  \n",
       "8  [[c0267454, c1446219, ref, heparin, c0605290, ...  \n",
       "2  [[c0267454, salivary, c0020933, ref, heparin, ...  \n",
       "4  [[c0267454, salivary, ref, c0605290, heparin, ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_prodLDA = pd.DataFrame(ctm.scores_train)\n",
    "# scores_prodLDA['topics'] = ctm.all_topics\n",
    "scores_prodLDA.to_csv('results/scores_10_250_prodLDA_10k.csv')\n",
    "scores_prodLDA.sort_values(by =['cv'], ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# relate CUI's to umls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:51:19.235479 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: dummy distance type: INT\n",
      "I0228 01:51:19.236166 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: dummy distance type: FLOAT\n",
      "I0228 01:51:19.236692 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: bit_hamming distance type: INT\n",
      "I0228 01:51:19.237624 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: bit_jaccard distance type: FLOAT\n",
      "I0228 01:51:19.238143 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: leven distance type: INT\n",
      "I0228 01:51:19.238675 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: normleven distance type: FLOAT\n",
      "I0228 01:51:19.239208 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: kldivfast distance type: FLOAT\n",
      "I0228 01:51:19.239706 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: kldivfastrq distance type: FLOAT\n",
      "I0228 01:51:19.240237 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: kldivgenfast distance type: FLOAT\n",
      "I0228 01:51:19.240717 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: kldivgenslow distance type: FLOAT\n",
      "I0228 01:51:19.241211 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: kldivgenfastrq distance type: FLOAT\n",
      "I0228 01:51:19.241696 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: itakurasaitofast distance type: FLOAT\n",
      "I0228 01:51:19.242199 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: jsdivslow distance type: FLOAT\n",
      "I0228 01:51:19.242585 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: jsdivfast distance type: FLOAT\n",
      "I0228 01:51:19.242964 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: jsdivfastapprox distance type: FLOAT\n",
      "I0228 01:51:19.243352 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: jsmetrslow distance type: FLOAT\n",
      "I0228 01:51:19.243734 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: jsmetrfast distance type: FLOAT\n",
      "I0228 01:51:19.244128 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: jsmetrfastapprox distance type: FLOAT\n",
      "I0228 01:51:19.244508 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: word_embed distance type: FLOAT\n",
      "I0228 01:51:19.244885 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: lp distance type: FLOAT\n",
      "I0228 01:51:19.246323 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: linf distance type: FLOAT\n",
      "I0228 01:51:19.246814 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: l1 distance type: FLOAT\n",
      "I0228 01:51:19.247273 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: l2 distance type: FLOAT\n",
      "I0228 01:51:19.247705 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: cosinesimil distance type: FLOAT\n",
      "I0228 01:51:19.248113 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: angulardist distance type: FLOAT\n",
      "I0228 01:51:19.248501 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: negdotprod distance type: FLOAT\n",
      "I0228 01:51:19.248885 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: lp_sparse distance type: FLOAT\n",
      "I0228 01:51:19.249281 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: linf_sparse distance type: FLOAT\n",
      "I0228 01:51:19.249665 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: l1_sparse distance type: FLOAT\n",
      "I0228 01:51:19.250049 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: l2_sparse distance type: FLOAT\n",
      "I0228 01:51:19.250436 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: cosinesimil_sparse distance type: FLOAT\n",
      "I0228 01:51:19.250816 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: angulardist_sparse distance type: FLOAT\n",
      "I0228 01:51:19.251202 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: negdotprod_sparse distance type: FLOAT\n",
      "I0228 01:51:19.251604 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: cosinesimil_sparse_fast distance type: FLOAT\n",
      "I0228 01:51:19.251995 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: angulardist_sparse_fast distance type: FLOAT\n",
      "I0228 01:51:19.252373 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: negdotprod_sparse_fast distance type: FLOAT\n",
      "I0228 01:51:19.253823 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: querynorm_negdotprod_sparse_fast distance type: FLOAT\n",
      "I0228 01:51:19.254218 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: jaccard_sparse distance type: FLOAT\n",
      "I0228 01:51:19.254605 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: abdiv_slow distance type: FLOAT\n",
      "I0228 01:51:19.255236 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: abdiv_fast distance type: FLOAT\n",
      "I0228 01:51:19.255575 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: renyidiv_slow distance type: FLOAT\n",
      "I0228 01:51:19.255909 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: renyidiv_fast distance type: FLOAT\n",
      "I0228 01:51:19.256228 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, space: l2sqr_sift distance type: INT\n",
      "I0228 01:51:19.256556 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: dummy distance type: FLOAT\n",
      "I0228 01:51:19.256888 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: dummy distance type: INT\n",
      "I0228 01:51:19.257205 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: pivot_neighb_invindx distance type: FLOAT\n",
      "I0228 01:51:19.257524 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: pivot_neighb_invindx distance type: INT\n",
      "I0228 01:51:19.257855 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: napp distance type: FLOAT\n",
      "I0228 01:51:19.258173 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: napp distance type: INT\n",
      "I0228 01:51:19.258490 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: brute_force distance type: FLOAT\n",
      "I0228 01:51:19.258819 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: brute_force distance type: INT\n",
      "I0228 01:51:19.259136 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: seq_search distance type: FLOAT\n",
      "I0228 01:51:19.259464 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: seq_search distance type: INT\n",
      "I0228 01:51:19.259793 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: sw-graph distance type: FLOAT\n",
      "I0228 01:51:19.260107 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: sw-graph distance type: INT\n",
      "I0228 01:51:19.260424 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: small_world_rand distance type: FLOAT\n",
      "I0228 01:51:19.260747 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: small_world_rand distance type: INT\n",
      "I0228 01:51:19.261059 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: hnsw distance type: FLOAT\n",
      "I0228 01:51:19.261374 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: hnsw distance type: INT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 01:51:19.261717 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: vptree distance type: INT\n",
      "I0228 01:51:19.262042 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: vptree distance type: FLOAT\n",
      "I0228 01:51:19.262367 140370323834688 <frozen importlib._bootstrap>:219] Registering at the factory, method: simple_invindx distance type: FLOAT\n",
      "I0228 01:51:29.698563 140370323834688 candidate_generation.py:140] Loading index from /home/dhamzeia/.scispacy/datasets/dda67f3ef5951cfedf2de7de7b3561d45ce9e8ed92346cc196310a76b15fe0b6.fdebf628ed43f2d0fba4f250d97a57ddcb36b183308702481009f67c5d771465.nmslib_index.bin\n",
      "I0228 01:51:29.699603 140370323834688 candidate_generation.py:140] Loading regular index.\n",
      "I0228 01:51:39.286520 140370323834688 candidate_generation.py:140] Finished loading index\n",
      "I0228 01:51:39.287150 140370323834688 candidate_generation.py:140] Set HNSW query-time parameters:\n",
      "I0228 01:51:39.287716 140370323834688 candidate_generation.py:140] ef(Search)         =20\n",
      "I0228 01:51:39.288203 140370323834688 candidate_generation.py:140] algoType           =2\n",
      "I0228 01:51:39.288704 140370323834688 candidate_generation.py:142] Set HNSW query-time parameters:\n",
      "I0228 01:51:39.289134 140370323834688 candidate_generation.py:142] ef(Search)         =200\n",
      "I0228 01:51:39.289546 140370323834688 candidate_generation.py:142] algoType           =2\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/scispacy/candidate_generation.py:228: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/dhamzeia/.scispacy/datasets/d485a39692e39f93339abf6102c3336dda07d84d8ae46a03d55079b54cad1137.a65425f2cddfa1f741a17c18fc68797f5434547425e30bbe236dea0dcb7a9389.concept_aliases.json' mode='r' encoding='UTF-8'>\n",
      "  open(cached_path(linker_paths.concept_aliases_list))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/scispacy/linking_utils.py:66: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/dhamzeia/.scispacy/datasets/cf6297e16bc34e731db62e18b99603d440223279d6574e101d610ab5c713b265.1b0c2eaa35a3a31246741feac4b0255581214437fb86cd941018283668d461b0.umls_2020_aa_cat0129.jsonl' mode='r' encoding='UTF-8'>\n",
      "  raw = [json.loads(line) for line in open(cached_path(file_path))]\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/scispacy/umls_semantic_type_tree.py:100: ResourceWarning: unclosed file <_io.TextIOWrapper name='/home/dhamzeia/.scispacy/datasets/21a1012c532c3a431d60895c509f5b4d45b0f8966c4178b892190a302b21836f.330707f4efe774134872b9f77f0e3208c1d30f50800b3b39a6b8ec21d9adf1b7.umls_semantic_type_tree.tsv' mode='r' encoding='UTF-8'>\n",
      "  for line in open(cached_path(filepath), \"r\"):\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from scispacy.umls_linking import UmlsEntityLinker\n",
    "linker = UmlsEntityLinker(resolve_abbreviations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhamzeia/.local/lib/python3.7/site-packages/ipykernel_launcher.py:8: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n"
     ]
    }
   ],
   "source": [
    "# store topics to csv\n",
    "import yaml\n",
    "scores_prodLDA = pd.read_csv('results/scores_10_250_prodLDA_10k.csv')\n",
    "regexp = re.compile(r'c\\d{7}')\n",
    "\n",
    "df = pd.DataFrame(columns = ['topic', 'words/entities'])\n",
    "to_write = ''\n",
    "for i, topic in enumerate(yaml.load(scores_prodLDA.sort_values(by =['cv'], ascending = False)['topics'][0])):\n",
    "    to_write+='topic '+str(i)+':\\n'\n",
    "    new_top =[]\n",
    "    for w in topic:\n",
    "        if regexp.search(w):\n",
    "            try:\n",
    "                new_top.append('C'+w[1:len(w)] +':'+ linker.umls.cui_to_entity['C'+w[1:len(w)]][1])\n",
    "            except Exception:\n",
    "                new_top.append(w)\n",
    "        else:\n",
    "            new_top.append(w)\n",
    "        df.loc[i] =  i, new_top\n",
    "#     print(topic)\n",
    "df.head()\n",
    "\n",
    "\n",
    "# with pd.option_context(\"max_colwidth\", 1000):\n",
    "\n",
    "#     print(df.to_latex(index=False)) \n",
    "#     print(str(df))\n",
    "df.to_csv('topics/topics_10_10k_250.csv')\n",
    "# text_file = open(\"topics/topics_10_10k_1000.txt\", \"w\")\n",
    "# text_file.write(df.to_latex(index=False))\n",
    "# text_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read results for plotting graphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhamzeia/.local/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/ipykernel_launcher.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  import sys\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/ipykernel_launcher.py:10: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/ipykernel_launcher.py:13: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  del sys.path[0]\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/ipykernel_launcher.py:16: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  app.launch_new_instance()\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/ipykernel_launcher.py:19: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/ipykernel_launcher.py:22: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/ipykernel_launcher.py:25: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/ipykernel_launcher.py:28: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/ipykernel_launcher.py:31: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/ipykernel_launcher.py:34: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/ipykernel_launcher.py:37: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/ipykernel_launcher.py:40: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "scores_10_10k = pd.read_csv('results/scores_10_1000_prodLDA_10k.csv')\n",
    "topics_10_10k = yaml.load(scores_10_10k.sort_values(by =['cv'], ascending = False).reset_index()['topics'][0])\n",
    "\n",
    "scores_15_10k = pd.read_csv('results/scores_15_250_prodLDA_10k.csv')\n",
    "topics_15_10k = yaml.load(scores_15_10k.sort_values(by =['cv'], ascending = False).reset_index()['topics'][0])\n",
    "\n",
    "scores_20_10k = pd.read_csv('results/scores_20_250_prodLDA_10k.csv')\n",
    "topics_20_10k = yaml.load(scores_20_10k.sort_values(by =['cv'], ascending = False).reset_index()['topics'][0])\n",
    "\n",
    "scores_25_10k = pd.read_csv('results/scores_25_1000_prodLDA_10k.csv')\n",
    "topics_25_10k = yaml.load(scores_25_10k.sort_values(by =['cv'], ascending = False).reset_index()['topics'][0])\n",
    "\n",
    "scores_30_10k = pd.read_csv('results/scores_30_250_prodLDA_10k.csv')\n",
    "topics_30_10k = yaml.load(scores_30_10k.sort_values(by =['cv'], ascending = False).reset_index()['topics'][0])\n",
    "\n",
    "scores_35_10k = pd.read_csv('results/scores_35_250_prodLDA_10k.csv')\n",
    "topics_35_10k = yaml.load(scores_35_10k.sort_values(by =['cv'], ascending = False).reset_index()['topics'][0])\n",
    "\n",
    "scores_40_10k = pd.read_csv('results/scores_40_250_prodLDA_10k.csv')\n",
    "topics_40_10k = yaml.load(scores_40_10k.sort_values(by =['cv'], ascending = False).reset_index()['topics'][0])\n",
    "\n",
    "scores_45_10k = pd.read_csv('results/scores_45_250_prodLDA_10k.csv')\n",
    "topics_45_10k = yaml.load(scores_45_10k.sort_values(by =['cv'], ascending = False).reset_index()['topics'][0])\n",
    "\n",
    "scores_50_10k = pd.read_csv('results/scores_50_1000_prodLDA_10k.csv')\n",
    "topics_50_10k = yaml.load(scores_50_10k.sort_values(by =['cv'], ascending = False).reset_index()['topics'][0])\n",
    "\n",
    "scores_25_2k = pd.read_csv('results/scores_25_1000_prodLDA_2k.csv')\n",
    "topics_25_2k = yaml.load(scores_25_2k.sort_values(by =['cv'], ascending = False).reset_index()['topics'][0])\n",
    "\n",
    "scores_10_2k = pd.read_csv('results/scores_10_1000_prodLDA_2k.csv')\n",
    "topics_10_2k = yaml.load(scores_10_2k.sort_values(by =['cv'], ascending = False).reset_index()['topics'][0])\n",
    "\n",
    "scores_25_2k = pd.read_csv('results/scores_25_1000_prodLDA_2k.csv')\n",
    "topics_25_2k = yaml.load(scores_25_2k.sort_values(by =['cv'], ascending = False).reset_index()['topics'][0])\n",
    "\n",
    "scores_50_2k = pd.read_csv('results/scores_50_1000_prodLDA_2k.csv')\n",
    "topics_50_2k = yaml.load(scores_50_2k.sort_values(by =['cv'], ascending = False).reset_index()['topics'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# external topic coherence calculation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from contextualized_topic_models.evaluation import measures\n",
    "cv_ext_10_10k_ = measures.CoherenceCV(topics = topics_10_10k, texts= test_texts).score()\n",
    "cv_ext_15_10k_ = measures.CoherenceCV(topics = topics_15_10k, texts= test_texts).score()\n",
    "cv_ext_20_10k_ = measures.CoherenceCV(topics = topics_20_10k, texts= test_texts).score()\n",
    "cv_ext_25_10k_ = measures.CoherenceCV(topics = topics_25_10k, texts= test_texts).score()\n",
    "cv_ext_30_10k_ = measures.CoherenceCV(topics = topics_30_10k, texts= test_texts).score()\n",
    "cv_ext_35_10k_ = measures.CoherenceCV(topics = topics_35_10k, texts= test_texts).score()\n",
    "cv_ext_40_10k_ = measures.CoherenceCV(topics = topics_40_10k, texts= test_texts).score()\n",
    "cv_ext_45_10k_ = measures.CoherenceCV(topics = topics_45_10k, texts= test_texts).score()\n",
    "cv_ext_50_10k_ = measures.CoherenceCV(topics = topics_50_10k, texts= test_texts).score()\n",
    "\n",
    "\n",
    "# npmi_ext_10_10k_ = measures.CoherenceNPMI(topics = topics_10_10k, texts= test_texts).score()\n",
    "# npmi_ext_15_10k_ = measures.CoherenceNPMI(topics = topics_15_10k, texts= test_texts).score()\n",
    "# npmi_ext_20_10k_ = measures.CoherenceNPMI(topics = topics_20_10k, texts= test_texts).score()\n",
    "# npmi_ext_25_10k_ = measures.CoherenceNPMI(topics = topics_25_10k, texts= test_texts).score()\n",
    "# npmi_ext_30_10k_ = measures.CoherenceNPMI(topics = topics_30_10k, texts= test_texts).score()\n",
    "# npmi_ext_35_10k_ = measures.CoherenceNPMI(topics = topics_35_10k, texts= test_texts).score()\n",
    "# npmi_ext_40_10k_ = measures.CoherenceNPMI(topics = topics_40_10k, texts= test_texts).score()\n",
    "# npmi_ext_45_10k_ = measures.CoherenceNPMI(topics = topics_45_10k, texts= test_texts).score()\n",
    "# npmi_ext_50_10k_ = measures.CoherenceNPMI(topics = topics_50_10k, texts= test_texts).score()\n",
    "\n",
    "\n",
    "# umass_ext_10_10k_ = measures.CoherenceUMASS(topics = topics_10_10k, texts= test_texts).score()\n",
    "# umass_ext_15_10k_ = measures.CoherenceUMASS(topics = topics_15_10k, texts= test_texts).score()\n",
    "# umass_ext_20_10k_ = measures.CoherenceUMASS(topics = topics_20_10k, texts= test_texts).score()\n",
    "# umass_ext_25_10k_ = measures.CoherenceUMASS(topics = topics_25_10k, texts= test_texts).score()\n",
    "# umass_ext_30_10k_ = measures.CoherenceUMASS(topics = topics_30_10k, texts= test_texts).score()\n",
    "# umass_ext_35_10k_ = measures.CoherenceUMASS(topics = topics_35_10k, texts= test_texts).score()\n",
    "# umass_ext_40_10k_ = measures.CoherenceUMASS(topics = topics_40_10k, texts= test_texts).score()\n",
    "# umass_ext_45_10k_ = measures.CoherenceUMASS(topics = topics_45_10k, texts= test_texts).score()\n",
    "# umass_ext_50_10k_ = measures.CoherenceUMASS(topics = topics_50_10k, texts= test_texts).score()\n",
    "\n",
    "# cv_ext_10_2k_ = measures.CoherenceCV(topics = topics_10_2k, texts= test_texts).score()\n",
    "# cv_ext_25_2k_ = measures.CoherenceCV(topics = topics_25_2k, texts= test_texts).score()\n",
    "# cv_ext_50_2k_ = measures.CoherenceCV(topics = topics_50_2k, texts= test_texts).score()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# internal topic coherence calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_int_10_10k_ = scores_10_10k.sort_values(by =['cv'], ascending = False).reset_index()['cv'][0]\n",
    "cv_int_15_10k_ = scores_15_10k.sort_values(by =['cv'], ascending = False).reset_index()['cv'][0]\n",
    "cv_int_20_10k_ = scores_20_10k.sort_values(by =['cv'], ascending = False).reset_index()['cv'][0]\n",
    "cv_int_25_10k_ = scores_25_10k.sort_values(by =['cv'], ascending = False).reset_index()['cv'][0]\n",
    "cv_int_30_10k_ = scores_30_10k.sort_values(by =['cv'], ascending = False).reset_index()['cv'][0]\n",
    "cv_int_35_10k_ = scores_35_10k.sort_values(by =['cv'], ascending = False).reset_index()['cv'][0]\n",
    "cv_int_40_10k_ = scores_40_10k.sort_values(by =['cv'], ascending = False).reset_index()['cv'][0]\n",
    "cv_int_45_10k_ = scores_45_10k.sort_values(by =['cv'], ascending = False).reset_index()['cv'][0]\n",
    "cv_int_50_10k_ = scores_50_10k.sort_values(by =['cv'], ascending = False).reset_index()['cv'][0]\n",
    "\n",
    "# npmi_int_10_10k_ = scores_10_10k.sort_values(by =['cv'], ascending = False).reset_index()['npmi'][0]\n",
    "# npmi_int_15_10k_ = scores_15_10k.sort_values(by =['cv'], ascending = False).reset_index()['npmi'][0]\n",
    "# npmi_int_20_10k_ = scores_20_10k.sort_values(by =['cv'], ascending = False).reset_index()['npmi'][0]\n",
    "# npmi_int_25_10k_ = scores_25_10k.sort_values(by =['cv'], ascending = False).reset_index()['npmi'][0]\n",
    "# npmi_int_30_10k_ = scores_30_10k.sort_values(by =['cv'], ascending = False).reset_index()['npmi'][0]\n",
    "# npmi_int_35_10k_ = scores_35_10k.sort_values(by =['cv'], ascending = False).reset_index()['npmi'][0]\n",
    "# npmi_int_40_10k_ = scores_40_10k.sort_values(by =['cv'], ascending = False).reset_index()['npmi'][0]\n",
    "# npmi_int_45_10k_ = scores_45_10k.sort_values(by =['cv'], ascending = False).reset_index()['npmi'][0]\n",
    "# npmi_int_50_10k_ = scores_50_10k.sort_values(by =['cv'], ascending = False).reset_index()['npmi'][0]\n",
    "\n",
    "\n",
    "# umass_int_10_10k_ = scores_10_10k.sort_values(by =['cv'], ascending = False).reset_index()['umass'][0]\n",
    "# umass_int_15_10k_ = scores_15_10k.sort_values(by =['cv'], ascending = False).reset_index()['umass'][0]\n",
    "# umass_int_20_10k_ = scores_20_10k.sort_values(by =['cv'], ascending = False).reset_index()['umass'][0]\n",
    "# umass_int_25_10k_ = scores_25_10k.sort_values(by =['cv'], ascending = False).reset_index()['umass'][0]\n",
    "# umass_int_30_10k_ = scores_30_10k.sort_values(by =['cv'], ascending = False).reset_index()['umass'][0]\n",
    "# umass_int_35_10k_ = scores_35_10k.sort_values(by =['cv'], ascending = False).reset_index()['umass'][0]\n",
    "# umass_int_40_10k_ = scores_40_10k.sort_values(by =['cv'], ascending = False).reset_index()['umass'][0]\n",
    "# umass_int_45_10k_ = scores_45_10k.sort_values(by =['cv'], ascending = False).reset_index()['umass'][0]\n",
    "# umass_int_50_10k_ = scores_50_10k.sort_values(by =['cv'], ascending = False).reset_index()['umass'][0]\n",
    "\n",
    "# cv_int_10_2k_ = scores_25_2k.sort_values(by =['cv'], ascending = False).reset_index()['cv'][0]\n",
    "# cv_int_25_2k_ = scores_25_2k.sort_values(by =['cv'], ascending = False).reset_index()['cv'][0]\n",
    "                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  compare LDA with ProdLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "scores_LDA_2k_50 = pd.read_csv('results/scores_50_1000_LDA_2k.csv')\n",
    "scores_ProdLDA_2k_50 = pd.read_csv('results/scores_50_1000_prodLDA_2k.csv')\n",
    "\n",
    "plt.plot(scores_ProdLDA_2k_50['epoch'].tolist(), scores_ProdLDA_2k_50['rbo'].tolist(), color='r', linestyle='-', label = 'ProdLDA')\n",
    "\n",
    "plt.plot(scores_LDA_2k_50['epoch'].tolist(), scores_LDA_2k_50['rbo'].tolist(), color='b', linestyle='-', label = 'LDA')\n",
    "# plt.plot(scores_LDA['epoch'].tolist(), scores_LDA['train_loss'].tolist(), color='b', linestyle='-', label = 'LDA')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('RBO')\n",
    "\n",
    "plt.legend(loc=\"figs/lower right\")\n",
    "# plt.title('')\n",
    "plt.savefig('figs/compare_rbo_50_1000_2k.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  compare vocab sizes and ntopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "scores_ProdLDA_2k_10 = pd.read_csv('results/scores_10_1000_prodLDA_2k.csv')[0:10]\n",
    "scores_ProdLDA_2k_25 = pd.read_csv('results/scores_25_1000_prodLDA_2k.csv')[0:10]\n",
    "scores_ProdLDA_2k_50 = pd.read_csv('results/scores_50_1000_prodLDA_2k.csv')[0:10]\n",
    "\n",
    "scores_ProdLDA_10k_10 = pd.read_csv('results/scores_10_1000_prodLDA_10k.csv')[0:10]\n",
    "scores_ProdLDA_10k_25 = pd.read_csv('results/scores_25_1000_prodLDA_10k.csv')[0:10]\n",
    "scores_ProdLDA_10k_50 = pd.read_csv('results/scores_50_1000_prodLDA_10k.csv')[0:10]\n",
    "\n",
    "scores_ProdLDA_all_10 = pd.read_csv('results/scores_10_500_prodLDA_all.csv')[0:5]\n",
    "scores_ProdLDA_all_25 = pd.read_csv('results/scores_25_500_prodLDA_all.csv')[0:5]\n",
    "scores_ProdLDA_all_50 = pd.read_csv('results/scores_50_250_prodLDA_all.csv')[0:5]\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(6,3))\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(5,5))\n",
    "\n",
    "\n",
    "# axs = []\n",
    "# for row in ax:\n",
    "#     for col in row:\n",
    "#         axs.append(row)\n",
    "\n",
    "score= 'npmi'\n",
    "for ax, score, alpha, name in zip(axs.flatten(), ['cv', 'npmi', 'umass', 'uci'], ['a', 'b', 'c', 'd'], ['$C_v$', 'NPMI', 'UMASS', 'UCI']):\n",
    "# for ax, score, alpha in zip(axs, ['rbo', 'td'], ['a', 'b']):\n",
    "#     ax.set_xlabel('epoch'+ '\\n'+ alpha+') '+ score.upper() + ' score')\n",
    "    ax.set_xlabel('epoch'+ '\\n'+ alpha+') '+ name + ' score')\n",
    "    ax.set_ylabel(name)\n",
    "    ax.plot(scores_ProdLDA_2k_10['epoch'].tolist(), scores_ProdLDA_2k_10[score].tolist(), color='r', linestyle='-', label = '2k-10')\n",
    "    ax.plot(scores_ProdLDA_2k_25['epoch'].tolist(), scores_ProdLDA_2k_25[score].tolist(), color='r', linestyle='-.', label = '2k-25')\n",
    "    ax.plot(scores_ProdLDA_2k_50['epoch'].tolist(), scores_ProdLDA_2k_50[score].tolist(), color='r', linestyle=':', label = '2k-50')\n",
    "\n",
    "\n",
    "    ax.plot(scores_ProdLDA_10k_10['epoch'].tolist(), scores_ProdLDA_10k_10[score].tolist(), color='b', linestyle='-', label = '10k-10')\n",
    "    ax.plot(scores_ProdLDA_10k_25['epoch'].tolist(), scores_ProdLDA_10k_25[score].tolist(), color='b', linestyle='-.', label = '10k-25')\n",
    "    ax.plot(scores_ProdLDA_10k_50['epoch'].tolist(), scores_ProdLDA_10k_50[score].tolist(), color='b', linestyle=':', label = '10k-50')\n",
    "\n",
    "    ax.plot(scores_ProdLDA_all_10['epoch'].tolist(), scores_ProdLDA_all_10[score].tolist(), color='g', linestyle='-', label = 'full-10')\n",
    "    ax.plot(scores_ProdLDA_all_25['epoch'].tolist(), scores_ProdLDA_all_25[score].tolist(), color='g', linestyle='-.', label = 'full-25')\n",
    "    ax.plot(scores_ProdLDA_all_50['epoch'].tolist(), scores_ProdLDA_all_50[score].tolist(), color='g', linestyle=':', label = 'full-50')\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "lgd = fig.legend(handles, labels, loc='lower center', ncol = 3, bbox_to_anchor=(0.5, 1))\n",
    "fig.tight_layout(pad=1.0)\n",
    "# plt.title(score)\n",
    "\n",
    "# plt.savefig('samplefigure', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "\n",
    "plt.savefig('figs/vocab.png',  bbox_extra_artists=(lgd, ), dpi = 300 , bbox_inches='tight')\n",
    "\n",
    "# plt.savefig('figs/vocab_td.png',  bbox_extra_artists=(lgd, ), dpi = 300 , bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare internal and external topic coherence vs ntopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([10, 15, 20, 25, 30, 35, 40, 45, 50], [cv_ext_10_10k_, cv_ext_15_10k_, cv_ext_20_10k_, cv_ext_25_10k_, cv_ext_30_10k_, cv_ext_35_10k_ , cv_ext_40_10k_, cv_ext_45_10k_, cv_ext_50_10k_], label = 'External')\n",
    "plt.plot([10, 15, 20, 25, 30, 35, 40, 45, 50], [cv_int_10_10k_, cv_int_15_10k_, cv_int_20_10k_, cv_int_25_10k_, cv_int_30_10k_, cv_int_35_10k_, cv_int_40_10k_, cv_int_45_10k_, cv_int_50_10k_], label = 'internal', color = 'r')\n",
    "\n",
    "# plt.plot([10, 15, 20, 25, 30, 35, 40, 45, 50], [npmi_ext_10_10k_, npmi_ext_15_10k_, npmi_ext_20_10k_, npmi_ext_25_10k_, npmi_ext_30_10k_, npmi_ext_35_10k_ , npmi_ext_40_10k_, npmi_ext_45_10k_, npmi_ext_50_10k_], label = 'External')\n",
    "# plt.plot([10, 15, 20, 25, 30, 35, 40, 45, 50], [npmi_int_10_10k_, npmi_int_15_10k_, npmi_int_20_10k_, npmi_int_25_10k_, npmi_int_30_10k_, npmi_int_35_10k_, npmi_int_40_10k_, npmi_int_45_10k_, npmi_int_50_10k_], label = 'internal', color = 'r')\n",
    "\n",
    "# plt.plot([10, 15, 20, 25, 30, 35, 40, 45, 50], [umass_ext_10_10k_, umass_ext_15_10k_, umass_ext_20_10k_, umass_ext_25_10k_, umass_ext_30_10k_, umass_ext_35_10k_ , umass_ext_40_10k_, umass_ext_45_10k_, umass_ext_50_10k_], label = 'External')\n",
    "# plt.plot([10, 15, 20, 25, 30, 35, 40, 45, 50], [umass_int_10_10k_, umass_int_15_10k_, umass_int_20_10k_, umass_int_25_10k_, umass_int_30_10k_, umass_int_35_10k_, umass_int_40_10k_, umass_int_45_10k_, umass_int_50_10k_], label = 'internal', color = 'r')\n",
    "\n",
    "# plt.plot([10,25], [cv_ext_10_2k_, cv_ext_25_2k_])\n",
    "# plt.plot([10, 25], [cv_int_10_2k_, cv_int_25_2k_], color = 'r')\n",
    "\n",
    "plt.legend(loc=\"figs/upper right\")\n",
    "plt.xlabel('Number of topics')\n",
    "plt.ylabel('$C_v$')\n",
    "# plt.title('cv')\n",
    "plt.savefig('figs/ntopics_10k_cv.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dominant topics frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n",
      "/home/dhamzeia/.local/lib/python3.7/site-packages/pandas/core/internals/blocks.py:867: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>words/entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[C0267454:Necrotic enteritis, C0605290:2-chlor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[C0015967:Fever, C0032285:Pneumonia, C0011900:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[C0543467:Operative Surgical Procedures, C0025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[C0032042:Placebos, C0199470:Mechanical ventil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[policy, crisis, disaster, threat, economic, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  topic                                     words/entities\n",
       "0     0  [C0267454:Necrotic enteritis, C0605290:2-chlor...\n",
       "1     1  [C0015967:Fever, C0032285:Pneumonia, C0011900:...\n",
       "2     2  [C0543467:Operative Surgical Procedures, C0025...\n",
       "3     3  [C0032042:Placebos, C0199470:Mechanical ventil...\n",
       "4     4  [policy, crisis, disaster, threat, economic, p..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = ctm.get_topics(25)\n",
    "scores_prodLDA = pd.read_csv('results/scores_20_250_prodLDA_10k.csv')\n",
    "regexp = re.compile(r'c\\d{7}')\n",
    "\n",
    "df = pd.DataFrame(columns = ['topic', 'words/entities'])\n",
    "to_write = ''\n",
    "for i in range(len(topics)):\n",
    "    to_write+='topic '+str(i)+':\\n'\n",
    "    new_top =[]\n",
    "    for w in topics[i]:\n",
    "        if regexp.search(w):\n",
    "            try:\n",
    "                new_top.append('C'+w[1:len(w)] +':'+ linker.umls.cui_to_entity['C'+w[1:len(w)]][1])\n",
    "            except Exception:\n",
    "                new_top.append(w)\n",
    "        else:\n",
    "            new_top.append(w)\n",
    "        df.loc[i] =  i, new_top\n",
    "#     print(topic)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_doc_dist_test = ctm.get_doc_topic_distribution(dataset=test_set, n_samples = 2)\n",
    "topic_doc_dist_train = ctm.get_doc_topic_distribution(dataset=dataset, n_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlike_topics = np.argmax(topic_doc_dist_test, axis=1)\n",
    "diff = np.sort(topic_doc_dist_test, axis = 1)[::-1]\n",
    "diff = (diff[:, len(df)-1]- diff[:, len(df)-2])/diff[:, len(df)-2]*100\n",
    "test_df['diff'] = diff\n",
    "test_df['dominant_topic'] = mlike_topics\n",
    "freq = pd.Series(mlike_topics).value_counts(sort = False)\n",
    "freq = freq/sum(freq)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['dominant_topic_words'] = ''\n",
    "for i in range(len(df)):\n",
    "#     test_df.loc[test_df['dominant_topic']==i, 'dominant_topic_words'].set_value( df.loc[i, 'words/entities'])\n",
    "    test_df.loc[np.where(test_df['dominant_topic']==i)[0], 'dominant_topic_words'] =  ', '.join(df.loc[i, 'words/entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('manual_validation_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.loc[test_df['dominant_topic']==2, ].to_csv('mv_test_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('topics_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Percentage of Documents')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "freq.plot.bar()\n",
    "plt.xlabel('Topics (0 to 19)')\n",
    "plt.ylabel('Percentage of Documents')\n",
    "# plt.savefig('figs/freq_topic.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
